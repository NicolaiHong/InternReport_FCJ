[
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/5-workshop/5.2-prerequiste/",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": "Required AWS Knowledge AWS Console Navigation: Ability to navigate the AWS Management Console and find services Basic AWS Concepts: Understanding of AWS regions, availability zones, and basic service interactions No prior experience with S3, CloudFront, or WAF required - we\u0026rsquo;ll cover everything step by step Required Technical Skills Basic Web Development: Understanding of HTML, CSS, and JavaScript File System Operations: Ability to create, edit, and organize files and folders Command Line Basics: Comfortable running basic terminal/command prompt commands Text Editing: Familiarity with any code editor or IDE Required AWS Account Setup Before starting this workshop, ensure you have:\nAWS Account\nActive AWS account with administrative access Credit card on file (required even for Free Tier) MFA (Multi-Factor Authentication) enabled on root account (strongly recommended) IAM User (Recommended)\nIAM user with appropriate permissions instead of using root account Required permissions: AmazonS3FullAccess CloudFrontFullAccess WAFv2FullAccess AWSCertificateManagerFullAccess (if using custom domain) Access key and secret key generated (for CLI access) Billing Alerts\nSet up AWS Budgets or billing alerts to monitor costs Recommended: Set alert at $10 threshold Required Tools and Software Install the following tools on your local machine:\nText Editor or IDE\nVS Code (recommended): https://code.visualstudio.com/ Or any editor of your choice (Sublime Text, Atom, etc.) Web Browser\nModern browser (Chrome, Firefox, Safari, or Edge) Multiple tabs recommended for console navigation Git (Optional but recommended)\nDownload: https://git-scm.com/ Used for version control and sample code retrieval Sample Application We\u0026rsquo;ll provide a simple static website for this workshop.\nOptional: Custom Domain Setup If you want to use a custom domain (e.g., www.yoursite.com):\nDomain Name: Registered domain (can use Route 53 or external registrar) DNS Access: Ability to modify DNS records for your domain Note: This is optional; you can complete the workshop using CloudFront\u0026rsquo;s default domain Cost Expectations for Part 1: Frontend Deployment Free Tier Eligible Services:\nS3: 5GB storage, 20,000 GET requests, 2,000 PUT requests (first 12 months) CloudFront: 1TB data transfer out, 10,000,000 HTTP/HTTPS requests (first 12 months) AWS WAF: No Free Tier, but minimal cost for basic rules Estimated Costs (if exceeding Free Tier):\nS3 storage: $0.023 per GB per month CloudFront data transfer: $0.085 per GB (varies by region) WAF: $5.00 per month per web ACL + $1.00 per rule per month Total estimated cost for this workshop: $0-$2 (within Free Tier) or $5-$10 (with WAF) Cost Saving Tips:\nDelete resources immediately after workshop if not continuing Use small sample files to minimize storage and transfer costs Start with basic WAF rules and expand later Ready to Begin? Once you\u0026rsquo;ve completed all prerequisites and verified your setup, you\u0026rsquo;re ready to start building your secure, globally distributed frontend infrastructure!\nLet\u0026rsquo;s move on to Part 1: S3 Static Website Hosting.\n"
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1: Introductions and fundamental AWS skills\nWeek 2: Scoping the typing-game project, AWS microservices, and operational basics\nWeek 3: Hands-on AWS labs, UI/UX documentation, and NoSQL integration for Project 1\nWeek 4: RDS, Auto Scaling, CloudWatch, Route 53, CLI, CI/CD, Docker, Serverless patterns, and Security Hub\nWeek 5: Networking to deployment: VPC Peering, Transit Gateway, WordPress, Lambda cost tuning, CI/CD, Storage Gateway, FSx \u0026amp; WAF\nWeek 6: IAM access control, monitoring stack (Grafana \u0026amp; CloudWatch), Systems Manager, EC2 right-sizing, S3 encryption, Cost Explorer, data lake, and CloudFormation automation\nWeek 7: Deep dive into DynamoDB, IAM federation \u0026amp; cost optimization, Lightsail \u0026amp; containers, Step Functions, Cloud9, Elastic Beanstalk, CI/CD pipelines, and AWS security essentials\nWeek 8: Infrastructure as Code with CloudFormation, resilience \u0026amp; auto-scaling, refactoring to microservices, serverless SPA with Cognito \u0026amp; X-Ray, AI services (Polly, Rekognition, Lex), S3 \u0026amp; CloudFront, and CloudWatch dashboards\nWeek 9: Lex chatbot and SNS pub/sub, DynamoDB \u0026amp; ElastiCache labs, EKS CI/CD \u0026amp; blueprints, serverless \u0026amp; Fargate microservices, storage performance evaluations, S3 security best practices, and a data lake with Glue, Athena \u0026amp; QuickSight\nWeek 10: ROSA deployments, analytics pipeline with Kinesis, Glue, EMR, Athena, QuickSight \u0026amp; Redshift, business dashboards, VPC Flow Logs, delegated billing, CDK, event-driven SNS/SQS, and a full serverless stack (Cognito, CloudFront, SQS/SNS) with CI/CD\nWeek 11: AWS community event, serverless text service backed by DynamoDB, caching \u0026amp; validation, Amazon Bedrock Agent integration, observability with CloudWatch \u0026amp; X-Ray, and GraphQL APIs via AppSync \u0026amp; DynamoDB\n"
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/4-eventparticipated/4.1-event1/",
	"title": "Kick-off AWS FCJ Workforce - FPTU OJT FALL 2025",
	"tags": [],
	"description": "",
	"content": "Post-experience Report Event Objectives Build a high-quality generation of AWS Builders for Vietnam. Equip them with practical skills in Cloud, DevOps, AI/ML, Security, and Data \u0026amp; Analytics. Connect students with the 47,000+ member AWS Study Group community and AWS partner businesses. Speakers Nguyen Gia Hung – Head of Solutions Architect, AWS Vietnam Do Huy Thang – DevOps Lead, VNG Doanh Doan Hieu Nghi – GenAI Engineer, Renova Bui Ho Linh Nhi – AI Engineer, SoftwareOne Pham Nguyen Hai Anh – Cloud Engineer, G-Asia Pacific Nguyen Dong Thanh Hiep – Principal Cloud Engineer, G-Asia Pacific Key Highlights Identifying the common pitfalls lead to failure Spending on what make you fun for the mean time while ignoring what make you better. Learning a course for it job title instead of viewing it as a competitive edges -\u0026gt; No one would see you as a valuable workforce. Learning is a life-long journey and no one can take a shortcut in the ladder of knowledge. The journey for a working opportunities No one have it easy when it come to finding a job. Its a long and challenging way involving hard working and the ability to seize opportunity. What awaits me at AWS First Cloud Journey A way to connect to others around me and find companions to accompany me for the AWS First Cloud Journey and even for life. Lots of challenges that I need to cross to be a better version of myself. Opportunities for hand-on experience to further improve my abilities. Key Takeaways Prioritize Long-Term Growth Over Short-Term Fun Success requires investing in skills that build your future value, not just spending on temporary enjoyment. Approach learning as a way to gain a genuine competitive edge, rather than just collecting a job title, to become a truly valuable professional.\nEmbrace the Challenge as a Lifelong Journey Understand that securing career opportunities is a difficult process requiring hard work and persistence. There are no shortcuts on the ladder of knowledge; view every challenge as a necessary step to becoming a better version of yourself.\nEvent Experience Attending the “Kick-off AWS First Cloud Journey” workshop was extremely valuable, giving me a solid foundation of essential concepts, the practical skills to start building, and the inspiration to continue my lifelong learning journey in the cloud. Key experiences included:\nLearning from highly skilled speakers The event provided a multi-faceted learning experience, blending high-level industry vision with practical, on-the-ground career advice. We received a strategic overview of the cloud\u0026rsquo;s future from Nguyễn Gia Hưng, Head of Solutions Architect at AWS Vietnam, and gained deep insights into the crucial role of DevOps from Đỗ Huy Thắng, DevOps Lead at VNG. This was perfectly complemented by the relatable and inspiring stories from program alumni, who shared their personal journeys from being students to becoming specialized professionals like a GenAI Engineer and a Cloud Engineer. Hearing directly about \u0026ldquo;a day in the life\u0026rdquo; and the transition from the program into a full-time tech role provided a clear and tangible picture of the path ahead.\nNetworking and discussions From the moment of check-in to the dedicated tea break, the atmosphere was buzzing with energy. There were invaluable opportunities to connect with fellow students who will be our peers and collaborators throughout this On-the-Job Training program. Beyond peer networking, the final Q\u0026amp;A session was a highlight, allowing us to directly engage with the speakers and mentors. This open forum provided a chance to ask specific questions about career paths, technical challenges, and personal development, turning the one-way flow of information into a dynamic and collaborative discussion.\nLessons learned Three core lessons stood out from the event. First, cloud computing is the foundational launchpad for modern careers, not just a single destination; it\u0026rsquo;s the gateway to specializations in AI, DevOps, Security, and Data. Second, the journey is a marathon, not a sprint. The diverse stories from the alumni emphasized that this program is a critical first step, but continuous learning and resilience are what shape a successful career. Finally, community is a powerful accelerator. The event solidified that we are now part of a larger ecosystem-the AWS Builders community-where collaboration and shared knowledge are essential for growth.\nSome event photos Overall, the event not only provided technical knowledge but also helped me reshape my thinking about the way of learning and encourage me to keep pushing harder.\n"
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/3-blogstranslated/3.4-blog4/",
	"title": "Blog 4",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/3-blogstranslated/3.5-blog5/",
	"title": "Blog 5",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/3-blogstranslated/3.6-blog6/",
	"title": "Blog 6",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/",
	"title": "Create a gateway endpoint",
	"tags": [],
	"description": "",
	"content": " Open the Amazon VPC console In the navigation pane, choose Endpoints, then click Create Endpoint: You will see 6 existing VPC endpoints that support AWS Systems Manager (SSM). These endpoints were deployed automatically by the CloudFormation Templates for this workshop.\nIn the Create endpoint console: Specify name of the endpoint: s3-gwe In service category, choose AWS services In Services, type s3 in the search box and choose the service with type gateway For VPC, select VPC Cloud from the drop-down. For Configure route tables, select the route table that is already associated with two subnets (note: this is not the main route table for the VPC, but a second route table created by CloudFormation). For Policy, leave the default option, Full Access, to allow full access to the service. You will deploy a VPC endpoint policy in a later lab module to demonstrate restricting access to S3 buckets based on policies. Do not add a tag to the VPC endpoint at this time. Click Create endpoint, then click x after receiving a successful creation message. "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Hong Le Dang Khoa\nPhone Number: 0773018623\nEmail: hongkhoa348@gmail.com\nUniversity: FPT University Campus HCM\nMajor: Software Engineering\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 09/2025 to 02/2026\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "VPC endpoints VPC endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between your compute resources and AWS services without imposing availability risks. Compute resources running in VPC can access Amazon S3 using a Gateway endpoint. PrivateLink interface endpoints can be used by compute resources running in VPC or on-premises. Workshop overview In this workshop, you will use two VPCs.\n\u0026ldquo;VPC Cloud\u0026rdquo; is for cloud resources such as a Gateway endpoint and an EC2 instance to test with. \u0026ldquo;VPC On-Prem\u0026rdquo; simulates an on-premises environment such as a factory or corporate datacenter. An EC2 instance running strongSwan VPN software has been deployed in \u0026ldquo;VPC On-prem\u0026rdquo; and automatically configured to establish a Site-to-Site VPN tunnel with AWS Transit Gateway. This VPN simulates connectivity from an on-premises location to the AWS cloud. To minimize costs, only one VPN instance is provisioned to support this workshop. When planning VPN connectivity for your production workloads, AWS recommends using multiple VPN devices for high availability. "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/5-workshop/5.4-s3-onprem/5.4.1-prepare/",
	"title": "Prepare the environment",
	"tags": [],
	"description": "",
	"content": "To prepare for this part of the workshop you will need to:\nDeploying a CloudFormation stack Modifying a VPC route table. These components work together to simulate on-premises DNS forwarding and name resolution.\nDeploy the CloudFormation stack The CloudFormation template will create additional services to support an on-premises simulation:\nOne Route 53 Private Hosted Zone that hosts Alias records for the PrivateLink S3 endpoint One Route 53 Inbound Resolver endpoint that enables \u0026ldquo;VPC Cloud\u0026rdquo; to resolve inbound DNS resolution requests to the Private Hosted Zone One Route 53 Outbound Resolver endpoint that enables \u0026ldquo;VPC On-prem\u0026rdquo; to forward DNS requests for S3 to \u0026ldquo;VPC Cloud\u0026rdquo; Click the following link to open the AWS CloudFormation console. The required template will be pre-loaded into the menu. Accept all default and click Create stack. It may take a few minutes for stack deployment to complete. You can continue with the next step without waiting for the deployemnt to finish.\nUpdate on-premise private route table This workshop uses a strongSwan VPN running on an EC2 instance to simulate connectivty between an on-premises datacenter and the AWS cloud. Most of the required components are provisioned before your start. To finalize the VPN configuration, you will modify the \u0026ldquo;VPC On-prem\u0026rdquo; routing table to direct traffic destined for the cloud to the strongSwan VPN instance.\nOpen the Amazon EC2 console\nSelect the instance named infra-vpngw-test. From the Details tab, copy the Instance ID and paste this into your text editor\nNavigate to the VPC menu by using the Search box at the top of the browser window.\nClick on Route Tables, select the RT Private On-prem route table, select the Routes tab, and click Edit Routes.\nClick Add route. Destination: your Cloud VPC cidr range Target: ID of your infra-vpngw-test instance (you saved in your editor at step 1) Click Save changes "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: Introduce myself and connect with the First Cloud Journey team. Learn the format for writing worklogs and how to run workshops. Get a basic understanding of core AWS services and how to operate the Console and CLI. Tasks carried out this week: Day Task Start Date Completion Date Reference Material 2 - Met members of FCJ and made initial introductions - Read through internship rules and guidelines and took notes 09/08/2025 09/08/2025 Policies: https://policies.fcjuni.com/ 3 - Studied AWS and its main service categories for future project work + Compute + Storage + Networking + Database + Others - Watched tutorials on how to prepare and deliver workshops 09/09/2025 09/09/2025 About AWS: https://cloudjourney.awsstudygroup.com/ About workshop: https://van-hoang-kha.github.io/vi/ 4 - Applied workshop guidelines to draft this worklog - Created an AWS Free Tier account - Explored the AWS Management Console and installed/configured the AWS CLI - Practiced: + Creating an AWS account + Installing \u0026amp; configuring AWS CLI + Basic AWS CLI usage 09/10/2025 09/10/2025 My workshop git: https://github.com/NicolaiHong/InternShipReport_FCJ AWS Console: https://aws.amazon.com/ 5 - Learned foundational EC2 concepts: + Instance types + AMIs + EBS volumes + Storage options - Tested launching an EC2 instance and different SSH connection methods - Reviewed Elastic IP usage 09/11/2025 09/11/2025 EC2 console: https://ap-southeast-1.console.aws.amazon.com/ec2/ Amazon EC2 Basics: https://www.coursera.org/learn/aws-amazon-ec2-basics/ 6 - Additional hands-on practice: + Launch an EC2 instance + Connect via SSH + Attach and manage an EBS volume 09/12/2025 09/12/2025 EC2 console: https://ap-southeast-1.console.aws.amazon.com/ec2/ Week 1 Achievements (ongoing): Gained a clear overview of AWS and became familiar with the main service families:\nCompute Storage Networking Database Others Successfully set up and configured an AWS Free Tier account.\nBecame comfortable with the workshop format and the process of writing worklogs.\nNavigated the AWS Management Console and learned how to locate and operate services through the web UI.\nInstalled and configured the AWS CLI, including:\nAccess Key Secret Key Default region and related settings Built a basic understanding of EC2 fundamentals:\nInstance types — evaluating cost vs. performance trade-offs. AMIs — using pre-built machine images to launch instances. EBS — persistent block storage attached to instances. Storage options — selecting appropriate storage for different needs. Elastic IP — assigning static public IPs for cloud instances. "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "Real-Time Typing Challenge Platform TypeRush An AWS-Powered Architecture for a Multiplayer Typing App 1. Executive Summary TypeRush is an interactive web application that replicates the core experience of Monkeytype while introducing multiplayer functionality for live competitive typing sessions. Designed as a showcase of real-time web technologies and scalable cloud architecture, the platform combines a modern frontend experience with a serverless backend powered by AWS.\nUsers can create or join typing rooms, compete in real time, and view performance metrics such as Words Per Minute (WPM), accuracy, and rankings. The application leverages React for the frontend, AWS Lambda and API Gateway (WebSocket) for real-time synchronization, and Amazon RDS for player data and Amazon ElastiCache for session data. It also leverages Amazon Bedrock to generate daily challenges. Amazon Cognito secures user access and authentication. The solution demonstrates how cloud-native services can deliver fast, scalable, and cost-efficient real-time applications without the need for traditional server management.\n2. Problem Statement What’s the Problem? Existing typing platforms are often limited to single-player functionality, lacking real-time multiplayer interaction or open implementation examples. Most multiplayer solutions require dedicated servers or complex infrastructure, making them costly and difficult to maintain. For developers and enthusiasts, there is no accessible, serverless example demonstrating real-time synchronization, event-driven updates, and secure user management in one integrated system.\nThe Solution This project provides a serverless, real-time typing platform that allows multiple users to connect, type simultaneously, and see instant feedback on their progress. It is designed to be lightweight, cost-efficient, and scalable.\nBenefits and Return on Investment The project serves as both a developer learning platform and a user-facing application. It demonstrates best practices in event-driven architecture, WebSocket implementation, and cloud-based scalability at minimal operational cost. For users, it provides an engaging, social typing experience. For developers, it offers an educational reference for building serverless real-time systems.\nThe platform requires no ongoing server maintenance, ensuring:\nLong-term sustainability Rapid scalability Low operational overhead 3. Solution Architecture The platform employs a serverless AWS architecture to manage a real-time typing challenge application. Data is processed and stored using a variety of AWS services, and the frontend is a modern, responsive UI. The architecture is detailed below: AWS Services Used Amazon S3: Hosts the static frontend. CloudFront: Provides a CDN for the frontend and API. AWS WAF: Acts as a firewall for CloudFront. Route 53: Manages domain and DNS routing. Amazon Cognito: Handles user sign-in and sign-up. API Gateway: Serves as the entry point for microservices and connects to private ECS via VPC Link. AWS Lambda: Powers the record and text microservices and the WebSocket API for the game microservice. Amazon RDS: Stores records and leaderboards. DynamoDB: Stores text data. AWS Bedrock (Titan Text G1 Express): Functions as the LLM model for text generation. Amazon SNS: Manages notifications and alarms. AWS CloudWatch: Handles logging and monitoring. AWS Secrets Manager: Stores database and API secrets. Amazon ECS (Fargate): Hosts the core game backend. Elastic Load Balancer: Balances game service traffic. ElastiCache (Redis): Caches real-time game data. CodePipeline: Orchestrates builds and deployments. CodeBuild: Builds Docker images and Lambda services. ECR (Elastic Container Registry): Stores Docker images. GitLab: Triggers pipeline builds. Component Design Frontend Layer: A modern, responsive UI built with React, hosted on Amazon S3, and served via Amazon CloudFront with AWS WAF and Amazon Route 53 for enhanced performance, global content delivery, and robust security. API \u0026amp; Real-Time Communication: AWS API Gateway serves as the unified gateway for both REST and WebSocket APIs, implementing rate limiting, authentication, and request validation. The WebSocket API powers real-time multiplayer interactions. Compute \u0026amp; Business Logic: AWS Lambda handles serverless processing tasks like data persistence and AI-powered text generation. Amazon ECS with AWS Fargate hosts containerized backend services, including the game server. VPC PrivateLink connects API Gateway to ECS Fargate containers, and an Elastic Load Balancer distributes traffic. Data Layer: Amazon RDS stores relational data like user profiles and leaderboards. Amazon DynamoDB manages non-relational data such as dynamically generated text. Amazon ElastiCache (Redis) provides in-memory caching for frequently accessed data. Authentication \u0026amp; User Management: Amazon Cognito handles secure user registration, authentication, and authorization, supporting social and federated login. Amazon Secret Manager secures storage for secrets like API keys. CI/CD \u0026amp; DevOps: AWS CodeBuild and AWS CodePipeline enable continuous integration and continuous deployment (CI/CD) across all application components, automating build, test, and deployment processes. 4. Technical Implementation Implementation Phases This project has two parts—setting up the cloud infrastructure and building the application—each following 4 phases:\nBuild Theory and Draw Architecture: Research and design the AWS serverless architecture. Calculate Price and Check Practicality: Use the AWS Pricing Calculator to estimate costs and adjust if needed. Fix Architecture for Cost or Solution Fit: Tweak the design to stay cost-effective and usable. Develop, Test, and Deploy: Code the application and AWS services, then test and release to production. Technical Requirements Frontend: Practical knowledge of React. Backend: Experience with AWS services including Lambda, API Gateway, ECS, Fargate, RDS, DynamoDB, and ElastiCache. CI/CD: Familiarity with CodePipeline, CodeBuild, and GitLab. Infrastructure as Code: Use AWS CDK/SDK to code interactions. 5. Timeline \u0026amp; Milestones Project Timeline\nMonth 1: Study all about AWS and AWS services. Month 2: Start planning for the project and the AWS services to implement. Month 3: Develop, implement, and launch. 6. Budget Estimation \u0026mdash;\u0026gt; Budget Estimation File \u0026lt;\u0026mdash;\nInfrastructure Costs AWS Services: Amazon Route 53: $0.90 AWS Web Application Firewall (WAF): $9.03 Amazon CloudFront: $0.40 ($0 with free tier) S3 Standard: $0.11 ($0 with free tier) Data Transfer: $0.00 ($0 with free tier) Amazon API Gateway: $15.92 ($0 with free tier) Amazon Cognito: $0.00 Network Load Balancer: $18.49 AWS Fargate: $8.88 AWS Lambda: $0.02 ($0 with free tier) Amazon ElastiCache: $16.06 Amazon RDS for PostgreSQL: $39.37 ($0 with free tier) AWS Bedrock (Workload 1): $2.63 DynamoDB: $0.03 ($0 with free tier) AWS Secrets Manager: $4.00 AWS CodePipeline: $1.00 ($0 with free tier) AWS CodeBuild: $5.00 ($0 with free tier) With free tier account: about $60/month With paid tier account: about $122/month Both amounts assume 24/7 running services; in reality, it might be much lower.\n7. Risk Assessment Risk Matrix Technical Integration: Medium impact, medium probability. Performance and Scalability: High impact, medium probability. Cost Management: Medium impact, medium probability. Security: High impact, low probability. Data Reliability: Medium impact, low probability. Operational Risks: Low impact, medium probability. Learning Curve: Medium impact, high probability. Mitigation Strategies Technical Integration: This risk is mitigated by phased implementation and adherence to AWS best practices. Performance and Scalability: Optimizing WebSocket communication, leveraging Redis caching, and monitoring with CloudWatch will help maintain responsiveness. Cost Management: These can be controlled through AWS Budgets, usage alerts, and auto-scaling policies. Security: Risks will be minimized using AWS WAF, Cognito, Secrets Manager, and strict IAM policies. Data Reliability: Clear data ownership boundaries and transactional writes will ensure consistency. Operational Risks: Risks in CI/CD pipelines will be reduced with staged deployments and rollback configurations. Learning Curve: The learning curve will be addressed through dedicated study and hands-on practice in the first project phase. Contingency Plans If the service fails: A maintenance page will be displayed to users. If multiplayer is too slow: Multiplayer mode will be temporarily disabled, but single-player will remain active. If costs spike unexpectedly: The change that caused the spike will be immediately reversed. If a new update breaks the site: The system will automatically revert to the previous working version. 8. Expected Outcomes Technical Improvements: Upon completion, the project will deliver a scalable, secure, and fully functional real-time multiplayer typing platform built entirely on AWS.\nLong-term Value: It will demonstrate best practices in serverless architecture, event-driven design, and CI/CD automation. For users, it provides a smooth, engaging multiplayer typing experience. For us, it serves as our first hands-on, practical reference model for building real-time, serverless web applications on AWS with minimal cost and maintenance.\n"
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/4-eventparticipated/4.2-event2/",
	"title": "Data Science On AWS",
	"tags": [],
	"description": "",
	"content": "Summary Report Event Objectives Provide a comprehensive overview of building a modern Data Science system on AWS. Introduce the end-to-end Data Science pipeline, from data processing to model deployment. Offer hands-on experience with key AWS services like AWS Glue and Amazon SageMaker. Discuss practical considerations such as cost, performance, and the benefits of cloud vs. on-premise solutions. Speakers Văn Hoàng Kha – Cloud Solutions Architect, AWS Community Builder Bạch Doãn Vương – Cloud Develops Engineer, AWS Community Builder Key Highlights End-to-End Data Science Pipeline on AWS The workshop outlined the complete data science journey on the cloud, using core services:\nAmazon S3: For scalable data storage. AWS Glue: For serverless data integration, ETL (Extract, Transform, Load), and data cleaning. Amazon SageMaker: For building, training, and deploying machine learning models at scale. Practical Demonstrations Demo 1: Data Processing with AWS Glue: Showcased how to process and clean a real-world IMDb dataset, emphasizing the importance of data quality for model accuracy. Demo 2: Model Training with SageMaker: Demonstrated the process of training and deploying a Sentiment Analysis model, making the abstract concepts of ML deployment concrete. Integrating Custom Models: Showcased how to leverage frameworks like TensorFlow and PyTorch within SageMaker, using a sample project from a provided GitHub repository. Broadening AI/ML Capabilities with Managed Services An overview of AWS\u0026rsquo;s pre-built AI services that accelerate development:\nAmazon Transcribe: Speech-to-text conversion. Amazon Comprehend: Natural language processing for sentiment analysis and topic extraction. Amazon Rekognition: Image and video analysis. Amazon Personalize: Building personalized recommendation systems. Key Takeaways Data-First Mindset Business-first approach: Always start with the business context of the data, as emphasized by the need for feature engineering. Data quality is paramount: The workshop stressed that the accuracy of any ML model is directly dependent on the quality of the input data. Data as an asset: Data collection, governance, and security are the foundational pillars of a data-driven organization. Technical Architecture Modular Pipeline: The standard architecture involves a pipeline from S3 (storage) to AWS Glue (ETL) to Amazon SageMaker (ML), allowing for a clean separation of concerns. Flexibility: AWS supports both low-code solutions like SageMaker Canvas and code-intensive custom model training using frameworks like TensorFlow/PyTorch. Serverless benefits: Using services like AWS Glue removes the need for managing infrastructure, allowing teams to focus on data and models. Strategy Phased approach: Start with data collection and cleaning before moving to complex model training. Cloud vs. On-premise: The discussion highlighted that the cloud offers significant advantages in scalability, pay-for-what-you-use cost models, and access to powerful computing resources without upfront investment. ROI Measurement: Leverage cloud benefits to reduce development time and infrastructure overhead, leading to faster time-to-market for AI-powered features. Applying to Work Automate ETL: Use AWS Glue to create automated data cleaning and preparation jobs for analytics and ML. Adopt SageMaker: Pilot Amazon SageMaker for training and deploying ML models to streamline the MLOps lifecycle. Implement Sentiment Analysis: Apply the concepts from the demo to analyze customer feedback from reviews or support tickets. Explore Pre-built AI: Integrate services like Amazon Rekognition for content moderation or Amazon Transcribe for call center analytics. Consolidate Knowledge: Build a small project based on the workshop\u0026rsquo;s guidance to reinforce the concepts learned. Event Experience Attending the \u0026ldquo;Data Science on AWS\u0026rdquo; workshop provided a valuable, hands-on journey into cloud-based machine learning. Key experiences included:\nLearning from highly skilled speakers The speakers, both AWS Community Builders, shared practical insights and best practices from their real-world experience. Hands-on technical exposure The live demos of processing data with AWS Glue and training a model with SageMaker were extremely effective at translating theory into practice. Leveraging modern tools Explored the comprehensive AWS ecosystem for data science, understanding how different services fit together to form a cohesive pipeline. Learned about both fully managed AI services and the powerful customization options available within SageMaker. Lessons learned A solid data preparation strategy is non-negotiable for success in machine learning. AWS significantly lowers the barrier to entry for building and deploying sophisticated data science systems. Modern cloud platforms provide the flexibility to choose between low-code tools for speed and custom code for specific, complex requirements. Some event photos Overall, the workshop provided not only technical knowledge but also practical experience with building end-to-end data science pipelines on AWS, emphasizing the importance of data quality and the power of cloud-native ML services.\n"
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: Define and scope the initial typing-game project (key features, microservice boundaries, future matchmaking plans). Build team foundations: create a shared repository, seed the backlog, refine ER diagrams, choose tech stack, and assign ownership. Standardize formulas for WPM and accuracy. Prototype a FastAPI microservice for text generation, sentence assembly, and chat; verify the feasibility of integrating with Bedrock. Configure AWS Budgets and alerting. Gain hands-on experience with core AWS components: Lambda (function URLs), VPC (subnets, gateways, peering vs transit), VPC Flow Logs, and load-balancing concepts. Provision an Amazon RDS instance and design/seed a schema for text retrieval. Refactor the text service to use DB-backed retrieval and benchmark it against the prior API-based approach. Introduce early operational practices: define roles, run benchmarks, and set up basic monitoring for scalability. Tasks carried out this week: Day Task Start Date Completion Date Reference Material 2 - Conducted a team brainstorming session to shape and prioritize the first project + Migrated canvas notes into an initial backlog on the project board + Researched and documented consistent WPM and accuracy calculation methods + Initialized the shared code repository and set up the basic project layout for chosen languages and microservices + Iterated on ER diagram sketches and began drafting database schemas + Assigned lead owners for each microservice to clarify responsibilities 09/15/2025 09/15/2025 3 - Configure AWS Budgets: + Reviewed budget types (Cost, Usage, RI, etc.) + Set a monthly cost threshold + Created the budget in the AWS console + Enabled email/SNS alerts - Build a small web app with AWS Lambda: + Studied Lambda and function URL basics + Implemented a simple \u0026ldquo;Hello World\u0026rdquo; function + Configured the function and associated IAM role + Enabled and tested the function URL endpoint - Experiment with FastAPI for microservices: + Followed the FastAPI tutorial + Set up a local dev environment + Built a proof-of-concept API + Implemented and tested endpoints using the built-in Swagger UI 09/16/2025 09/16/2025 AWS Lambda: https://ap-southeast-1.console.aws.amazon.com/lambda AWS Budgets: https://us-east-1.console.aws.amazon.com/costmanagement/ FastAPI: https://www.coursera.org/learn/packt-mastering-rest-apis-with-fastapi-1xeea/ 4 - Studied AWS networking and security fundamentals: + Reviewed Amazon VPC as an isolated network in AWS + Distinguished public and private subnets and their roles + Learned when to use an Internet Gateway vs. a NAT Gateway + Explored VPC Flow Logs for monitoring and troubleshooting traffic + Compared options for connecting on-premises to AWS: Site-to-Site VPN vs. Direct Connect + Evaluated VPC Peering vs. Transit Gateway use cases + Reviewed Elastic Load Balancing concepts for distributing traffic and ensuring availability 09/17/2025 09/17/2025 Module 02-(01 to 03): https://www.youtube.com/watch?v=O9Ac_vGHquM https://www.youtube.com/watch?v=BPuD1l2hEQ4 https://www.youtube.com/watch?v=CXU8D3kyxIc 5 - Explored the Amazon Bedrock playground: + Reviewed available foundation models (e.g., Claude, Titan) + Selected a candidate model for text generation + Experimented with prompts and tuning parameters + Generated and analyzed sample outputs - Built a microservice prototype: + Designed the service flow and integrated multiple text-generation APIs + Implemented functions for random sentence creation and a chat capability + Reviewed limitations of the initial implementation and defined next steps + Validated the chosen technical approach 09/18/2025 09/18/2025 Amazon Bedrock:https://ap-southeast-1.console.aws.amazon.com/bedrock 6 - Provisioned and configured an Amazon RDS database: + Selected an appropriate engine for the project + Set up instance configuration, credentials, VPC and security group rules + Launched the instance, monitored creation, and recorded the connection endpoint securely - Implemented a DB-driven TextService prototype: + Designed a simple schema with tables for words and sentences + Wrote a one-time script to seed the RDS instance with initial data + Refactored the TextService to retrieve content from the database instead of external APIs and ran a benchmark to compare performance + Measured response-time improvements between the prior API approach and the new DB-backed method 09/19/2025 09/19/2025 Aurora and RDS: https://ap-southeast-1.console.aws.amazon.com/rds Week 2 Achievements: Scoped the first typing game: defined core functionality, microservice boundaries, populated an initial backlog, and assigned ownership. Documented standardized WPM and accuracy calculation methods. Initialized a shared repository with a baseline multi-language project layout. Refined ER diagrams and drafted the initial relational schema. Delivered a FastAPI prototype (text generation, sentence assembly, chat) and validated endpoints through Swagger UI. Configured AWS Budgets with a monthly threshold and alert notifications. Gained practical knowledge of core AWS building blocks: Lambda (function URLs), VPC (subnets, IGW, NAT, Flow Logs), peering vs transit gateway, and load-balancing basics. Evaluated Amazon Bedrock models and validated a candidate model plus prompting strategy. Launched an RDS instance, created the schema, and loaded a seed dataset. Refactored the TextService to use DB-based retrieval and observed an initial performance improvement in benchmarks. Established early operational practices: role assignments, benchmarking focus, and initial scalability considerations. "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/",
	"title": "Create an S3 Interface endpoint",
	"tags": [],
	"description": "",
	"content": "In this section you will create and test an S3 interface endpoint using the simulated on-premises environment deployed as part of this workshop.\nReturn to the Amazon VPC menu. In the navigation pane, choose Endpoints, then click Create Endpoint.\nIn Create endpoint console:\nName the interface endpoint In Service category, choose aws services In the Search box, type S3 and press Enter. Select the endpoint named com.amazonaws.us-east-1.s3. Ensure that the Type column indicates Interface. For VPC, select VPC Cloud from the drop-down. Make sure to choose \u0026ldquo;VPC Cloud\u0026rdquo; and not \u0026ldquo;VPC On-prem\u0026rdquo;\nExpand Additional settings and ensure that Enable DNS name is not selected (we will use this in the next part of the workshop) Select 2 subnets in the following AZs: us-east-1a and us-east-1b For Security group, choose SGforS3Endpoint: Keep the default policy - full access and click Create endpoint Congratulation on successfully creating S3 interface endpoint. In the next step, we will test the interface endpoint.\n"
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/",
	"title": "Test the Gateway Endpoint",
	"tags": [],
	"description": "",
	"content": "Create S3 bucket Navigate to S3 management console In the Bucket console, choose Create bucket In the Create bucket console Name the bucket: choose a name that hasn\u0026rsquo;t been given to any bucket globally (hint: lab number and your name) Leave other fields as they are (default) Scroll down and choose Create bucket Successfully create S3 bucket. Connect to EC2 with session manager For this workshop, you will use AWS Session Manager to access several EC2 instances. Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon EC2 instances and on-premises virtual machines (VMs) through an interactive one-click browser-based shell. Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.\nFirst cloud journey Lab for indepth understanding of Session manager.\nIn the AWS Management Console, start typing Systems Manager in the quick search box and press Enter: From the Systems Manager menu, find Node Management in the left menu and click Session Manager: Click Start Session, and select the EC2 instance named Test-Gateway-Endpoint. This EC2 instance is already running in \u0026ldquo;VPC Cloud\u0026rdquo; and will be used to test connectivity to Amazon S3 through the Gateway endpoint you just created (s3-gwe).\nSession Manager will open a new browser tab with a shell prompt: sh-4.2 $\nYou have successfully start a session - connect to the EC2 instance in VPC cloud. In the next step, we will create a S3 bucket and a file in it.\nCreate a file and upload to s3 bucket Change to the ssm-user\u0026rsquo;s home directory by typing cd ~ in the CLI Create a new file to use for testing with the command fallocate -l 1G testfile.xyz, which will create a file of 1GB size named \u0026ldquo;testfile.xyz\u0026rdquo;. Upload file to S3 bucket with command aws s3 cp testfile.xyz s3://your-bucket-name. Replace your-bucket-name with the name of S3 bucket that you created earlier. You have successfully uploaded the file to your S3 bucket. You can now terminate the session.\nCheck object in S3 bucket Navigate to S3 console. Click the name of your s3 bucket In the Bucket console, you will see the file you have uploaded to your S3 bucket Section summary Congratulation on completing access to S3 from VPC. In this section, you created a Gateway endpoint for Amazon S3, and used the AWS CLI to upload an object. The upload worked because the Gateway endpoint allowed communication to S3, without needing an Internet Gateway attached to \u0026ldquo;VPC Cloud\u0026rdquo;. This demonstrates the functionality of the Gateway endpoint as a secure path to S3 without traversing the Public Internet.\n"
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 12 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 12 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/4-eventparticipated/4.3-event3/",
	"title": "Reinventing DevOps with AWS Generative AI",
	"tags": [],
	"description": "",
	"content": "Summary Report Event Objectives Share the current context of DevOps and the impact of Generative AI. Present real-world case studies and practical frameworks for integrating AI into DevOps. Provide a live demonstration of AWS Generative AI tools that enhance the development lifecycle. Discuss the evolving role of DevOps engineers and the skills needed for the future. Speakers Le Thanh Duc – Cloud Delivery Manager, CMC Global Du Quoc Thanh – Technical Leader, CMC Global Van Hoang Kha – Cloud Engineer, AWS Community Builder Key Highlights The Evolution from DevOps to DevSecOps The modern DevOps mindset integrates security from the start, making it a shared responsibility across development, security, and operations teams. Shifting from a reactive, post-development security model to a proactive approach where security is embedded in every phase. This cultural shift is the core of DevSecOps, aiming to balance development speed with system security. Phases of a Secure DevOps Lifecycle A comprehensive, seven-phase approach to embedding security throughout the pipeline:\nPlan: Define security requirements and perform threat modeling. Code: Use static analysis (SAST) tools to detect vulnerabilities early. Build: Automate security checks, dependency scans, and configuration validation. Test: Integrate penetration testing and compliance auditing. Deploy: Scan Infrastructure as Code (IaC) to ensure secure environments. Operate: Automate patching, incident response, and remediation. Monitor: Use real-time analytics and AI-powered anomaly detection for proactive defense. Leveraging AI in the DevOps Toolchain Automation: AI automates repetitive tasks like code review, log analysis, vulnerability scanning, and filtering false positives. Enhanced Security: AI-driven tools can prioritize critical risks, suggest fixes, and detect anomalous behavior in runtime environments. Efficiency: AI assists in generating documentation, reports, and compliance policies, reducing manual workload. Tooling Examples: The session highlighted tools like SonarQube, Checkov, Prometheus, and GitHub Actions, along with AI\u0026rsquo;s role in enhancing their capabilities. AWS Tools for AI-Enhanced DevOps Amazon CodeGuru: A service demonstrated to scan code for vulnerabilities (e.g., SQL injection, secret leaks) and provide actionable recommendations for fixes. AWS Managed Control Plane (MCP) \u0026amp; Base (MCB): Tools for automating security compliance and updates for Terraform and Kubernetes (EKS) configurations. Cost Optimization: AI/ML services like AWS Cost Anomaly Detection and Compute Optimizer help predict resource needs and reduce waste. Key Takeaways Security Mindset Proactive Integration: Always start with security in mind, embedding it into the earliest stages of planning and development, not as an afterthought. Shared Responsibility: Foster a culture where developers, operations, and security teams are collectively responsible for security. Continuous Improvement: Use feedback loops from monitoring and incidents to continuously refine security processes. Technical Architecture Automated Security Pipeline: Embed automated security checks at every stage of the CI/CD pipeline, from code scanning to deployment. Observability: Implement robust monitoring, logging, and alerting systems (e.g., Prometheus, Grafana, Loki) to gain real-time insights into system health and security. Infrastructure as Code (IaC) Security: Utilize tools to scan IaC configurations to prevent misconfigurations before they reach production. AI Integration Strategy Phased Approach: Select and adopt AI tools that fit specific project needs to avoid performance overhead and unnecessary complexity. Human-in-the-Loop: View AI as a powerful assistant that enhances human capabilities, not as a replacement. Human oversight and judgment remain critical. Measure ROI: The integration of AI should be measured by its ability to increase development velocity, improve security posture, and reduce manual effort. Applying to Work Enhance CI/CD: Integrate automated static analysis (SAST) and dependency scanning tools into current pipelines. Adopt IaC Scanning: Implement tools like Checkov to validate Terraform or other IaC scripts. Pilot AWS AI Tools: Experiment with Amazon CodeGuru on a small-scale project to review code quality and security. Improve Monitoring: Leverage AI-powered anomaly detection to get proactive alerts on potential issues. Automate Documentation: Use AI to assist in generating and maintaining project documentation and reports. Event Experience Attending the \u0026ldquo;Reinventing DevOps with AWS Generative AI\u0026rdquo; session was highly valuable, offering a comprehensive overview of how AI is reshaping security and efficiency in software development. Key experiences included:\nLearning from highly skilled speakers Experts from CMC Global and AWS Vietnam shared deep insights from their extensive experience in cloud and DevOps. Through real-world case studies from clients in the Philippines and Singapore, I gained a practical understanding of implementing secure CI/CD pipelines. Hands-on technical exposure The live demonstration of Amazon CodeGuru was particularly insightful, showing how AI can concretely identify vulnerabilities and suggest code fixes in real-time. Leveraging modern tools Explored a modern DevOps toolchain, including SonarQube, Checkov, Prometheus, and GitLab CI, and understood how AI integrates with them. Learned how to use AI for infrastructure management and compliance with tools like AWS MCP and MCB. Networking and discussions The interactive Q\u0026amp;A session offered a chance to dive deeper into specific topics, such as the evolution of DevOps roles, AI\u0026rsquo;s limitations, and career advice for cloud architects. Discussions reinforced the importance of balancing AI automation with human expertise and critical thinking. Lessons learned Shifting to a DevSecOps culture is essential for building secure and reliable applications at speed. AI tools like Amazon CodeGuru can significantly boost productivity and security, but they require human oversight to verify and implement suggestions effectively. Modernization requires a clear strategy; a phased approach to adopting new tools and processes is less risky and more effective. Overall, the event provided not only technical knowledge but also reshaped my thinking about the future of DevOps, the indispensable role of integrated security, and the collaborative potential between AI and human engineers.\n"
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives: Complete essential AWS hands-on labs, including Site-to-Site VPN and core EC2 operations. Work through and finish all four modules of the AWS Cloud Technical Essentials course. Improve proficiency with AWS Console and CLI (credentials, key pairs, region/service navigation). Collaborate with product and design teams to analyze and document TypeRush UI/UX from Figma. Assess storage options and finalize the decision to adopt a scalable NoSQL model for TextService. Prototype MongoDB integration (environment setup, data seeding, service refactoring, validation). Establish consistent and effective communication routines with the First Cloud Journey team. Tasks carried out this week: Day Task Start Date Completion Date Reference Material 2 - Lab 03: AWS Site-to-Site VPN: + Built a complete Site-to-Site VPN setup, including a new VPC, a customer gateway EC2 instance, a Virtual Private Gateway, and the VPN connection. + Configured the tunnel and validated successful end-to-end connectivity. - Lab 04: Amazon EC2 Fundamentals: + Launched and connected to both Windows Server and Amazon Linux EC2 instances. + Deployed a sample “AWS User Management” CRUD application on both platforms. + Explored core EC2 capabilities such as instance resizing, EBS snapshot management, and building custom AMIs. 09/22/2025 09/22/2025 VPN Lab (Lab 03): https://000003.awsstudygroup.com/ EC2 Lab (Lab 04): https://000004.awsstudygroup.com/ 3 - Started the AWS Cloud Technical Essentials course and completed the first 2 modules: + Module 1: Cloud Foundations \u0026amp; IAM - Defined cloud computing and its value proposition. - Compared on-premises workloads with cloud workloads. - Created an AWS account and explored different interaction methods (Console, CLI, SDK). - Studied the AWS Global Infrastructure (Regions, Availability Zones). - Learned and applied IAM best practices. + Module 2: Compute \u0026amp; Networking - Reviewed EC2 architecture components. - Differentiated containers vs virtual machines. - Explored serverless technologies and their use cases. - Studied core VPC networking concepts and created a custom VPC. 09/23/2025 09/23/2025 AWS Cloud Technical Essentials: https://www.coursera.org/learn/aws-cloud-technical-essentials 4 - Collaborated with design to document the TypeRush UI/UX: + Participated in a cross-functional review session to evaluate the latest Figma flows. + Analyzed major screens (login, game, score summary, settings) to understand layout, hierarchy, and interaction patterns. + Listed technical feasibility questions and UI considerations for further alignment. + Began translating the designs into early component requirements and user stories. - Discussed TextService storage approach with the team lead: + Compared relational and non-relational storage models for word/sentence data. + Presented pros and cons for each approach based on usage patterns. + Finalized the decision to adopt a NoSQL solution due to dynamic schema needs and scalability. 09/24/2025 09/24/2025 5 - Integrated and tested MongoDB for the TextService prototype: + Set up a MongoDB environment using Docker. + Updated the data seeding script to insert word/sentence documents into MongoDB collections. + Refactored TextService logic to read/write data through MongoDB queries. + Performed full integration testing to ensure connectivity and correct data operations. 09/25/2025 09/25/2025 6 - Completed the remaining AWS Cloud Technical Essentials modules: + Module 3: Storage \u0026amp; Databases - Compared file, block, and object storage types. - Studied Amazon S3 concepts and created an S3 bucket. - Reviewed EBS usage with EC2. - Explored AWS database services and created a DynamoDB table. + Module 4: Monitoring \u0026amp; High Availability - Understood CloudWatch monitoring and alarm basics. - Learned cost/performance optimization techniques. - Studied Elastic Load Balancing for traffic routing. - Compared vertical vs horizontal scaling and built a high-availability setup. 09/26/2025 09/26/2025 AWS Cloud Technical Essentials: https://www.coursera.org/learn/aws-cloud-technical-essentials Week 3 Achievements: AWS Hands-on Labs Completed:\nBuilt a full Site-to-Site VPN setup (VPC, customer gateway EC2, virtual private gateway, tunnel configuration). Completed EC2 fundamentals including Windows/Linux instances, CRUD app deployment, snapshot handling, and custom AMIs. Completed all 4 modules of AWS Cloud Technical Essentials\n(Cloud Foundations/IAM, Compute \u0026amp; Networking, Storage \u0026amp; Databases, Monitoring \u0026amp; High Availability).\nImproved AWS Console \u0026amp; CLI Practice:\nAccount setup and IAM credential management. Navigated services and regions efficiently. Worked with key pairs and resource inspection commands. TypeRush UI/UX Documentation:\nReviewed Figma flows and major interface screens. Captured component behaviors and feasibility questions. Drafted early user stories and component specifications. TextService Storage Strategy:\nEvaluated SQL vs NoSQL approaches. Chose NoSQL for improved flexibility and scalability. MongoDB Prototype Implementation:\nSet up MongoDB via Docker. Updated seeding scripts. Refactored service logic to use MongoDB. Verified smooth end-to-end read/write operations. "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/5-workshop/5.3-s3-vpc/",
	"title": "Access S3 from VPC",
	"tags": [],
	"description": "",
	"content": "Using Gateway endpoint In this section, you will create a Gateway eendpoint to access Amazon S3 from an EC2 instance. The Gateway endpoint will allow upload an object to S3 buckets without using the Public Internet. To create an endpoint, you must specify the VPC in which you want to create the endpoint, and the service (in this case, S3) to which you want to establish the connection.\nContent Create gateway endpoint Test gateway endpoint "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/",
	"title": "Test the Interface Endpoint",
	"tags": [],
	"description": "",
	"content": "Get the regional DNS name of S3 interface endpoint From the Amazon VPC menu, choose Endpoints.\nClick the name of newly created endpoint: s3-interface-endpoint. Click details and save the regional DNS name of the endpoint (the first one) to your text-editor for later use.\nConnect to EC2 instance in \u0026ldquo;VPC On-prem\u0026rdquo; Navigate to Session manager by typing \u0026ldquo;session manager\u0026rdquo; in the search box\nClick Start Session, and select the EC2 instance named Test-Interface-Endpoint. This EC2 instance is running in \u0026ldquo;VPC On-prem\u0026rdquo; and will be used to test connectivty to Amazon S3 through the Interface endpoint we just created. Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nChange to the ssm-user\u0026rsquo;s home directory with command \u0026ldquo;cd ~\u0026rdquo;\nCreate a file named testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file to the same S3 bucket we created in section 3.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; This command requires the \u0026ndash;endpoint-url parameter, because you need to use the endpoint-specific DNS name to access S3 using an Interface endpoint. Do not include the leading \u0026rsquo; * \u0026rsquo; when copying/pasting the regional DNS name. Provide your S3 bucket name created earlier Now the file has been added to your S3 bucket. Let check your S3 bucket in the next step.\nCheck Object in S3 bucket Navigate to S3 console Click Buckets Click the name of your bucket and you will see testfile2.xyz has been added to your bucket "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nThis section will list and introduce the blogs you have translated. For example:\nBlog 1 - Getting started with healthcare data lakes: Using microservices This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 2 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 3 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 4 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 5 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 6 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\n"
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/4-eventparticipated/",
	"title": "Events Participated In",
	"tags": [],
	"description": "",
	"content": "During the internship, I attended 5 events. Each event offered a memorable experience, providing profound insights, valuable takeaways, and excellent networking opportunities.\nKick-off AWS FCJ Workforce Event Name: Kick-off AWS FCJ Workforce - FPTU OJT FALL 2025\nTime: 08:30, September 6, 2025\nLocation: 26th Floor, Bitexco Financial Tower, No. 02 Hai Trieu Street, Ben Nghe Ward, Ho Chi Minh City\nRole: Attendee\nSummary: Strategic orientation sessions covering AWS career paths (Cloud, DevOps, AI/ML, Security, Data), featuring alumni sharing, networking activities, and interactive Q\u0026amp;A.\nKey Takeaways \u0026amp; Value: Refined long-term development mindset; gained clarity on the DevOps/Cloud roadmap; committed to engaging with the AWS Builders community and sharing knowledge within the team.\nData Science On AWS Event Name: Data Science On AWS\nTime: 09:30, October 16, 2025\nLocation: FPT University HCMC, High Tech Park, Thu Duc City, Ho Chi Minh City\nRole: Attendee\nSummary: Demonstration of an end-to-end Data Science pipeline on AWS (S3 → Glue → SageMaker), featuring live demos (IMDb ETL, Sentiment analysis), and an overview of managed AI services (Transcribe, Comprehend, Rekognition, Personalize).\nKey Takeaways \u0026amp; Value: Acquired new skills in Glue ETL and SageMaker training/deployment; reinforced a data-first mindset; Contributions include automating ETL processes, piloting sentiment analysis models, and streamlining MLOps workflows.\nReinventing DevOps with AWS Generative AI Event Name: Reinventing DevOps with AWS Generative AI\nTime: 19:30, October 16, 2025\nLocation: Online via Microsoft Teams (Hosted by CMC Global)\nRole: Attendee\nSummary: Transitioning from DevOps to DevSecOps utilizing a seven-phase secure lifecycle and AI-augmented toolchains (including Amazon CodeGuru live demos), with a strong emphasis on IaC security and observability.\nKey Takeaways \u0026amp; Value: Mastered SAST/dependency scanning, IaC security scanning (e.g., Checkov), and AI-powered anomaly detection; Contributions involve implementing CI/CD security gates, piloting CodeGuru, and enhancing monitoring documentation.\nGenerative AI with Amazon Bedrock Event Name: Generative AI with Amazon Bedrock\nTime: 08:30, November 15, 2025\nLocation: 26th Floor, Bitexco Financial Tower, No. 02 Hai Trieu Street, Ben Nghe Ward, Ho Chi Minh City\nRole: Attendee\nSummary: High-level overview of Generative AI on AWS using Amazon Bedrock and Foundation Models. Covered advanced Prompt Engineering (Zero-Shot, Few-Shot, Chain-of-Thought), RAG implementation with Amazon Titan and vector databases, and an introduction to Agentic AI and Amazon Bedrock AgentCore for production-ready agents.\nKey Takeaways \u0026amp; Value: Understood how to architect RAG-powered applications, optimize prompt designs, and deploy scalable, secure AI prototypes to production. Defined next steps to utilize aws-samples and Bedrock_AgentCore for building internal AI assistants grounded in proprietary data.\nDevOps on AWS Event Name: DevOps on AWS\nTime: 08:30, November 17, 2025\nLocation: 26th Floor, Bitexco Financial Tower, No. 02 Hai Trieu Street, Ben Nghe Ward, Ho Chi Minh City\nRole: Attendee\nSummary: End-to-end DevOps practices on AWS: Infrastructure as Code (IaC) via CloudFormation and CDK; CI/CD pipelines using the CodeSuite (CodeCommit, CodeBuild, CodeDeploy, CodePipeline); Containerization with Docker and the AWS ecosystem (ECR, ECS, EKS, Fargate, App Runner); and Observability via CloudWatch and X-Ray.\nKey Takeaways \u0026amp; Value: Solidified the DevOps mindset and practical skills to replace manual \u0026ldquo;ClickOps\u0026rdquo; with automated IaC, standardized CI/CD workflows, and adopted a container-first architecture for microservices. Action items include codifying existing infrastructure, containerizing applications, and enhancing observability with custom dashboards and traces.\n"
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/4-eventparticipated/4.4-event4/",
	"title": "Generative AI with Amazon Bedrock",
	"tags": [],
	"description": "",
	"content": "Summary Report Event Vision To provide a strategic guide to Generative AI, from the foundational principles of large language models to the sophisticated art of prompt engineering. To unlock the power of Retrieval Augmented Generation (RAG) for building AI that is grounded in proprietary, real-time data. To introduce the next evolutionary step in AI: autonomous, goal-oriented agents that can reason and act. To unveil Amazon Bedrock AgentCore, a comprehensive platform designed to bridge the chasm between AI prototypes and secure, scalable production systems. Speakers Lam Tuan Kiet – Sr. DevOps Engineer, FPT Software Danh Hoang Hieu Nghi – AI Engineer, Renova Cloud Dinh Le Hoang Anh – Cloud Engineer Trainee, First Cloud AI Journey Key Highlights A Glimpse into the Future \u0026ldquo;The journey of AI is an evolution in autonomy.\u0026rdquo; This central theme framed the entire workshop, charting a course from simple AI assistants that follow rules to fully autonomous Agentic AI systems that can reason, plan, and execute complex workflows with minimal human oversight.\nPart 1: Mastering the Fundamentals with Amazon Bedrock The Foundation Model Revolution: The session began by contrasting traditional ML models with the broad, general-purpose Foundation Models (FMs). Amazon Bedrock was positioned as the secure gateway to a diverse range of cutting-edge FMs. The Art of the Prompt: Prompt Engineering was deconstructed as the skill of communicating effectively with an AI. The session covered a spectrum of techniques from Zero-Shot and Few-Shot to advanced Chain-of-Thought prompting. Retrieval Augmented Generation (RAG): Grounding AI in Reality: RAG was presented as the critical technology for enterprise AI. By using embedding models like Amazon Titan to convert private data into vector representations, RAG allows the AI to retrieve relevant, up-to-date information before generating an answer, combating hallucinations and tailoring responses to specific business contexts. Part 2: The Evolution into Autonomous Agents Bridging the Prototype-to-Production Chasm: The speakers identified the critical hurdles that prevent promising AI prototypes from becoming valuable business tools: Performance, Scalability, Security, and Governance. The Rise of Agentic AI: The workshop then pivoted to the future: AI Agents. These are not just chatbots; they are systems designed to achieve specific goals by automating entire workflows, leveraging open-source frameworks like LangChain, LlamaIndex, and Crew.AI. Part 3: Introducing Amazon Bedrock AgentCore The Production-Ready Platform for Agents: The highlight of the event was the introduction of Amazon Bedrock AgentCore, a comprehensive platform designed to solve the production chasm. AgentCore provides all the foundational services needed to run agents securely and at scale: Runtime \u0026amp; Identity: Securely execute and manage agent operations. Memory: Provide agents with context for coherent interactions. Tools: Equip agents with a Browser Tool to access live web data and a Code Interpreter to perform calculations. Gateway: A secure entry point for agents to interact with company APIs. Observability: Gain deep operational insights into agent performance, cost, and behavior. From Theory to Practice: The Live Demos and Code Blueprints The workshop transcended theory by grounding every concept in practical, hands-on code, referencing two key GitHub repositories as invaluable resources for attendees.\nThe Foundational Cookbook: aws-samples/amazon-bedrock-samples This official AWS repository was presented as the essential \\\u0026ldquo;cookbook\\\u0026rdquo; for mastering the core components of Bedrock. The demos showcased how this collection of Jupyter notebooks and code samples provides a flight simulator for AI development, allowing engineers to:\nExperiment with Prompting: Explore dozens of examples for Zero-shot, Few-shot, and Chain-of-Thought prompting across different models like Claude and Llama. Build a RAG Pipeline from Scratch: Step-by-step guides demonstrated how to use Amazon Titan to create embeddings, store them in a vector database, and build a complete, functional RAG pipeline for Q\u0026amp;A over private documents. Master the APIs: Provide clear, reusable code snippets for interacting with virtually every feature of the Bedrock service, from text generation to image creation. The Capstone Project: ihatesea69/Bedrock_AgentCore This repository served as the capstone demo, illustrating how to assemble the individual components from the \\\u0026ldquo;cookbook\\\u0026rdquo; into a sophisticated, functioning AI agent using the new AgentCore platform. The live demonstration walked through this repository to showcase how to:\nDefine an Agent: Structure the agent\u0026rsquo;s identity, instructions, and goals in code. Equip the Agent with Tools: Grant the agent the ability to perform actions beyond text generation, such as calling external APIs or interpreting code. Orchestrate a Mission: Tie everything together in a live example where the agent receives a complex request, autonomously chooses the right tools, retrieves necessary information, and executes a multi-step plan to achieve its goal. This made the abstract concept of \\\u0026ldquo;Agentic AI\\\u0026rdquo; tangible and achievable. Key Takeaways Your Quick-Start Blueprint: Applying This to Your Work Build a RAG-Powered Expert: Clone the amazon-bedrock-samples repository and adapt the RAG notebooks to connect to your own internal documentation, creating a powerful, context-aware internal search engine. Prototype Your First Agent: Use the Bedrock_AgentCore repository as a template. Define a simple agent that automates a multi-step business process unique to your team, such as generating a daily sales report by calling an internal API and summarizing the results. Refine Prompts with the Cookbook: Take an existing AI workflow and use the diverse examples in the amazon-bedrock-samples repo to upgrade your prompts from simple instructions to sophisticated, few-shot examples, dramatically improving output quality. Explore AgentCore Tools: Investigate how the tools within Bedrock AgentCore, like the Browser Tool or Code Interpreter, could solve a specific business problem that requires live data or dynamic calculations. Event Experience This workshop was a fascinating journey through the entire landscape of modern AI. The inclusion of comprehensive GitHub repositories transformed it from a theoretical lecture into an actionable masterclass. Attendees left not just with knowledge, but with the specific code blueprints needed to begin building the next generation of AI-driven applications immediately. The introduction of Amazon Bedrock AgentCore, backed by a practical demo, provided a clear and compelling roadmap for moving beyond simple AI prototypes to create secure, scalable, and truly autonomous enterprise agents.\nSome event photos "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives: Gain hands-on experience configuring Amazon RDS, including VPC setup, security groups, and backup management. Learn how to deploy scalable web applications using Auto Scaling Groups and Application Load Balancers. Strengthen monitoring skills with CloudWatch metrics, logs, alarms, and custom dashboards. Explore hybrid DNS architectures with Route 53 Resolver for enterprise environments. Improve proficiency with AWS CLI for managing resources across S3, EC2, VPC, and IAM. Build end-to-end CI/CD pipelines using CodeCommit, CodeBuild, CodeDeploy, and CodePipeline. Implement automated backup strategies with AWS Backup and lifecycle policies. Deploy containerized applications using Docker and container registries on AWS. Study virtual machine migration workflows, including import and export between environments. Build serverless applications using AWS Lambda and API Gateway with proper IAM configuration. Understand centralized security monitoring using AWS Security Hub and its service integrations. Tasks Completed This Week: Day Task Start Date Completion Date Reference Material 2 - Configure and Manage Amazon RDS: + Configure VPC, security groups, and DB subnet group + Launch EC2 and RDS instances + Deploy a sample app on EC2 connected to RDS + Perform backup and restore operations + Clean up all created resources - Deploy a Scalable Web Application using Auto Scaling: + Set up VPC, subnets, and security groups + Create an EC2 Launch Template + Configure Target Group and Application Load Balancer + Create Auto Scaling Group with manual, scheduled, and dynamic policies + Clean up all AWS resources - Monitor Resources with CloudWatch: + Analyze metrics using search and math expressions + Query logs with CloudWatch Logs Insights + Create Metric Filters + Configure CloudWatch Alarms + Build dashboards for visualization + Clean up alarms and dashboards 29/09/2025 29/09/2025 RDS, Auto Scaling, CloudWatch Labs 3 - Implement Hybrid DNS with Route 53 Resolver: + Deploy base infrastructure with CloudFormation + Set up Microsoft AD for on-prem DNS simulation + Create outbound and inbound Resolver endpoints + Configure forwarding rules + Test bidirectional name resolution and clean up resources - Manage AWS Services via CLI: + Install and configure AWS CLI + Use CLI to manage S3, SNS, IAM resources + Perform S3 bucket/object operations + Create and manage VPC components + Launch and terminate EC2 instances with CLI + Clean up all resources 30/09/2025 30/09/2025 Route 53 Resolver, AWS CLI Labs 4 - Build a CI/CD Pipeline for Automated Deployment: + Store source code in CodeCommit + Configure CodeBuild for compiling and packaging + Set up CodeDeploy for automated deployments + Orchestrate the full workflow with CodePipeline + Test automated deployment with a code push + Clean up all pipeline resources - Automate EC2 Backups with AWS Backup: + Deploy infrastructure using CloudFormation + Create backup plans with lifecycle rules + Configure backup notifications + Test backup and restore operations + Remove all created backups and stacks 01/10/2025 01/10/2025 CodeCommit, CodeBuild, CodeDeploy, AWS Backup Labs 5 - Deploy a Dockerized Application on AWS: + Set up VPC, security groups, IAM roles + Launch RDS instance as backend + Deploy application on EC2 using Docker image + Redeploy using Docker Compose + Push image to container registry (ECR or Docker Hub) + Clean up resources - Migrate Virtual Machines with Import/Export: + Export an on-prem VM + Upload VM image to S3 + Import VM to create AMI + Launch EC2 from imported AMI + Export EC2 instance back to S3 + Clean up all resources 02/10/2025 02/10/2025 Docker on AWS, VM Import/Export Labs 6 - Deploy a Serverless Application using Lambda and API Gateway: + Package and zip Lambda function with dependencies + Create IAM execution role + Deploy Lambda function + Create HTTP API in API Gateway and integrate with Lambda + Deploy and test endpoint + Clean up Lambda, API, and IAM role - Centralize Security Monitoring with Security Hub: + Enable Security Hub + Review aggregated findings and dashboards + Analyze detections from GuardDuty, Inspector, and Macie + Explore risk summaries and charts 03/10/2025 03/10/2025 Lambda \u0026amp; Security Hub Labs Week 4 Achievements: Successfully deployed Amazon RDS with full networking and backup configuration. Built scalable application infrastructure using Auto Scaling and ALB. Implemented comprehensive CloudWatch monitoring with metrics, logs, alarms, and dashboards. Configured hybrid DNS architecture using Route 53 Resolver and Microsoft AD. Strengthened AWS CLI proficiency across multiple services. Developed automated CI/CD pipelines with AWS developer tools. Implemented lifecycle-based backup strategies with AWS Backup. Deployed Dockerized applications and published container images to ECR. Completed VM migration workflows using import/export features. Built serverless applications with Lambda and API Gateway. Centralized security monitoring using Security Hub with service integrations. "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/5-workshop/5.4-s3-onprem/",
	"title": "Access S3 from on-premises",
	"tags": [],
	"description": "",
	"content": "Overview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/",
	"title": "On-premises DNS Simulation",
	"tags": [],
	"description": "",
	"content": "AWS PrivateLink endpoints have a fixed IP address in each Availability Zone where they are deployed, for the life of the endpoint (until it is deleted). These IP addresses are attached to Elastic Network Interfaces (ENIs). AWS recommends using DNS to resolve the IP addresses for endpoints so that downstream applications use the latest IP addresses when ENIs are added to new AZs, or deleted over time.\nIn this section, you will create a forwarding rule to send DNS resolution requests from a simulated on-premises environment to a Route 53 Private Hosted Zone. This section leverages the infrastructure deployed by CloudFormation in the Prepare the environment section.\nCreate DNS Alias Records for the Interface endpoint Navigate to the Route 53 management console (Hosted Zones section). The CloudFormation template you deployed in the Prepare the environment section created this Private Hosted Zone. Click on the name of the Private Hosted Zone, s3.us-east-1.amazonaws.com: Create a new record in the Private Hosted Zone: Record name and record type keep default options Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor (you saved when doing section 4.3) Click Add another record, and add a second record using the following values. Click Create records when finished to create both records. Record name: *. Record type: keep default value (type A) Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor The new records appear in the Route 53 console:\nCreate a Resolver Forwarding Rule Route 53 Resolver Forwarding Rules allow you to forward DNS queries from your VPC to other sources for name resolution. Outside of a workshop environment, you might use this feature to forward DNS queries from your VPC to DNS servers running on-premises. In this section, you will simulate an on-premises conditional forwarder by creating a forwarding rule that forwards DNS queries for Amazon S3 to a Private Hosted Zone running in \u0026ldquo;VPC Cloud\u0026rdquo; in-order to resolve the PrivateLink interface endpoint regional DNS name.\nFrom the Route 53 management console, click Inbound endpoints on the left side bar In the Inbound endpoints console, click the ID of the inbound endpoint Copy the two IP addresses listed to your text editor From the Route 53 menu, choose Resolver \u0026gt; Rules, and click Create rule: In the Create rule console: Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: Enter both IP addresses from your text editor (inbound endpoint addresses) and then click Submit You have successfully created resolver forwarding rule.\nTest the on-premises DNS Simulation Connect to Test-Interface-Endpoint EC2 instance with Session manager Test DNS resolution. The dig command will return the IP addresses assigned to the VPC Interface endpoint running in VPC Cloud (your IP\u0026rsquo;s will be different): dig +short s3.us-east-1.amazonaws.com The IP addresses returned are the VPC endpoint IP addresses, NOT the Resolver IP addresses you pasted from your text editor. The IP addresses of the Resolver endpoint and the VPC endpoint look similar because they are all from the VPC Cloud CIDR block.\nNavigate to the VPC menu (Endpoints section), select the S3 Interface endpoint. Click the Subnets tab and verify that the IP addresses returned by Dig match the VPC endpoint: Return to your shell and use the AWS CLI to test listing your S3 buckets: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Terminate your Session Manager session: In this section you created an Interface endpoint for Amazon S3. This endpoint can be reached from on-premises through Site-to-Site VPN or AWS Direct Connect. Route 53 Resolver outbound endpoints simulated forwarding DNS requests from on-premises to a Private Hosted Zone running the cloud. Route 53 inbound Endpoints recieved the resolution request and returned a response containing the IP addresses of the VPC interface endpoint. Using DNS to resolve the endpoint IP addresses provides high availability in-case of an Availability Zone outage.\n"
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/4-eventparticipated/4.5-event5/",
	"title": "DevOps on AWS",
	"tags": [],
	"description": "",
	"content": "Summary Report Event Objectives Instill the DevOps mindset, covering its culture, principles, and key performance metrics. Provide a deep dive into building CI/CD pipelines using native AWS DevOps services. Teach Infrastructure as Code (IaC) principles using AWS CloudFormation and the AWS CDK. Explore the AWS container ecosystem, including Docker, Amazon ECR, ECS, EKS, and App Runner. Demonstrate how to implement comprehensive monitoring and observability using CloudWatch and AWS X-Ray. Speakers Truong Quang Tinh - Platform Engineer (TymeX), AWS Community Builder Bao Huynh - AWS Community Builder Thinh Nguyen - AWS Community Builder Vi Tran - AWS Community Builder Van Hoang Kha – Cloud Engineer, AWS Community Builder Long Huynh - AWS Community Builder Quy Pham - AWS Community Builder Nghiem Le - AWS Community Builder Key Highlights From Manual Operations to Infrastructure as Code (IaC) The workshop highlighted the pitfalls of \u0026ldquo;ClickOps\u0026rdquo; (manual console-based management), such as being slow, error-prone, and difficult to replicate. AWS CloudFormation: Introduced as the native IaC solution, using YAML/JSON templates to define and manage AWS resources in \u0026ldquo;Stacks\u0026rdquo; and its ability to detect configuration drift. AWS Cloud Development Kit (CDK): Presented as a developer-centric IaC framework that allows defining infrastructure in familiar programming languages (e.g., Python, TypeScript), using reusable \u0026ldquo;Constructs\u0026rdquo; to accelerate development. Building a Full CI/CD Pipeline A complete, automated pipeline was demonstrated using the suite of AWS developer tools: AWS CodeCommit: For secure source control. AWS CodeBuild: For automated builds and testing. AWS CodeDeploy: For managing complex deployments like Blue/Green and Canary releases. AWS CodePipeline: To orchestrate the entire release process from source to deployment. Containerization and Orchestration The session covered the fundamentals of containerization with Docker and the importance of a container registry like Amazon ECR for storing and scanning images. A detailed comparison of orchestration services was provided: Amazon ECS: An AWS-native, simpler solution deeply integrated with the AWS ecosystem, ideal for teams wanting lower operational overhead. Amazon EKS: A managed Kubernetes service that aligns with the open-source standard, offering greater flexibility and multi-cloud portability at the cost of higher complexity. AWS Fargate \u0026amp; App Runner: Serverless compute options that remove the need to manage underlying servers for containers, simplifying deployment and operations. Monitoring and Observability The importance of full-stack observability was emphasized for maintaining and debugging distributed systems. Amazon CloudWatch: Used for collecting metrics, logs, and setting up alarms and dashboards. AWS X-Ray: Demonstrated for distributed tracing to analyze and debug performance bottlenecks in microservices architectures. Key Takeaways Design Mindset Automate Everything: Transition from manual \u0026ldquo;ClickOps\u0026rdquo; to a fully automated IaC approach to ensure consistency, speed, and reliability. Infrastructure as Code is Non-Negotiable: IaC is the foundation for modern DevOps, enabling collaboration, versioning, and reproducibility of environments. Choose the Right Tool for the Team: The choice between CloudFormation, CDK, ECS, and EKS should be based on team skills, ecosystem needs, and the desired balance between simplicity and control. Technical Architecture CI/CD Pipelines: Every project should have an automated pipeline that handles code integration, testing, and deployment to ensure rapid and safe releases. Container-First for Microservices: Use containers to package applications and their dependencies, and an orchestrator (ECS or EKS) to manage them at scale. Full-Stack Observability: Implement a robust monitoring strategy with metrics, logs (CloudWatch), and distributed tracing (X-Ray) to gain deep insights into application performance and health. Modernization Strategy Phased Adoption: Introduce DevOps practices incrementally. Start by converting one manual process to IaC or building a CI/CD pipeline for a single service. Leverage Serverless: Use serverless options like AWS Fargate and App Runner to reduce operational complexity and allow teams to focus on application logic rather than infrastructure management. Measure What Matters: Focus on key DevOps metrics like Deployment Frequency, Lead Time for Changes, and Mean Time to Recovery (MTTR) to drive continuous improvement. Applying to Work Automate a Deployment: Convert a manually deployed application to use an AWS CodePipeline workflow. Codify Infrastructure: Define an existing S3 bucket or EC2 instance using an AWS CloudFormation template or a CDK application. Containerize an Application: Create a Dockerfile for a web application and push the image to Amazon ECR. Pilot a Container Service: Deploy a simple containerized application using AWS App Runner or ECS with the Fargate launch type. Improve Observability: Create a CloudWatch Dashboard for a critical application and configure alarms for key metrics like CPU utilization and error rates. Event Experience Attending the \u0026ldquo;DevOps on AWS\u0026rdquo; workshop was extremely valuable, offering a comprehensive and practical guide to implementing modern DevOps practices on the cloud.\nLearning from highly skilled speakers The AWS Community Builders provided deep, practical knowledge, breaking down complex topics into understandable concepts. Hands-on technical exposure The multiple live demos, including a full CI/CD pipeline walkthrough and a microservices deployment on ECS, provided a clear, real-world context for the tools and services discussed. Leveraging modern tools The workshop provided a thorough exploration of the modern AWS DevOps toolkit, from advanced IaC with the CDK to serverless containers with App Runner. Networking and discussions The Q\u0026amp;A sessions offered opportunities to discuss career pathways and the AWS certification roadmap, providing valuable guidance for professional development. Lessons learned Adopting IaC is the single most impactful step towards achieving a mature DevOps practice. AWS provides a complete, integrated toolset to build a sophisticated DevOps platform, with options suitable for teams of all sizes and skill levels. Effective monitoring and observability are not optional; they are critical for operating reliable and performant applications in the cloud. Some event photos "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives: Implement advanced networking techniques with VPC Peering and Transit Gateway to interconnect multiple VPCs. Deploy a full-stack application using EC2, RDS, Auto Scaling and integrate with CloudFront. Build a cost-optimization solution using serverless patterns with AWS Lambda to automate EC2 management. Establish a CI/CD pipeline using AWS Developer Tools for automated deployments. Configure hybrid cloud storage with AWS Storage Gateway to connect on-premises environments. Manage enterprise file systems with Amazon FSx and strengthen web security with AWS WAF. Organize AWS resources effectively using Tags and Resource Groups. Improve operational skills using both the AWS Management Console and the AWS CLI. Tasks carried out this week: Day Task Start Date Completion Date Reference Material 2 - Set up VPC Peering between two VPCs:\n+ Provision the environment using CloudFormation\n+ Create Security Groups for EC2\n+ Launch EC2 instances in each VPC to verify connectivity\n+ Update Network ACLs\n+ Create \u0026amp; accept the Peering connection\n+ Configure Route Tables for routing\n+ Enable Cross-Peer DNS to resolve hostnames\n- Deploy a hub-and-spoke network using Transit Gateway:\n+ Create Key Pair\n+ Provision environment using CloudFormation\n+ Create a Transit Gateway as the central connector\n+ Attach VPCs to the Transit Gateway\n+ Configure Transit Gateway Route Tables\n+ Update VPC Route Tables 10/06/2025 10/06/2025 VPC Peering: https://000019.awsstudygroup.com/ Transit Gateway: https://000020.awsstudygroup.com/ 3 - Deploy WordPress on AWS:\n+ Prepare VPC/Subnets\n+ Create Security Groups for EC2 and RDS\n+ Launch EC2 host for WordPress\n+ Provision RDS for the database\n+ Install and configure WordPress\n+ Set up Auto Scaling\n+ Perform database backup/restore\n+ Integrate CloudFront to improve performance\n- Optimize EC2 costs with Lambda:\n+ Tag EC2 instances for cost management\n+ Create IAM Role for Lambda\n+ Write Lambda to automatically stop/start EC2 instances\n+ Test Lambda behavior 10/07/2025 10/07/2025 WordPress: https://000021.awsstudygroup.com/ Lambda Optimization: https://000022.awsstudygroup.com/ 4 - Automate application deployment with a CI/CD pipeline:\n+ Prepare required resources\n+ Install the CodeDeploy Agent on EC2\n+ Create a CodeCommit repository for source code\n+ Configure CodeBuild to build the application\n+ Set up CodeDeploy for automated deployments\n+ Build CodePipeline to orchestrate the pipeline\n+ Troubleshoot pipeline execution issues\n- Use Storage Gateway for hybrid cloud storage:\n+ Create an S3 Bucket\n+ Launch an EC2 host for the Storage Gateway appliance\n+ Activate the Storage Gateway\n+ Create file shares\n+ Mount file shares on on-premises machines 10/08/2025 10/08/2025 CI/CD: https://000023.awsstudygroup.com/ Storage Gateway: https://000024.awsstudygroup.com/ 5 - Manage Amazon FSx for Windows File Server:\n+ Create the environment\n+ Provision SSD and HDD Multi-AZ file systems\n+ Create file shares\n+ Run performance tests\n+ Enable Data Deduplication \u0026amp; Shadow Copies\n+ Manage sessions, open files, and quotas\n+ Enable Continuous Access shares\n+ Scale throughput and storage\n+ Delete the environment when finished\n+ Reference AWS CLI for FSx management\n- Deploy AWS WAF:\n+ Create an S3 bucket and deploy a sample web app\n+ Use Managed Rules\n+ Create advanced Custom Rules\n+ Test rules\n+ Enable logging\n+ Clean up resources 10/09/2025 10/09/2025 Amazon FSx: https://000025.awsstudygroup.com/ AWS WAF: https://000026.awsstudygroup.com/ 6 - Manage resources with Tags \u0026amp; Resource Groups:\n+ Understand and apply tags in the Console\n+ Launch EC2 with tags\n+ Add/remove tags on resources\n+ Filter resources by tags\n+ Use tags with the AWS CLI\n+ Tag EC2 via CLI\n+ Tag resources at creation via CLI\n+ List tagged resources via CLI\n+ Create Resource Groups based on tags\n+ Manage resources within Resource Groups 10/10/2025 10/10/2025 Tags \u0026amp; Resource Groups: https://000027.awsstudygroup.com/ Week 5 Achievements: Completed advanced networking on AWS:\nVPC Peering to connect VPCs directly Transit Gateway as a centralized connectivity hub Configured routing and DNS across multiple VPCs Deployed and optimized cloud applications:\nWordPress integrated with RDS Auto Scaling + CloudFront for improved performance and availability Lambda-based automation for EC2 cost optimization Built a complete DevOps workflow:\nFull CI/CD pipeline using CodeCommit – CodeBuild – CodeDeploy – CodePipeline Automated deployment process Hybrid storage solution via Storage Gateway Enterprise file management and web security:\nFSx Multi-AZ setup, quotas management, and deduplication AWS WAF with advanced rule management Applied effective resource governance:\nCost optimization \u0026amp; resource management using Tags Centralized resource grouping with Resource Groups Advanced operations via both Console \u0026amp; CLI Gained hands-on experience with Infrastructure as Code using CloudFormation to create consistent environments.\n"
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/5-workshop/5.5-policy/",
	"title": "VPC Endpoint Policies",
	"tags": [],
	"description": "",
	"content": "When you create an interface or gateway endpoint, you can attach an endpoint policy to it that controls access to the service to which you are connecting. A VPC endpoint policy is an IAM resource policy that you attach to an endpoint. If you do not attach a policy when you create an endpoint, AWS attaches a default policy for you that allows full access to the service through the endpoint.\nYou can create a policy that restricts access to specific S3 buckets only. This is useful if you only want certain S3 Buckets to be accessible through the endpoint.\nIn this section you will create a VPC endpoint policy that restricts access to the S3 bucket specified in the VPC endpoint policy.\nConnect to an EC2 instance and verify connectivity to S3 Start a new AWS Session Manager session on the instance named Test-Gateway-Endpoint. From the session, verify that you can list the contents of the bucket you created in Part 1: Access S3 from VPC: aws s3 ls s3://\\\u0026lt;your-bucket-name\\\u0026gt; The bucket contents include the two 1 GB files uploaded in earlier.\nCreate a new S3 bucket; follow the naming pattern you used in Part 1, but add a \u0026lsquo;-2\u0026rsquo; to the name. Leave other fields as default and click create Successfully create bucket\nNavigate to: Services \u0026gt; VPC \u0026gt; Endpoints, then select the Gateway VPC endpoint you created earlier. Click the Policy tab. Click Edit policy. The default policy allows access to all S3 Buckets through the VPC endpoint.\nIn Edit Policy console, copy \u0026amp; Paste the following policy, then replace yourbucketname-2 with your 2nd bucket name. This policy will allow access through the VPC endpoint to your new bucket, but not any other bucket in Amazon S3. Click Save to apply the policy. { \u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;, \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;, \u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34; ], \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34; } ] } Successfully customize policy\nFrom your session on the Test-Gateway-Endpoint instance, test access to the S3 bucket you created in Part 1: Access S3 from VPC aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy:\nReturn to your home directory on your EC2 instance cd~ Create a file fallocate -l 1G test-bucket2.xyz Copy file to 2nd bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; This operation succeeds because it is permitted by the VPC endpoint policy.\nThen we test access to the first bucket by copy the file to 1st bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy.\nPart 3 Summary: In this section, you created a VPC endpoint policy for Amazon S3, and used the AWS CLI to test the policy. AWS CLI actions targeted to your original S3 bucket failed because you applied a policy that only allowed access to the second bucket you created. AWS CLI actions targeted for your second bucket succeeded because the policy allowed them. These policies can be useful in situations where you need to control access to resources through VPC endpoints.\n"
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nSecure Hybrid Access to S3 using VPC Endpoints Overview AWS PrivateLink provides private connectivity to AWS services from VPCs and your on-premises networks, without exposing your traffic to the Public Internet.\nIn this lab, you will learn how to create, configure, and test VPC endpoints that enable your workloads to reach AWS services without traversing the Public Internet.\nYou will create two types of endpoints to access Amazon S3: a Gateway VPC endpoint, and an Interface VPC endpoint. These two types of VPC endpoints offer different benefits depending on if you are accessing Amazon S3 from the cloud or your on-premises location\nGateway - Create a gateway endpoint to send traffic to Amazon S3 or DynamoDB using private IP addresses.You route traffic from your VPC to the gateway endpoint using route tables. Interface - Create an interface endpoint to send traffic to endpoint services that use a Network Load Balancer to distribute traffic. Traffic destined for the endpoint service is resolved using DNS. Content Workshop overview Prerequiste Access S3 from VPC Access S3 from On-premises VPC Endpoint Policies (Bonus) Clean up "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives: Learn to manage EC2 access control using IAM services with resource tags and permission boundaries. Set up and configure monitoring tools including Grafana and AWS CloudWatch. Implement AWS Systems Manager for patch management and remote command execution. Optimize EC2 instances through right-sizing practices and AWS Compute Optimizer. Apply encryption to S3 data using AWS KMS and configure audit logging. Analyze AWS costs and usage patterns with Cost Explorer. Build a data pipeline and lake using S3, Kinesis, Glue, Athena, and QuickSight. Automate infrastructure provisioning with AWS CloudFormation templates. Tasks carried out this week: Day Task Start Date Completion Date Reference Material 2 - Manage access to EC2 services with resource tags through IAM services: + Create an IAM user for preparation + Create a custom IAM Policy to define specific permissions + Set up an IAM Role to be assumed by users or services + Verify the policy by switching roles and testing access - Getting started with Grafana basic: + Create a VPC and subnet to establish a network environment + Configure a Security Group to control inbound and outbound traffic + Launch an EC2 instance to host the monitoring application + Create an IAM User and Role for secure access to AWS resources + Assign the IAM Role to the EC2 instance + Install Grafana on the EC2 instance + Set up monitoring dashboards within Grafana 10/13/2025 10/13/2025 IAM services: https://000028.awsstudygroup.com/ Grafana basic: https://000029.awsstudygroup.com/ 3 - Limit user permissions with IAM Permission Boundary: + Perform preparatory steps for the exercise + Create a restriction policy to define the maximum allowable permissions + Create a new IAM user with limited permissions + Test the IAM user\u0026rsquo;s limits to verify the permission boundaries - Manage patches and run commands on multiple servers with AWS System Manager: + Create a VPC and Subnet for the network environment + Launch a public Windows EC2 instance + Create an IAM Role with necessary permissions + Assign the IAM Role to the EC2 instance + Configure and use Patch Manager to handle server patching + Use Run Command to execute commands on the servers 10/14/2025 10/14/2025 IAM permission boundary: https://000030.awsstudygroup.com/ AWS Systems Manager: https://000031.awsstudygroup.com/ 4 - Implement right-sizing practices for Amazon EC2: + Get acquainted with Amazon CloudWatch for monitoring + Create and attach an IAM Role for the CloudWatch Agent + Install the CloudWatch Agent on an EC2 instance + Use AWS Compute Optimizer to analyze and optimize EC2 configurations - Encrypt data at rest in S3 using AWS KMS: + Create necessary IAM policies, roles, groups, and users + Set up a Key Management Service (KMS) key + Create an S3 bucket and upload data + Configure AWS CloudTrail for logging and Amazon Athena for querying data + Test and share encrypted data stored in S3 10/15/2025 10/15/2025 EC2 right-sizing: https://000032.awsstudygroup.com/ S3 encryption with KMS: https://000033.awsstudygroup.com/ 5 - Visualize and analyze costs with AWS Cost Explorer: + View cost and usage data by service and by account + Analyze the scope and effectiveness of Savings Plans and Reserved Instances + Evaluate cost elasticity + Create custom reports for EC2 instances + Use Cost Explorer for in-depth cost analysis + Review data transfer costs for common architectures - Build a data lake on AWS: + Create an IAM Role and Policy for necessary permissions + Set up an S3 bucket for data storage + Create a Kinesis Data Firehose delivery stream for data collection + Use a Glue Crawler to create a data catalog + Perform data transformation + Analyze data using Amazon Athena + Visualize data with Amazon QuickSight 10/16/2025 10/17/2025 AWS Cost Explorer: https://000034.awsstudygroup.com/ Data lake on AWS: https://000035.awsstudygroup.com/ 6 - Study AWS CloudWatch for monitoring and observability: + Explore CloudWatch Metrics, including viewing, searching, and using expressions + Work with CloudWatch Logs, Logs Insights, and Metric Filters + Configure CloudWatch Alarms to trigger notifications + Create CloudWatch Dashboards for visualizing data - Automate infrastructure with AWS CloudFormation: + Create IAM Users and Roles for preparation + Develop a basic CloudFormation template to provision resources + Explore advanced features like Custom Resources with Lambda + Use Mappings, Stacksets, and Drift Detection for complex deployments 10/17/2025 10/17/2025 AWS CloudWatch: https://000036.awsstudygroup.com/ AWS CloudFormation: https://000037.awsstudygroup.com/ Week 6 Achievements: Successfully managed EC2 access control through IAM services: Configured resource-based access policies using tags Implemented permission boundaries to limit user capabilities Set up monitoring and observability infrastructure: Deployed Grafana on EC2 for custom dashboards Configured CloudWatch Metrics, Logs, and Alarms for resource monitoring Implemented AWS Systems Manager capabilities: Used Patch Manager for automated server updates Executed remote commands across multiple instances with Run Command Applied EC2 optimization and cost management practices: Installed and configured CloudWatch Agent for detailed metrics Analyzed instance configurations using AWS Compute Optimizer Reviewed cost patterns and trends with Cost Explorer Secured data storage with encryption: Created and managed KMS keys for S3 encryption Set up CloudTrail and Athena for audit logging and analysis Built a complete data lake pipeline: Configured Kinesis Data Firehose for streaming data ingestion Created data catalogs with Glue Crawler Performed data analysis with Athena and visualization with QuickSight Automated infrastructure deployment with CloudFormation: Developed templates for resource provisioning Explored advanced features including Lambda-backed custom resources and StackSets "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/5-workshop/5.6-cleanup/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "Congratulations on completing this workshop! In this workshop, you learned architecture patterns for accessing Amazon S3 without using the Public Internet.\nBy creating a gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, without traversing an Internet Gateway. By creating an interface endpoint you extended S3 connectivity to resources running in your on-premises data center via AWS Site-to-Site VPN or Direct Connect. clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nDuring my internship at [Company/Organization Name] from [start date] to [end date], I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in [briefly describe the main project or task], through which I improved my skills in [list skills: programming, analysis, reporting, communication, etc.].\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ✅ ☐ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ☐ ✅ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives: Advance hands-on proficiency with Amazon DynamoDB, covering basic operations, advanced data modeling, and multi-region/global architectures. Set up identity federation and IAM role configurations, and apply cost-optimization techniques across AWS and Azure AD. Deploy and operate applications using Lightsail, containers, Step Functions, and IAM roles to ensure secure access. Use Cloud9, Elastic Beanstalk, and CI/CD tools to automate application delivery pipelines. Improve AWS security posture through core IAM best practices, detective controls, incident response, and infrastructure protection. Design and automate microservices architectures using Lambda, DynamoDB, Step Functions, and CodeStar. Tasks carried out this week: Day Task Start Date Completion Date Reference Material 2 - DynamoDB fundamentals: + Create tables and populate sample records + Use AWS CLI for read/query/scan/insert/update operations + Inspect table data and GSIs in the console + Perform backups and restores (PITR and on-demand) - Advanced DynamoDB patterns: + Review capacity units and partitioning behavior + Compare sequential vs. parallel scans for throughput + Apply GSI write-sharding, key overloading, and sparse GSIs + Use composite keys and adjacency lists for complex access patterns - Change Data Capture: + Enable DynamoDB Streams and process changes + Create a Lambda to consume stream events + Explore CDC with Kinesis Data Streams as an alternative - Global serverless app with DynamoDB: + Provision serverless backend components + Configure Global Tables for multi-region replication + Interact with a sample global application front-end - Game player data modeling: + Design data model from entities and access patterns + Define primary keys and table layout + Use sparse GSIs to find available games + Add inverted indexes for retrieving a user’s past games - Cost \u0026amp; performance analysis with Glue and Athena: + Build a database using Glue crawlers + Query cost and usage reports with Athena + Apply tagging strategies for cost allocation 10/20/2025 10/20/2025 CDK basic: https://000038.awsstudygroup.com/ Amazon DynamoDB Immersion: https://000039.awsstudygroup.com/ Analysis with Glue and Athena: https://000040.awsstudygroup.com/ 3 - IAM federation from Azure AD: + Prepare Azure AD (tenant, users) + Create an Enterprise Application in Azure to connect to AWS + Configure an Identity Provider and matching IAM roles in AWS + Synchronize roles and grant users federated AWS console access + Validate sign-in from Azure AD to AWS console - Cost optimization (Savings Plans \u0026amp; RIs): + Compare Savings Plans and Reserved Instances + Use AWS recommendations to find savings opportunities + Purchase a Savings Plan to lower EC2 costs + Review RI types and RDS Reserved DB Instances options - Schema conversion \u0026amp; migration: + Prepare an EC2 host and install the Schema Conversion Tool + Configure source DB (Oracle/SQL Server) and convert schema for the target + Create DMS replication instance, endpoints, and tasks + Run migrations and replicate ongoing changes + Test DMS Serverless for auto-scaling during migration + Monitor migration progress with CloudWatch and task logs - IAM roles \u0026amp; condition policies: + Create IAM groups and users + Set up an admin role and enable role switching + Limit role use by IP address and time-based conditions - Deploy \u0026amp; manage apps on Lightsail: + Launch a database and WordPress on Lightsail + Configure networking and application settings + Deploy Prestashop and Akaunting instances + Secure apps, create snapshots, scale instance size, and add alarms 10/21/2025 10/21/2025 IAM Federation with Azure AD: https://000041.awsstudygroup.com/ AWS Cost Optimization: https://000042.awsstudygroup.com/ Database Migration with DMS: https://000043.awsstudygroup.com/ IAM Roles and Conditions: https://000044.awsstudygroup.com/ Amazon Lightsail Applications: https://000045.awsstudygroup.com/ 4 - Lightsail containers: + Create a Lightsail container service and deploy a public image + Provision a Lightsail instance, install Docker and the AWS CLI + Build, push, and deploy a custom container image from the instance - AWS Step Functions: + Launch a Cloud9 environment and deploy sample services + Create Step Functions workflows to orchestrate Lambdas with Task states + Add branching with Choice states, manage state I/O, and include Wait tokens + Implement error handling with retry and catch, and run parallel states - Authorize applications using IAM roles: + Create an EC2 instance and S3 bucket for app testing + Demonstrate drawbacks of long-lived access keys + Create an EC2 IAM role with S3 permissions and attach it to the instance 10/22/2025 10/22/2025 Lightsail Containers: https://000046.awsstudygroup.com/ AWS Step Functions: https://000047.awsstudygroup.com/ IAM Roles for Applications: https://000048.awsstudygroup.com/ 5 - Cloud9 basics: + Create a Cloud9 environment + Use the command line, edit files, and run AWS CLI tasks inside Cloud9 - Deploy monolithic app on Elastic Beanstalk: + Set up key pair, CloudFormation stack, and database + Configure and access the instance, test locally via Eclipse, and deploy to Beanstalk + Update the app and verify API endpoints - Automated release pipeline: + Create a CodeStar project and connect Eclipse to CodeCommit + Replace sample app, trigger pipeline, and deploy a Windows Service with CodeDeploy to EC2 + Monitor deployments from the IDE and pipeline tools - Foundational AWS security practices: + Secure the root account and enforce MFA + Create a dedicated IAM admin user/group for daily tasks + Enforce strong password policies and consider SCP guardrails in Organizations - IAM analysis \u0026amp; validation: + Use IAM Access Analyzer to validate least-privilege permissions + Create and test cross-account roles for temporary access + Inspect resource policies for unintended public/cross-account exposure + Review service last-accessed reports to remove unused permissions 10/23/2025 10/23/2025 AWS Cloud9: https://000049.awsstudygroup.com/ Elastic Beanstalk: https://000050.awsstudygroup.com/ CI/CD Pipeline: https://000051.awsstudygroup.com/ AWS Well-Architected Security Workshop: https://catalog.workshops.aws/well-architected-security 6 - Create a microservice: + Configure Eclipse IDE and develop a Lambda function + Test locally and deploy to AWS Lambda + Implement an ImageManager Lambda and automate with CodeStar CI/CD - Refactor data \u0026amp; workflows: + Provision a CloudFormation stack and a new DynamoDB table with a GSI + Build a Scan \u0026amp; Query microservice, create its API, update IAM policies, and redeploy via CodeStar + Implement a Calculator microservice using Step Functions + Lambda - Detective controls \u0026amp; incident response: + Deploy GuardDuty and review findings + Aggregate and prioritize findings in Security Hub and configure automated remediation + Use Detective for root cause analysis of security incidents - Infrastructure protection: + Design a VPC with segmented subnets and security groups + Deploy Network Firewall to inspect inter-subnet and egress/ingress traffic + Configure WAF rules to shield web apps from common attacks + Review AWS Shield Advanced for DDoS mitigation strategies - Data protection \u0026amp; encryption: + Use Amazon Macie to discover/classify sensitive S3 data + Apply customer-managed KMS keys for S3/EBS encryption + Provision SSL/TLS via ACM for load balancers + Manage and rotate secrets using Secrets Manager 10/24/2025 10/24/2025 Create Microservice: https://000052.awsstudygroup.com/ Refactor Data and Workflows: https://000053.awsstudygroup.com/ AWS Well-Architected Security Workshop: https://catalog.workshops.aws/well-architected-security Week 7 Achievements: Solid practical experience with Amazon DynamoDB: created tables, loaded data, used GSIs and composite keys, enabled backups, and implemented change-data-capture via Streams and Kinesis.\nPerformed cost and performance analysis using DynamoDB metrics alongside AWS Glue and Amazon Athena; experimented with tagging to improve cost allocation.\nImplemented identity federation and IAM governance:\nConfigured Azure AD federation to AWS, synchronized IAM roles, and verified federated console access. Created IAM groups, users, and roles with conditional policies (IP- and time-based restrictions). Improved cost optimization and database migration skills by evaluating Savings Plans and Reserved Instances, and practicing schema conversion and data migration using AWS SCT and DMS (including DMS Serverless and monitoring).\nDeployed and managed applications on Lightsail and container services: ran WordPress/Prestashop/Akaunting, managed snapshots and alarms, and built custom Docker images for deployment.\nOrchestrated workflows and secured application access:\nDeveloped Step Functions workflows (Task, Choice, Wait, Retry/Catch, Parallel). Favored IAM roles over long-lived access keys and attached EC2 roles for secure S3 access. Automated application lifecycle and CI/CD:\nUsed Cloud9, CodeStar, CodeCommit, CodeDeploy, and Elastic Beanstalk to streamline development and automated deployments. Strengthened organizational security posture:\nHardened root account practices (MFA), defined admin user/group usage, enforced password policies, explored SCPs, and used IAM Access Analyzer and last-accessed reports to tighten privileges. Built microservices and layered security controls:\nImplemented Lambda-based microservices with DynamoDB and Step Functions, automated deployment via CodeStar, and deployed monitoring/security tools (GuardDuty, Security Hub, Detective) plus network/data protections (VPC design, Network Firewall, WAF, Shield Advanced, Macie, KMS, ACM, Secrets Manager). "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nHere, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don’t understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? What do you think the company should improve for future interns? If recommending to a friend, would you suggest they intern here? Why or why not? Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Would you like to continue this program in the future? Any other comments (free sharing): "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives Apply Infrastructure as Code with AWS CloudFormation to deploy, update, and scale application environments. Strengthen workload reliability through resiliency testing, Auto Scaling, and automated recovery patterns. Build and secure modern architectures such as serverless SPAs with authentication and performance tracing. Explore AI, storage, and content delivery services including Amazon Polly, Rekognition, Lex, S3, and CloudFront. Monitor resources using CloudWatch dashboards across different OS platforms and prepare for the weekly assessment. Tasks Completed This Week Day Task Start Date Completion Date Reference Material 2 - Infrastructure as Code with CloudFormation + Deploy a foundational VPC and networking components via CloudFormation + Launch a multi-tier web application stack on the VPC + Review the running architecture (load balancer, Auto Scaling group, EC2 instances) + Examine stack resources and outputs to understand the deployment - Advanced Health Checks \u0026amp; Dependency Handling + Deploy a baseline web stack via CloudFormation + Simulate a dependency failure and observe application behavior + Configure deep health checks for the ALB + Implement a \u0026ldquo;fail open\u0026rdquo; mechanism to maintain limited functionality during outages - Evolving Infrastructure Using CloudFormation + Deploy a baseline stack and analyze its components + Update the running stack using parameter changes + Extend the template by adding an S3 bucket + Add another EC2 instance with custom configuration + Deploy the same stack in another AWS Region - Refactor Monolith to Microservices + Prepare the development environment and connect to the Windows instance + Analyze the monolithic application\u0026rsquo;s architecture + Build and deploy microservices: Advert, Invoice, ShoppingCart, Order, User + Serve static content via a dedicated Static microservice + Validate overall functionality of the microservices-based system 10/27/2025 10/27/2025 AWS Well-Architected Reliability Workshop https://catalog.workshops.aws/well-architected-reliability Refactoring to Microservices https://000054.awsstudygroup.com/ 3 - Resiliency Testing with AWS FIS + Create IAM roles and policies for FIS + Build an experiment template targeting specific resources + Run the fault injection experiment and monitor its effects + Review logs and experiment output to evaluate system behavior - Configure Auto Scaling for Load \u0026amp; Recovery + Create a launch template for web-tier EC2 instances + Configure a target group for the ALB + Create an Auto Scaling group + Deploy a load generator and validate scaling behavior - Automated Replacement via Health Checks + Manually terminate an EC2 instance to trigger recovery + Verify ASG replacement and ALB target health - Environment Setup + Create an EC2 Key Pair + Deploy foundational infrastructure via CloudFormation + Connect to the Windows instance to prepare the environment - Serverless Single Page Application Deployment + Create DynamoDB table + Build and deploy a Lambda microservice + Configure an API via API Gateway + Set up CI/CD with CodeStar + Deploy the SPA website and implement a client to consume the API - Authentication \u0026amp; Authorization + Integrate Cognito User Pools + Secure API/Lambda with authentication + Implement user sign-up and sign-in + Test complete auth flows - Performance Tracing with AWS X-Ray + Integrate X-Ray to trace requests and identify bottlenecks 10/28/2025 10/28/2025 AWS Well-Architected Reliability Workshop https://catalog.workshops.aws/well-architected-reliability Serverless Web Application https://000055.awsstudygroup.com/ 4 - Amazon Polly Integration + Explore the Polly console + Generate speech and speech marks using CLI + Use the Java SDK to synthesize speech - Object \u0026amp; Face Recognition with Rekognition + Prepare the environment + Detect objects in images + Implement basic facial recognition with a sample app - Build a Chatbot with Amazon Lex + Deploy base application and APIs + Create and enhance Lex bot + Implement Lambda fulfillment handler + Publish the bot - Static Web Hosting via S3 \u0026amp; CloudFront + Create S3 bucket and upload website content + Enable static website hosting + Configure public access settings and object permissions + Create CloudFront distribution - S3 Data Protection \u0026amp; Replication + Enable versioning + Practice object movement within buckets + Configure cross-region replication 10/29/2025 10/29/2025 AI Services Integration https://000056.awsstudygroup.com/ S3 \u0026amp; CloudFront https://000057.awsstudygroup.com/ 5 - CloudWatch Dashboards for Monitoring + Create dashboards for metrics visualization + Add metric and Logs Insights widgets - Monitor Windows EC2 Instance + Deploy VPC networking + Launch \u0026amp; configure Windows EC2 + Build a custom dashboard + Add CPU, network, and performance metrics + Generate synthetic load to observe changes - Monitor Linux EC2 Instance + Deploy VPC and launch Linux EC2 with a web server + Create a monitoring dashboard + Track CPU and network performance + Generate load to test responsiveness 10/30/2025 10/30/2025 AWS Well-Architected Performance Efficiency Workshop https://catalog.workshops.aws/well-architected-performance-efficiency/ 6 - EXAM DAY 10/31/2025 10/31/2025 Week 8 Achievements Developed strong proficiency in CloudFormation: provisioning VPCs, multi-tier stacks, and evolving infrastructure with new resources such as S3 buckets, EC2 instances, and multi-region deployments. Strengthened understanding of resiliency through fault injection, deep health checks, Auto Scaling configurations, and automated instance recovery. Successfully decomposed a monolithic application into multiple microservices (Advert, Invoice, ShoppingCart, Order, User, Static) and validated end-to-end operations. Built a serverless SPA with API Gateway, Lambda, DynamoDB, CI/CD (CodeStar), Cognito authentication, and X-Ray tracing. Gained hands-on experience with AI and content delivery services including Polly, Rekognition, Lex, S3, and CloudFront. Enhanced monitoring skills by creating CloudWatch dashboards and analyzing key metrics for Windows and Linux EC2 workloads. Completed the weekly assessment and consolidated knowledge across reliability, serverless, AI services, and monitoring domains. "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 9 Objectives: Architect conversational interfaces and event-driven systems utilizing Amazon Lex and Amazon SNS. Implement managed data and caching solutions with DynamoDB and ElastiCache, while automating EKS deployments. Enforce governance and scalability via Service Quotas, IAM-based usage controls, and EKS Blueprints. Develop and maintain serverless and containerized workloads, and assess storage performance across S3 and EFS. Secure S3 infrastructure and establish a fundamental data lake pipeline using Glue, Athena, and QuickSight. Tasks carried out this week: Day Task Start Date Completion Date Reference Material 2 - Configure an Amazon Lex Chatbot + Provision backend infrastructure and APIs + Initialize and configure a new Lex chatbot instance + Refine conversational intents and slot types + Implement Lambda hooks for fulfillment logic + Publish chatbot aliases for production use - Implement a Publish/Subscribe Messaging Model with Amazon SNS + Deploy base infrastructure via SAM template + Configure SNS Topics for event broadcasting + Build decoupled subscribers: Notification, Accounting, and Ride services + Apply message filtering policies for specific routing + Update publisher services and validate fan-out delivery 11/03/2025 11/03/2025 Amazon Lex Chatbot: https://000058.awsstudygroup.com/ Messaging with Amazon SNS: https://000059.awsstudygroup.com/ 3 - Work with Amazon DynamoDB + Execute core CRUD operations and table provisioning + Implement and query Global Secondary Indexes (GSI) + Administer tables via AWS CloudShell + Automate table management and queries using the Python SDK - Work with Amazon ElastiCache for Redis + Configure subnet groups for caching layers + Launch Redis clusters (Cluster Mode Enabled/Disabled) + Secure access and connect via client nodes + Integrate AWS SDK for Redis operations + Implement advanced patterns: Strings, Hashes, Pub/Sub, and Streams - Build a CI/CD Pipeline for an EKS Cluster + Set up Cloud9 environment and Kubernetes tools + Configure IAM roles for EKS authentication + Provision EKS cluster and validate sample app deployment + Setup CodePipeline and CodeBuild with IAM permissions + Automate manifest deployment from source repositories + Verify pipeline execution via code commit triggers 11/04/2025 11/04/2025 Amazon DynamoDB Workshop: https://000060.awsstudygroup.com/ Amazon ElastiCache Workshop: https://000061.awsstudygroup.com/ EKS CI/CD Workshop: https://000062.awsstudygroup.com/ 4 - Manage Service Quotas + Audit current utilization against limits + Request quota increases via the Service Quotas console - Implement Resource Usage and Cost Management with IAM + Structure IAM groups/users for permission boundaries + Enforce policies restricting resources by AWS Region + Limit allowed EC2 instance families + Apply restrictions on EC2 instance sizes + Control costs by limiting available EBS volume types - Deploy and Manage an EKS Cluster using EKS Blueprints + Bootstrap infrastructure: VPC and EC2 setup + Configure IAM roles for EKS Blueprints + Initialize EKS Blueprints and CDK projects + Construct deployment pipelines for cluster management + Manage multi-team access via Infrastructure as Code (IaC) + Integrate add-ons like Cluster Autoscaler + Deploy workloads using GitOps (ArgoCD) 11/05/2025 11/05/2025 Service Quotas Workshop: https://000063.awsstudygroup.com/ IAM Resource Management: https://000064.awsstudygroup.com/ EKS Blueprints Workshop: https://000065.awsstudygroup.com/ 5 - Build a Serverless Web Application using Lambda and API Gateway + Setup Cloud9 and CodeCommit for version control + Host frontend via AWS Amplify Console + Deploy serverless backend (Lambda + API Gateway) + Seed DynamoDB state data + Implement ride-booking logic integration + Develop photo processing pipelines using Lambda - Transition a Monolithic Application to Microservices using Docker and AWS Fargate + Provision environment via CloudFormation + Containerize legacy application with Docker + Deploy containers to serverless AWS Fargate + Configure Application Load Balancer and ECS Services + Manage task definition revisions and updates + Refactor and deploy microservices alongside the monolith - Evaluate Storage Performance on AWS + Deploy test infrastructure via CloudFormation + Benchmark S3 throughput and sync efficiency + Analyze performance for small-file and copy operations + Tune EFS IOPS and evaluate I/O size impact + Assess multi-threading effects on EFS performance 11/06/2025 11/06/2025 Serverless Web Application: https://000066.awsstudygroup.com/ Microservices with Fargate: https://000067.awsstudygroup.com/ Storage Performance Evaluation: https://000068.awsstudygroup.com/ 6 - Implement S3 Security Best Practices + Secure network access via CloudFormation + Configure secure access keys for EC2 + Enforce HTTPS transport and SSE-S3 encryption + Enable \u0026ldquo;Block Public Access\u0026rdquo; and disable ACLs + Restrict access using S3 VPC Endpoints + Audit configurations with AWS Config + Verify permissions with Access Analyzer - Build a Data Lake with Your Data + Stage raw datasets in Cloud9 and S3 + Profile and clean data using AWS DataBrew + Catalog and ingest data with AWS Glue + Transform datasets to Parquet format + Execute analytical queries via Amazon Athena + Visualize insights with Amazon QuickSight dashboards 11/07/2025 11/07/2025 S3 Security Best Practices: https://000069.awsstudygroup.com/ Data Lake Workshop: https://000070.awsstudygroup.com/ Week 9 Achievements: Engineered conversational workflows and messaging systems by deploying Amazon Lex chatbots with Lambda hooks and establishing a decoupled SNS Pub/Sub architecture.\nMastered managed data and caching services:\nExecuted core DynamoDB operations, including GSI management and SDK-based automation. Deployed ElastiCache for Redis clusters, implementing advanced caching patterns like Pub/Sub and Streams. Automated Kubernetes lifecycles by provisioning EKS clusters, defining IAM roles, and constructing CI/CD pipelines with CodePipeline to streamline application deployment.\nEnforced cloud governance and cost optimization:\nAudited and managed AWS Service Quotas for scalability. Applied granular IAM policies to restrict resource usage by Region, Instance Type, and Volume characteristics. Leveraged EKS Blueprints and IaC to bootstrap network infrastructure, manage multi-team access, and deploy add-ons (Autoscaler) and workloads via ArgoCD.\nArchitected modern application solutions:\nBuilt a full-stack serverless ride-share app using Amplify, API Gateway, and Lambda. Refactored a monolithic application into microservices, containerized via Docker, and deployed on AWS Fargate. Benchmarked storage performance by analyzing S3 throughput and transfer speeds, alongside tuning EFS IOPS and threading configurations.\nHardened S3 security posture:\nEnforced encryption and HTTPS, blocked public access via ACLs, and restricted traffic through VPC Endpoints. Utilized AWS Config and Access Analyzer to audit and remediate potential security risks. Established a foundational Data Lake: staged data in S3, transformed it via Glue/DataBrew, and enabled analytics with Athena querying and QuickSight visualizations.\n"
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 10 Objectives: Deploy and manage containerized workloads using Red Hat OpenShift Service on AWS (ROSA) and establish a fundamental CI/CD workflow. Architect and analyze a comprehensive data platform on AWS utilizing streaming ingestion, Glue, EMR, Athena, QuickSight, and Redshift. Design business intelligence dashboards and enhance observability via Amazon QuickSight, VPC Flow Logs, and delegated billing permissions. Develop and launch cloud-native applications utilizing AWS CDK, event-driven patterns with SNS/SQS, and serverless Lambda functions. Construct full-stack serverless web applications with API Gateway, Lambda, DynamoDB, Cognito, and CloudFront, ensuring SSL security and authentication. Automate order processing workflows and implement CI/CD pipelines for serverless architectures using SQS, SNS, SAM, and CodePipeline. Tasks carried out this week: Day Task Start Date Completion Date Reference Material 2 - Deploy Applications via Red Hat OpenShift Service on AWS (ROSA) + Enable ROSA service and install required CLI utilities + Provision and configure an OpenShift cluster on AWS infrastructure + Deploy a sample workload onto the active OpenShift cluster + Establish a basic CI/CD pipeline using CodeCommit, CodeBuild, and CodePipeline + Configure a dedicated CI/CD workflow for automated application deployment - Construct an Analytics Platform on AWS + Ingest and persist streaming data utilizing Amazon Kinesis Firehose + Automate data cataloging with AWS Glue Crawlers + Execute data transformations via Glue interactive sessions, Studio, and DataBrew + Process big data workloads using Amazon EMR + Perform interactive analysis with Amazon Athena and real-time analytics with Kinesis + Visualize insights by designing Amazon QuickSight dashboards + Serve data via AWS Lambda and architect a data warehouse with Amazon Redshift 11/10/2025 11/10/2025 ROSA Hands-on Lab: https://000071.awsstudygroup.com/ Analytics Platform: https://000072.awsstudygroup.com/ 3 - Initialize Amazon QuickSight Dashboards + Ingest data and construct initial visualizations including line charts, pie charts, and pivot tables + Refine the dashboard with advanced formatting, visual additions, and detailed data tables + Implement interactivity by configuring filters, actions, and navigation prior to publishing - Monitor Network Infrastructure via VPC Flow Logs + Configure and activate VPC Flow Logs to capture IP traffic metadata + Route flow log streams to Amazon CloudWatch Logs + Analyze traffic patterns to diagnose security group effectiveness and network health - Delegate Permissions to the AWS Billing Console + Establish an IAM user group and enable Billing Console access + Author a custom IAM policy for granular billing and cost management permissions + Attach the policy to the IAM group to finalize delegation + Validate the configuration by accessing the console as a delegated user 11/11/2025 11/11/2025 Amazon QuickSight Guide: https://000073.awsstudygroup.com/ VPC Flow Logs Lab: https://000074.awsstudygroup.com/ AWS Billing Console: https://000075.awsstudygroup.com/ 4 - Develop Infrastructure as Code with AWS CDK + Bootstrap the environment with IAM Roles and EC2 instances + Configure the VSCode development environment + Define application architecture (API Gateway, ELB, ECS) using CDK + Integrate Lambda logic and S3 storage constructs + Optimize structure using nested stacks for reusability - Architect Event-driven Systems with SNS and SQS + Provision core infrastructure and configure event generators + Implement a decoupled publish/subscribe model via SNS topics and SQS queues + Apply message filtering policies to SNS subscriptions for targeted routing + Design complex routing logic using advanced message filtering techniques - Develop Serverless Logic with AWS Lambda + Author a Lambda function for S3-triggered image processing + Configure S3 buckets and IAM execution roles for the function + Validate the image resizing operations + Provision a DynamoDB table for data persistence + Implement data writing to DynamoDB via Lambda functions 11/12/2025 11/12/2025 AWS CDK Development: https://000076.awsstudygroup.com/ Event-driven Architecture Lab: https://000077.awsstudygroup.com/ Serverless Lambda Guide: https://000078.awsstudygroup.com/ 5 - Integrate Serverless Frontend with API Gateway + Deploy client-side application assets + Provision a DynamoDB table + Deploy Lambda handlers for Create, Read, and Delete operations + Configure API Gateway methods and enable CORS + Validate API endpoints using Postman and the frontend interface - Deploy Serverless Applications via SAM + Automate frontend deployment using SAM + Define DynamoDB resources in the template + Deploy Lambda functions for listing, writing, deleting, and resizing images + Configure GET, POST, and DELETE verbs in API Gateway + Execute end-to-end testing via Postman and the web interface - Implement Identity Management with Amazon Cognito + Configure a Cognito User Pool + Secure the API and corresponding Lambda function + Verify the authentication flow via the frontend application - Configure SSL/TLS for Serverless Applications + Register a custom domain and Route 53 hosted zone + Provision an SSL certificate via AWS Certificate Manager + Configure a CloudFront distribution for HTTPS delivery 11/13/2025 11/13/2025 Serverless Frontend with API Gateway: https://000079.awsstudygroup.com/ Serverless Application with SAM: https://000080.awsstudygroup.com/ Cognito Authentication Lab: https://000081.awsstudygroup.com/ SSL Configuration: https://000082.awsstudygroup.com/ 6 - Automate Order Processing with SQS and SNS + Provision SQS queues and SNS topics + Create a DynamoDB table for order persistence + Develop Lambda functions for checkout, management, handling, and deletion + Validate the complete order processing lifecycle - Construct a CI/CD Pipeline for Serverless Workloads + Initialize a Git repository for the SAM pipeline code + Configure the SAM deployment pipeline utilizing AWS CodePipeline + Initialize a Git repository for frontend assets + Build a deployment pipeline for the frontend application + Verify the functionality and stability of the web application 11/14/2025 11/14/2025 Order Processing with SQS and SNS: https://000083.awsstudygroup.com/ CI/CD Pipeline for Serverless: https://000084.awsstudygroup.com/ Week 10 Achievements: Deployed applications using Red Hat OpenShift Service on AWS (ROSA) by enabling the service, provisioning a cluster, running sample workloads, and wiring a fundamental CI/CD workflow with CodeCommit, CodeBuild, and CodePipeline.\nArchitected and explored a data analytics platform on AWS:\nIngested streaming data via Kinesis Firehose and cataloged it using AWS Glue Crawlers. Transformed and processed data utilizing Glue (Studio and interactive sessions), DataBrew, and EMR, followed by analysis with Athena and Kinesis Data Analytics. Visualized insights via QuickSight dashboards and served data through Lambda and Redshift warehousing. Enhanced business intelligence and observability:\nDesigned interactive QuickSight dashboards featuring diverse charts, formatting, filters, and navigation actions. Activated and analyzed VPC Flow Logs in CloudWatch to interpret traffic patterns and validate security groups. Delegated AWS Billing Console access via IAM groups and custom policies, verifying permissions with user testing. Developed infrastructure and event-driven applications using AWS CDK:\nEstablished a CDK development environment on EC2 with VSCode and IAM roles, defining architectures for API Gateway, ELB, ECS, Lambda, and S3. Implemented SNS and SQS event-driven flows, incorporating both basic pub/sub models and advanced message filtering logic. Created Lambda-based workflows for image processing and data persistence utilizing S3 and DynamoDB. Built secure, end-to-end serverless application stacks:\nDeployed frontend apps integrating with API Gateway, Lambda, and DynamoDB using both manual configuration and SAM automation. Configured API Gateway with CORS, enforced Cognito-based authentication, and validated flows via Postman and UI. Secured applications with custom domains, Route 53 zones, ACM certificates, and CloudFront HTTPS distributions. Implemented automated order processing and CI/CD for serverless systems:\nConstructed an order processing pipeline leveraging SQS, SNS, DynamoDB, and Lambda functions for full lifecycle management. Established CI/CD pipelines for SAM-based backends and frontend applications using CodePipeline, linked to Git repositories for automated deployment. "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 11 Objectives: Participate in AWS community events to gain insights into advanced cloud architectures and industry standards. Engineer a serverless backend for text services, focusing on DynamoDB schema design and IAM role security. Program efficient database retrieval logic featuring pagination and complex filter expressions. Architect an in-memory caching strategy and robust API request validation mechanisms. Integrate Generative AI capabilities using Amazon Bedrock Agents for dynamic content generation. Implement comprehensive observability and debugging for serverless workloads. Construct scalable GraphQL APIs utilizing AWS AppSync and DynamoDB resolvers. Tasks carried out this week: Day Task Start Date Completion Date Reference Material 2 - Participation in \u0026ldquo;AWS Cloud Mastery Series #2\u0026rdquo; 11/17/2025 11/17/2025 3 - Provision Serverless Infrastructure for Text Service: + Deploy DynamoDB table wordsntexts with a string partition key for storage + Configure IAM execution roles permitting dynamodb:Scan, dynamodb:Query, and logging + Initialize Lambda handler skeleton and establish boto3 database connections - Develop Data Retrieval Algorithms: + Implement fetch_words_from_db utilizing scan operations with LastEvaluatedKey pagination + Create fetch_paragraph_from_db applying attributes filtering for content type and length 11/18/2025 11/18/2025 4 - Implement Caching and Data Processing Logic: + Architect a global in-memory cache with a 300-second TTL to optimize read costs + Code get_random_words to sample data from either cache or database sources + Code get_paragraph to handle quantity clamping (1-3) and text splitting logic - Engineer API Validation and Response Formatting: + Develop a Decimal_encoder class to serialize DynamoDB numeric types for JSON + Implement validation for query parameters type and count to catch TypeError/ValueError + Standardize JSON responses with correct HTTP headers and status codes 11/19/2025 11/19/2025 5 - Integrate Amazon Bedrock Agent for GenAI: + Update IAM policies to allow bedrock:InvokeAgent on Agent ID HUEBUXSALX + Instantiate bedrock-agent-runtime client and manage invoke_agent sessions + Program logic to decode streaming byte chunks into a cohesive string response - Execute Prompt Engineering and Model Testing: + Refine trigger prompts to enforce a specific output format (three paragraphs with separators) + Evaluate underlying models for formatting consistency + Implement parsing logic to segment the AI-generated text 11/20/2025 11/20/2025 6 - Monitor and Debug via CloudWatch and X-Ray + Audit Lambda logs in CloudWatch to diagnose execution failures + Define custom metrics to track specific application performance indicators + Set up CloudWatch Alarms to alert on critical metric deviations + Activate AWS X-Ray tracing to map service dependencies and latency - Build GraphQL APIs with AWS AppSync + Provision AppSync instance and link DynamoDB as a datasource + Develop resolvers for atomic Write and Read operations + Implement Update and Delete resolvers for data lifecycle management + Configure Scan/Query resolvers for bulk retrieval + Design complex resolvers for nested data fields 11/21/2025 11/21/2025 CloudWatch and X-Ray Monitoring: https://000085.awsstudygroup.com/ AppSync GraphQL APIs: https://000086.awsstudygroup.com/ Week 11 Achievements: Participated in the \u0026ldquo;AWS Cloud Mastery Series #2\u0026rdquo; to acquire knowledge on advanced AWS services and architectural patterns.\nEngineered the serverless infrastructure for a typing practice application:\nProvisioned DynamoDB table wordsntexts to house ~64,726 items using a string partition key. Secured the backend with granular IAM roles for DynamoDB access and CloudWatch logging. Configured optimized Lambda handlers (128 MB, 15s timeout) with persistent boto3 connections. Programmed advanced data retrieval algorithms:\nImplemented fetch_words_from_db utilizing efficient pagination via LastEvaluatedKey. Built fetch_paragraph_from_db utilizing filter expressions for length categories (Short: 10-25, Medium: 25-60, Long: 60+). Optimized performance and request handling:\nDeployed an in-memory caching mechanism (300s TTL) to minimize DB reads under a load of ~500 requests/hour. Implemented data sampling functions get_random_words and get_paragraph with input clamping. Created a custom Decimal_encoder for JSON serialization and robust input validation to prevent runtime errors. Standardized API responses with correct HTTP status codes and structure. Integrated Generative AI via Amazon Bedrock Agents:\nConfigured IAM access for bedrock:InvokeAgent targeting Agent ID HUEBUXSALX. Developed a runtime client to handle session-based invocation and stream decoding. Engineered prompts to generate structured content (three separated paragraphs) for daily challenges. Implemented parsing logic to seamlessly process AI-generated outputs. Established comprehensive monitoring and debugging:\nUtilized CloudWatch Logs for error analysis and defined custom metrics for performance tracking. Configured CloudWatch Alarms for critical thresholds and enabled X-Ray for distributed tracing. Developed a robust GraphQL API layer using AWS AppSync:\nIntegrated DynamoDB data sources and implemented full CRUD resolvers. Enabled bulk data access via Scan/Query resolvers and handled nested structures with complex object resolvers. "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]