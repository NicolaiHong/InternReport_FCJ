[
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/5-workshop/5.1-frontend-deployment/",
	"title": "Part 1: Frontend Deployment with CloudFront, WAF, and S3",
	"tags": [],
	"description": "",
	"content": "Introduction Welcome to the first workshop in our serverless application series! In this hands-on session, you\u0026rsquo;ll learn how to deploy a secure, high-performance static website using AWS\u0026rsquo;s content delivery and storage services.\nModern web applications require fast, reliable, and secure content delivery to users worldwide. In this workshop, you\u0026rsquo;ll build the frontend infrastructure that forms the foundation of a production-ready serverless application. You\u0026rsquo;ll configure Amazon S3 to host your static website files, set up Amazon CloudFront to distribute your content globally with low latency, and implement AWS WAF (Web Application Firewall) to protect your application from common web exploits.\nWhat You\u0026rsquo;ll Build By the end of this workshop, you\u0026rsquo;ll have deployed a complete frontend infrastructure featuring:\nStatic Website Hosting: An S3 bucket configured to serve your HTML, CSS, and JavaScript files Global Content Delivery: A CloudFront distribution that caches and delivers your content from edge locations worldwide Security Layer: AWS WAF rules protecting your application from common threats like SQL injection, cross-site scripting (XSS), and DDoS attacks HTTPS Security: SSL/TLS certificate configuration for secure communication Custom Domain (Optional): Your website accessible via a custom domain name Why This Architecture? This architecture offers several key benefits:\nPerformance: CloudFront edge locations ensure fast load times for users regardless of their geographic location Scalability: Automatically handles traffic spikes without manual intervention Cost-Effective: Pay only for what you use, with no servers to manage Security: Multiple layers of protection including WAF, DDoS protection, and encryption Reliability: Built on AWS\u0026rsquo;s highly available infrastructure with 99.9% uptime SLA Workshop Structure This workshop is divided into the following sections:\nPart 1: S3 Static Website Hosting\nCreate and configure S3 bucket Upload website files Configure bucket for static website hosting Test basic website access Part 2: CloudFront Distribution Setup\nCreate CloudFront distribution Configure origin settings Set up cache behaviors Test global content delivery Part 3: AWS WAF Configuration\nCreate Web ACL Configure security rules Associate WAF with CloudFront Test security rules Part 4: Cleanup (Optional)\nRemove resources to avoid charges Save configuration for future use Workshop Duration Estimated Time: 2-3 hours\nSetup and preparation: 15 minutes S3 configuration: 30 minutes CloudFront setup: 45 minutes WAF implementation: 45 minutes Testing and validation: 30 minutes "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/5-workshop/5.1-frontend-deployment/5.1.1-prerequisite/",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": "Required AWS Knowledge AWS Console Navigation: Ability to navigate the AWS Management Console and find services Basic AWS Concepts: Understanding of AWS regions, availability zones, and basic service interactions No prior experience with S3, CloudFront, or WAF required - we\u0026rsquo;ll cover everything step by step Required Technical Skills Basic Web Development: Understanding of HTML, CSS, and JavaScript File System Operations: Ability to create, edit, and organize files and folders Command Line Basics: Comfortable running basic terminal/command prompt commands Text Editing: Familiarity with any code editor or IDE Required AWS Account Setup Before starting this workshop, ensure you have:\nAWS Account\nActive AWS account with administrative access Credit card on file (required even for Free Tier) MFA (Multi-Factor Authentication) enabled on root account (strongly recommended) IAM User (Recommended)\nIAM user with appropriate permissions instead of using root account Required permissions: AmazonS3FullAccess CloudFrontFullAccess WAFv2FullAccess AWSCertificateManagerFullAccess (if using custom domain) Access key and secret key generated (for CLI access) Billing Alerts\nSet up AWS Budgets or billing alerts to monitor costs Recommended: Set alert at $10 threshold Required Tools and Software Install the following tools on your local machine:\nText Editor or IDE\nVS Code (recommended): https://code.visualstudio.com/ Or any editor of your choice (Sublime Text, Atom, etc.) Web Browser\nModern browser (Chrome, Firefox, Safari, or Edge) Multiple tabs recommended for console navigation Git (Optional but recommended)\nDownload: https://git-scm.com/ Used for version control and sample code retrieval Sample Application We\u0026rsquo;ll provide a simple static website for this workshop.\nOptional: Custom Domain Setup If you want to use a custom domain (e.g., www.yoursite.com):\nDomain Name: Registered domain (can use Route 53 or external registrar) DNS Access: Ability to modify DNS records for your domain Note: This is optional; you can complete the workshop using CloudFront\u0026rsquo;s default domain Cost Expectations for Part 1: Frontend Deployment Free Tier Eligible Services:\nS3: 5GB storage, 20,000 GET requests, 2,000 PUT requests (first 12 months) CloudFront: 1TB data transfer out, 10,000,000 HTTP/HTTPS requests (first 12 months) AWS WAF: No Free Tier, but minimal cost for basic rules Estimated Costs (if exceeding Free Tier):\nS3 storage: $0.023 per GB per month CloudFront data transfer: $0.085 per GB (varies by region) WAF: $5.00 per month per web ACL + $1.00 per rule per month Total estimated cost for this workshop: $0-$2 (within Free Tier) or $5-$10 (with WAF) Cost Saving Tips:\nDelete resources immediately after workshop if not continuing Use small sample files to minimize storage and transfer costs Start with basic WAF rules and expand later Ready to Begin? Once you\u0026rsquo;ve completed all prerequisites and verified your setup, you\u0026rsquo;re ready to start building your secure, globally distributed frontend infrastructure!\nLet\u0026rsquo;s move on to Part 1: S3 Static Website Hosting.\n"
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/5-workshop/5.2-serverless-backend/5.2.1-prerequisite/",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": "Required AWS Knowledge Part 1: Frontend Deployment Completion: Understanding of AWS console navigation and basic services; Having a frontend application ready to connect HTTP/REST APIs: Knowledge of HTTP methods (GET, POST, PUT, DELETE) and REST principles Database Basics: Understanding of relational databases, tables, and SQL queries JSON: Familiarity with JSON format for API requests/responses Basic Networking: Understanding of concepts like VPC, subnets, and security groups IAM: Familiarity with IAM roles and policies. Apply best practice of least-privileges to resources Required Technical Skills Programming: Intermediate knowledge of either: Node.js (JavaScript/ TypeScript) - Recommended for this workshop SQL Queries: Basic SELECT, INSERT, UPDATE, DELETE statements API Testing: Using tools like Postman or curl Command Line: Comfortable with terminal commands Environment Variables: Understanding of configuration management Required AWS Account Setup Before starting this workshop, ensure you have:\nAWS Account\nActive AWS account with administrative access or permissions for: Lambda API Gateway RDS Cognito Secrets Manager VPC IAM CloudWatch Logs IAM User/Role Permissions\nRequired managed policies: AWSLambda_FullAccess AmazonAPIGatewayAdministrator AmazonRDSFullAccess AmazonCognitoPowerUser SecretsManagerReadWrite AmazonVPCFullAccess IAMFullAccess (for creating Lambda execution roles) CloudWatchLogsFullAccess Billing Alerts Configured\nSet up AWS Budgets or billing alerts Recommended threshold: $20-30 for this workshop RDS can incur higher costs than previous workshop Required Tools and Software Install the following tools on your local machine:\nAWS CLI (Version 2) (optional)\nDownload: https://aws.amazon.com/cli/ Verify installation: aws --version Ensure credentials are configured: aws configure Node.js and npm (if using Node.js for Lambda)\nDownload: https://nodejs.org/ (LTS version recommended) Minimum version: Node.js 18.x or higher Verify: node --version and npm --version Python (if using Python for Lambda)\nDownload: https://www.python.org/ Minimum version: Python 3.9 or higher Verify: python --version or python3 --version pip installed: pip --version Git\nDownload: https://git-scm.com/ Verify: git --version Text Editor or IDE\nVS Code (recommended): https://code.visualstudio.com/ Extensions recommended: AWS Toolkit ESLint (for JavaScript) Python (if using Python) API Testing Tool\nPostman (recommended): https://www.postman.com/downloads/ Or Insomnia: https://insomnia.rest/download Or curl (command line) Database Client (Optional but helpful)\nDBeaver (free, supports PostgreSQL): https://dbeaver.io/ Or pgAdmin: https://www.pgadmin.org/ Or psql command line tool Sample Application Code We\u0026rsquo;ll provide sample Lambda function code and SQL scripts:\nOption 1: Clone the workshop repository\ngit clone https://github.com/your-workshop/serverless-app-backend.git cd serverless-app-backend Option 2: Write code from scratch following the workshop\nWe\u0026rsquo;ll provide all code snippets in the tutorial Good for learning and understanding each component Optional: Completed Part 1: Frontend Deployment While not strictly required, having completed Part 1: Frontend Deployment provides context:\nYou\u0026rsquo;ll understand how the frontend will consume these APIs You\u0026rsquo;ll have a complete end-to-end application If you skipped Part 1: Frontend Deployment:\nYou can still complete this workshop independently We\u0026rsquo;ll test APIs using Postman instead of the frontend You can integrate with any frontend later Cost Expectations for Workshop 2 Estimated Costs for this Workshop:\nWithin Free Tier (first 12 months):\nRDS (single-AZ, limited hours): $0-5 Lambda: $0 API Gateway: $0 Cognito: $0 Secrets Manager: $0 (30-day trial) Total: $0-5 Pre-Workshop Checklist Before beginning, verify you have:\nAWS account with appropriate permissions AWS CLI installed and configured Node.js/Python installed (based on your preference) Git installed Code editor/IDE installed API testing tool installed (Postman recommended) Billing alerts configured at $20 Sample code repository cloned (or ready to write from scratch) At least 3-4 hours available for focused work Understanding of REST APIs and databases What You\u0026rsquo;ll Learn By completing this workshop, you will:\nDesign and implement serverless API architectures Create and configure Lambda functions with proper permissions Set up and secure RDS databases in VPC Implement API Gateway REST APIs with multiple endpoints Configure Cognito user pools and authentication flows Integrate Cognito authorizers with API Gateway Manage secrets securely with AWS Secrets Manager Restrict API Gateway access with Lambda functions and AWS Secrets Manager Connect Lambda functions to RDS databases Implement proper error handling and logging Test APIs with authentication tokens Ready to Begin? Once you\u0026rsquo;ve completed all prerequisites and verified your setup, you\u0026rsquo;re ready to start building your secure, scalable serverless backend infrastructure!\nExpected Outcomes By the end of this workshop, you\u0026rsquo;ll have:\nA fully functional REST API with multiple endpoints Secure user authentication and authorization Database-backed data persistence Professional logging and monitoring Production-ready security practices A backend that integrates with your Part 1: Frontend Deployment frontend (if completed) Next Step Let\u0026rsquo;s move on to Part 1: VPC and Network Setup to create the secure network foundation for your database and Lambda functions.\nLet\u0026rsquo;s build your serverless backend! üöÄ\n"
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1: Introductions and fundamental AWS skills\nWeek 2: Scoping the typing-game project, AWS microservices, and operational basics\nWeek 3: Hands-on AWS labs, UI/UX documentation, and NoSQL integration for Project 1\nWeek 4: RDS, Auto Scaling, CloudWatch, Route 53, CLI, CI/CD, Docker, Serverless patterns, and Security Hub\nWeek 5: Networking to deployment: VPC Peering, Transit Gateway, WordPress, Lambda cost tuning, CI/CD, Storage Gateway, FSx \u0026amp; WAF\nWeek 6: IAM access control, monitoring stack (Grafana \u0026amp; CloudWatch), Systems Manager, EC2 right-sizing, S3 encryption, Cost Explorer, data lake, and CloudFormation automation\nWeek 7: Deep dive into DynamoDB, IAM federation \u0026amp; cost optimization, Lightsail \u0026amp; containers, Step Functions, Cloud9, Elastic Beanstalk, CI/CD pipelines, and AWS security essentials\nWeek 8: Infrastructure as Code with CloudFormation, resilience \u0026amp; auto-scaling, refactoring to microservices, serverless SPA with Cognito \u0026amp; X-Ray, AI services (Polly, Rekognition, Lex), S3 \u0026amp; CloudFront, and CloudWatch dashboards\nWeek 9: Lex chatbot and SNS pub/sub, DynamoDB \u0026amp; ElastiCache labs, EKS CI/CD \u0026amp; blueprints, serverless \u0026amp; Fargate microservices, storage performance evaluations, S3 security best practices, and a data lake with Glue, Athena \u0026amp; QuickSight\nWeek 10: ROSA deployments, analytics pipeline with Kinesis, Glue, EMR, Athena, QuickSight \u0026amp; Redshift, business dashboards, VPC Flow Logs, delegated billing, CDK, event-driven SNS/SQS, and a full serverless stack (Cognito, CloudFront, SQS/SNS) with CI/CD\nWeek 11: AWS community event, serverless text service backed by DynamoDB, caching \u0026amp; validation, Amazon Bedrock Agent integration, observability with CloudWatch \u0026amp; X-Ray, and GraphQL APIs via AppSync \u0026amp; DynamoDB\n"
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/4-eventparticipated/4.1-event1/",
	"title": "Kick-off AWS FCJ Workforce - FPTU OJT FALL 2025",
	"tags": [],
	"description": "",
	"content": "Post-experience Report Event Objectives Build a high-quality generation of AWS Builders for Vietnam. Equip them with practical skills in Cloud, DevOps, AI/ML, Security, and Data \u0026amp; Analytics. Connect students with the 47,000+ member AWS Study Group community and AWS partner businesses. Speakers Nguyen Gia Hung ‚Äì Head of Solutions Architect, AWS Vietnam Do Huy Thang ‚Äì DevOps Lead, VNG Doanh Doan Hieu Nghi ‚Äì GenAI Engineer, Renova Bui Ho Linh Nhi ‚Äì AI Engineer, SoftwareOne Pham Nguyen Hai Anh ‚Äì Cloud Engineer, G-Asia Pacific Nguyen Dong Thanh Hiep ‚Äì Principal Cloud Engineer, G-Asia Pacific Key Highlights Identifying the common pitfalls lead to failure Spending on what make you fun for the mean time while ignoring what make you better. Learning a course for it job title instead of viewing it as a competitive edges -\u0026gt; No one would see you as a valuable workforce. Learning is a life-long journey and no one can take a shortcut in the ladder of knowledge. The journey for a working opportunities No one have it easy when it come to finding a job. Its a long and challenging way involving hard working and the ability to seize opportunity. What awaits me at AWS First Cloud Journey A way to connect to others around me and find companions to accompany me for the AWS First Cloud Journey and even for life. Lots of challenges that I need to cross to be a better version of myself. Opportunities for hand-on experience to further improve my abilities. Key Takeaways Prioritize Long-Term Growth Over Short-Term Fun Success requires investing in skills that build your future value, not just spending on temporary enjoyment. Approach learning as a way to gain a genuine competitive edge, rather than just collecting a job title, to become a truly valuable professional.\nEmbrace the Challenge as a Lifelong Journey Understand that securing career opportunities is a difficult process requiring hard work and persistence. There are no shortcuts on the ladder of knowledge; view every challenge as a necessary step to becoming a better version of yourself.\nEvent Experience Attending the ‚ÄúKick-off AWS First Cloud Journey‚Äù workshop was extremely valuable, giving me a solid foundation of essential concepts, the practical skills to start building, and the inspiration to continue my lifelong learning journey in the cloud. Key experiences included:\nLearning from highly skilled speakers The event provided a multi-faceted learning experience, blending high-level industry vision with practical, on-the-ground career advice. We received a strategic overview of the cloud\u0026rsquo;s future from Nguy·ªÖn Gia H∆∞ng, Head of Solutions Architect at AWS Vietnam, and gained deep insights into the crucial role of DevOps from ƒê·ªó Huy Th·∫Øng, DevOps Lead at VNG. This was perfectly complemented by the relatable and inspiring stories from program alumni, who shared their personal journeys from being students to becoming specialized professionals like a GenAI Engineer and a Cloud Engineer. Hearing directly about \u0026ldquo;a day in the life\u0026rdquo; and the transition from the program into a full-time tech role provided a clear and tangible picture of the path ahead.\nNetworking and discussions From the moment of check-in to the dedicated tea break, the atmosphere was buzzing with energy. There were invaluable opportunities to connect with fellow students who will be our peers and collaborators throughout this On-the-Job Training program. Beyond peer networking, the final Q\u0026amp;A session was a highlight, allowing us to directly engage with the speakers and mentors. This open forum provided a chance to ask specific questions about career paths, technical challenges, and personal development, turning the one-way flow of information into a dynamic and collaborative discussion.\nLessons learned Three core lessons stood out from the event. First, cloud computing is the foundational launchpad for modern careers, not just a single destination; it\u0026rsquo;s the gateway to specializations in AI, DevOps, Security, and Data. Second, the journey is a marathon, not a sprint. The diverse stories from the alumni emphasized that this program is a critical first step, but continuous learning and resilience are what shape a successful career. Finally, community is a powerful accelerator. The event solidified that we are now part of a larger ecosystem-the AWS Builders community-where collaboration and shared knowledge are essential for growth.\nSome event photos Overall, the event not only provided technical knowledge but also helped me reshape my thinking about the way of learning and encourage me to keep pushing harder.\n"
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/3-blogstranslated/3.4-blog4/",
	"title": "Blog 4",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/3-blogstranslated/3.5-blog5/",
	"title": "Blog 5",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/3-blogstranslated/3.6-blog6/",
	"title": "Blog 6",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Hong Le Dang Khoa\nPhone Number: 0773018623\nEmail: hongkhoa348@gmail.com\nUniversity: FPT University Campus HCM\nMajor: Software Engineering\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 09/2025 to 02/2026\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: Introduce myself and connect with the First Cloud Journey team. Learn the format for writing worklogs and how to run workshops. Get a basic understanding of core AWS services and how to operate the Console and CLI. Tasks carried out this week: Day Task Start Date Completion Date Reference Material 2 - Met members of FCJ and made initial introductions - Read through internship rules and guidelines and took notes 09/08/2025 09/08/2025 Policies: https://policies.fcjuni.com/ 3 - Studied AWS and its main service categories for future project work + Compute + Storage + Networking + Database + Others - Watched tutorials on how to prepare and deliver workshops 09/09/2025 09/09/2025 About AWS: https://cloudjourney.awsstudygroup.com/ About workshop: https://van-hoang-kha.github.io/vi/ 4 - Applied workshop guidelines to draft this worklog - Created an AWS Free Tier account - Explored the AWS Management Console and installed/configured the AWS CLI - Practiced: + Creating an AWS account + Installing \u0026amp; configuring AWS CLI + Basic AWS CLI usage 09/10/2025 09/10/2025 My workshop git: https://github.com/NicolaiHong/InternShipReport_FCJ AWS Console: https://aws.amazon.com/ 5 - Learned foundational EC2 concepts: + Instance types + AMIs + EBS volumes + Storage options - Tested launching an EC2 instance and different SSH connection methods - Reviewed Elastic IP usage 09/11/2025 09/11/2025 EC2 console: https://ap-southeast-1.console.aws.amazon.com/ec2/ Amazon EC2 Basics: https://www.coursera.org/learn/aws-amazon-ec2-basics/ 6 - Additional hands-on practice: + Launch an EC2 instance + Connect via SSH + Attach and manage an EBS volume 09/12/2025 09/12/2025 EC2 console: https://ap-southeast-1.console.aws.amazon.com/ec2/ Week 1 Achievements (ongoing): Gained a clear overview of AWS and became familiar with the main service families:\nCompute Storage Networking Database Others Successfully set up and configured an AWS Free Tier account.\nBecame comfortable with the workshop format and the process of writing worklogs.\nNavigated the AWS Management Console and learned how to locate and operate services through the web UI.\nInstalled and configured the AWS CLI, including:\nAccess Key Secret Key Default region and related settings Built a basic understanding of EC2 fundamentals:\nInstance types ‚Äî evaluating cost vs. performance trade-offs. AMIs ‚Äî using pre-built machine images to launch instances. EBS ‚Äî persistent block storage attached to instances. Storage options ‚Äî selecting appropriate storage for different needs. Elastic IP ‚Äî assigning static public IPs for cloud instances. "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "Real-Time Typing Challenge Platform TypeRush An AWS-Powered Architecture for a Multiplayer Typing App 1. Executive Summary TypeRush is an interactive web application that replicates the core experience of Monkeytype while introducing multiplayer functionality for live competitive typing sessions. Designed as a showcase of real-time web technologies and scalable cloud architecture, the platform combines a modern frontend experience with a serverless backend powered by AWS.\nUsers can create or join typing rooms, compete in real time, and view performance metrics such as Words Per Minute (WPM), accuracy, and rankings. The application leverages React for the frontend, AWS Lambda and API Gateway (WebSocket) for real-time synchronization, and Amazon RDS for player data and Amazon ElastiCache for session data. It also leverages Amazon Bedrock to generate daily challenges. Amazon Cognito secures user access and authentication. The solution demonstrates how cloud-native services can deliver fast, scalable, and cost-efficient real-time applications without the need for traditional server management.\n2. Problem Statement What‚Äôs the Problem? Existing typing platforms are often limited to single-player functionality, lacking real-time multiplayer interaction or open implementation examples. Most multiplayer solutions require dedicated servers or complex infrastructure, making them costly and difficult to maintain. For developers and enthusiasts, there is no accessible, serverless example demonstrating real-time synchronization, event-driven updates, and secure user management in one integrated system.\nThe Solution This project provides a serverless, real-time typing platform that allows multiple users to connect, type simultaneously, and see instant feedback on their progress. It is designed to be lightweight, cost-efficient, and scalable.\nBenefits and Return on Investment The project serves as both a developer learning platform and a user-facing application. It demonstrates best practices in event-driven architecture, WebSocket implementation, and cloud-based scalability at minimal operational cost. For users, it provides an engaging, social typing experience. For developers, it offers an educational reference for building serverless real-time systems.\nThe platform requires no ongoing server maintenance, ensuring:\nLong-term sustainability Rapid scalability Low operational overhead 3. Solution Architecture The platform employs a serverless AWS architecture to manage a real-time typing challenge application. Data is processed and stored using a variety of AWS services, and the frontend is a modern, responsive UI. The architecture is detailed below: AWS Services Used Amazon S3: Hosts the static frontend. CloudFront: Provides a CDN for the frontend and API. AWS WAF: Acts as a firewall for CloudFront. Route 53: Manages domain and DNS routing. Amazon Cognito: Handles user sign-in and sign-up. API Gateway: Serves as the entry point for microservices and connects to private ECS via VPC Link. AWS Lambda: Powers the record and text microservices and the WebSocket API for the game microservice. Amazon RDS: Stores records and leaderboards. DynamoDB: Stores text data. AWS Bedrock (Titan Text G1 Express): Functions as the LLM model for text generation. Amazon SNS: Manages notifications and alarms. AWS CloudWatch: Handles logging and monitoring. AWS Secrets Manager: Stores database and API secrets. Amazon ECS (Fargate): Hosts the core game backend. Elastic Load Balancer: Balances game service traffic. ElastiCache (Redis): Caches real-time game data. CodePipeline: Orchestrates builds and deployments. CodeBuild: Builds Docker images and Lambda services. ECR (Elastic Container Registry): Stores Docker images. GitLab: Triggers pipeline builds. Component Design Frontend Layer: A modern, responsive UI built with React, hosted on Amazon S3, and served via Amazon CloudFront with AWS WAF and Amazon Route 53 for enhanced performance, global content delivery, and robust security. API \u0026amp; Real-Time Communication: AWS API Gateway serves as the unified gateway for both REST and WebSocket APIs, implementing rate limiting, authentication, and request validation. The WebSocket API powers real-time multiplayer interactions. Compute \u0026amp; Business Logic: AWS Lambda handles serverless processing tasks like data persistence and AI-powered text generation. Amazon ECS with AWS Fargate hosts containerized backend services, including the game server. VPC PrivateLink connects API Gateway to ECS Fargate containers, and an Elastic Load Balancer distributes traffic. Data Layer: Amazon RDS stores relational data like user profiles and leaderboards. Amazon DynamoDB manages non-relational data such as dynamically generated text. Amazon ElastiCache (Redis) provides in-memory caching for frequently accessed data. Authentication \u0026amp; User Management: Amazon Cognito handles secure user registration, authentication, and authorization, supporting social and federated login. Amazon Secret Manager secures storage for secrets like API keys. CI/CD \u0026amp; DevOps: AWS CodeBuild and AWS CodePipeline enable continuous integration and continuous deployment (CI/CD) across all application components, automating build, test, and deployment processes. 4. Technical Implementation Implementation Phases This project has two parts‚Äîsetting up the cloud infrastructure and building the application‚Äîeach following 4 phases:\nBuild Theory and Draw Architecture: Research and design the AWS serverless architecture. Calculate Price and Check Practicality: Use the AWS Pricing Calculator to estimate costs and adjust if needed. Fix Architecture for Cost or Solution Fit: Tweak the design to stay cost-effective and usable. Develop, Test, and Deploy: Code the application and AWS services, then test and release to production. Technical Requirements Frontend: Practical knowledge of React. Backend: Experience with AWS services including Lambda, API Gateway, ECS, Fargate, RDS, DynamoDB, and ElastiCache. CI/CD: Familiarity with CodePipeline, CodeBuild, and GitLab. Infrastructure as Code: Use AWS CDK/SDK to code interactions. 5. Timeline \u0026amp; Milestones Project Timeline\nMonth 1: Study all about AWS and AWS services. Month 2: Start planning for the project and the AWS services to implement. Month 3: Develop, implement, and launch. 6. Budget Estimation \u0026mdash;\u0026gt; Budget Estimation File \u0026lt;\u0026mdash;\nInfrastructure Costs AWS Services: Amazon Route 53: $0.90 AWS Web Application Firewall (WAF): $9.03 Amazon CloudFront: $0.40 ($0 with free tier) S3 Standard: $0.11 ($0 with free tier) Data Transfer: $0.00 ($0 with free tier) Amazon API Gateway: $15.92 ($0 with free tier) Amazon Cognito: $0.00 Network Load Balancer: $18.49 AWS Fargate: $8.88 AWS Lambda: $0.02 ($0 with free tier) Amazon ElastiCache: $16.06 Amazon RDS for PostgreSQL: $39.37 ($0 with free tier) AWS Bedrock (Workload 1): $2.63 DynamoDB: $0.03 ($0 with free tier) AWS Secrets Manager: $4.00 AWS CodePipeline: $1.00 ($0 with free tier) AWS CodeBuild: $5.00 ($0 with free tier) With free tier account: about $60/month With paid tier account: about $122/month Both amounts assume 24/7 running services; in reality, it might be much lower.\n7. Risk Assessment Risk Matrix Technical Integration: Medium impact, medium probability. Performance and Scalability: High impact, medium probability. Cost Management: Medium impact, medium probability. Security: High impact, low probability. Data Reliability: Medium impact, low probability. Operational Risks: Low impact, medium probability. Learning Curve: Medium impact, high probability. Mitigation Strategies Technical Integration: This risk is mitigated by phased implementation and adherence to AWS best practices. Performance and Scalability: Optimizing WebSocket communication, leveraging Redis caching, and monitoring with CloudWatch will help maintain responsiveness. Cost Management: These can be controlled through AWS Budgets, usage alerts, and auto-scaling policies. Security: Risks will be minimized using AWS WAF, Cognito, Secrets Manager, and strict IAM policies. Data Reliability: Clear data ownership boundaries and transactional writes will ensure consistency. Operational Risks: Risks in CI/CD pipelines will be reduced with staged deployments and rollback configurations. Learning Curve: The learning curve will be addressed through dedicated study and hands-on practice in the first project phase. Contingency Plans If the service fails: A maintenance page will be displayed to users. If multiplayer is too slow: Multiplayer mode will be temporarily disabled, but single-player will remain active. If costs spike unexpectedly: The change that caused the spike will be immediately reversed. If a new update breaks the site: The system will automatically revert to the previous working version. 8. Expected Outcomes Technical Improvements: Upon completion, the project will deliver a scalable, secure, and fully functional real-time multiplayer typing platform built entirely on AWS.\nLong-term Value: It will demonstrate best practices in serverless architecture, event-driven design, and CI/CD automation. For users, it provides a smooth, engaging multiplayer typing experience. For us, it serves as our first hands-on, practical reference model for building real-time, serverless web applications on AWS with minimal cost and maintenance.\n"
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/4-eventparticipated/4.2-event2/",
	"title": "Data Science On AWS",
	"tags": [],
	"description": "",
	"content": "Summary Report Event Objectives Provide a comprehensive overview of building a modern Data Science system on AWS. Introduce the end-to-end Data Science pipeline, from data processing to model deployment. Offer hands-on experience with key AWS services like AWS Glue and Amazon SageMaker. Discuss practical considerations such as cost, performance, and the benefits of cloud vs. on-premise solutions. Speakers VƒÉn Ho√†ng Kha ‚Äì Cloud Solutions Architect, AWS Community Builder B·∫°ch Do√£n V∆∞∆°ng ‚Äì Cloud Develops Engineer, AWS Community Builder Key Highlights End-to-End Data Science Pipeline on AWS The workshop outlined the complete data science journey on the cloud, using core services:\nAmazon S3: For scalable data storage. AWS Glue: For serverless data integration, ETL (Extract, Transform, Load), and data cleaning. Amazon SageMaker: For building, training, and deploying machine learning models at scale. Practical Demonstrations Demo 1: Data Processing with AWS Glue: Showcased how to process and clean a real-world IMDb dataset, emphasizing the importance of data quality for model accuracy. Demo 2: Model Training with SageMaker: Demonstrated the process of training and deploying a Sentiment Analysis model, making the abstract concepts of ML deployment concrete. Integrating Custom Models: Showcased how to leverage frameworks like TensorFlow and PyTorch within SageMaker, using a sample project from a provided GitHub repository. Broadening AI/ML Capabilities with Managed Services An overview of AWS\u0026rsquo;s pre-built AI services that accelerate development:\nAmazon Transcribe: Speech-to-text conversion. Amazon Comprehend: Natural language processing for sentiment analysis and topic extraction. Amazon Rekognition: Image and video analysis. Amazon Personalize: Building personalized recommendation systems. Key Takeaways Data-First Mindset Business-first approach: Always start with the business context of the data, as emphasized by the need for feature engineering. Data quality is paramount: The workshop stressed that the accuracy of any ML model is directly dependent on the quality of the input data. Data as an asset: Data collection, governance, and security are the foundational pillars of a data-driven organization. Technical Architecture Modular Pipeline: The standard architecture involves a pipeline from S3 (storage) to AWS Glue (ETL) to Amazon SageMaker (ML), allowing for a clean separation of concerns. Flexibility: AWS supports both low-code solutions like SageMaker Canvas and code-intensive custom model training using frameworks like TensorFlow/PyTorch. Serverless benefits: Using services like AWS Glue removes the need for managing infrastructure, allowing teams to focus on data and models. Strategy Phased approach: Start with data collection and cleaning before moving to complex model training. Cloud vs. On-premise: The discussion highlighted that the cloud offers significant advantages in scalability, pay-for-what-you-use cost models, and access to powerful computing resources without upfront investment. ROI Measurement: Leverage cloud benefits to reduce development time and infrastructure overhead, leading to faster time-to-market for AI-powered features. Applying to Work Automate ETL: Use AWS Glue to create automated data cleaning and preparation jobs for analytics and ML. Adopt SageMaker: Pilot Amazon SageMaker for training and deploying ML models to streamline the MLOps lifecycle. Implement Sentiment Analysis: Apply the concepts from the demo to analyze customer feedback from reviews or support tickets. Explore Pre-built AI: Integrate services like Amazon Rekognition for content moderation or Amazon Transcribe for call center analytics. Consolidate Knowledge: Build a small project based on the workshop\u0026rsquo;s guidance to reinforce the concepts learned. Event Experience Attending the \u0026ldquo;Data Science on AWS\u0026rdquo; workshop provided a valuable, hands-on journey into cloud-based machine learning. Key experiences included:\nLearning from highly skilled speakers The speakers, both AWS Community Builders, shared practical insights and best practices from their real-world experience. Hands-on technical exposure The live demos of processing data with AWS Glue and training a model with SageMaker were extremely effective at translating theory into practice. Leveraging modern tools Explored the comprehensive AWS ecosystem for data science, understanding how different services fit together to form a cohesive pipeline. Learned about both fully managed AI services and the powerful customization options available within SageMaker. Lessons learned A solid data preparation strategy is non-negotiable for success in machine learning. AWS significantly lowers the barrier to entry for building and deploying sophisticated data science systems. Modern cloud platforms provide the flexibility to choose between low-code tools for speed and custom code for specific, complex requirements. Some event photos Overall, the workshop provided not only technical knowledge but also practical experience with building end-to-end data science pipelines on AWS, emphasizing the importance of data quality and the power of cloud-native ML services.\n"
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/5-workshop/5.2-serverless-backend/",
	"title": "Part 2: Backend Deployment with API Gateway, Lambda, RDS, and Cognito",
	"tags": [],
	"description": "",
	"content": "Introduction Welcome to the second part in our workshop building serverless application series! In this hands-on session, you\u0026rsquo;ll build a complete, secure backend infrastructure for your serverless application using AWS managed services.\nYou will then learn how to connect our serverless backend to frontend that we previously built in Part 1\nModern applications require robust, scalable backend systems that can handle business logic, manage data securely, and authenticate users. In this workshop, you\u0026rsquo;ll create a fully serverless backend architecture using AWS Lambda for compute, Amazon RDS for data persistence, API Gateway for RESTful APIs, Amazon Cognito for user authentication, and AWS Secrets Manager for secure credential management.\nWhat You\u0026rsquo;ll Build By the end of this part, you\u0026rsquo;ll have deployed a complete backend infrastructure featuring:\nRESTful APIs: API Gateway endpoints that handle HTTP requests and route them to appropriate Lambda functions Serverless Computing: Lambda functions written in Node.js/Python that execute your business logic Managed Database: Amazon RDS PostgreSQL database for reliable data storage User Authentication: Amazon Cognito user pools for sign-up, sign-in, and user management API Security: Cognito authorizers protecting your API endpoints. Furthermore, enforce API Gateway to only accept requests originated from CloudFront Secrets Management: AWS Secrets Manager securely storing database credentials and generate a secure value to embed to CloudFront custom header to restrict access to the API Gateway Network Security: VPC configuration isolating your database from public internet Why This Architecture? This serverless backend architecture offers several key benefits:\nNo Server Management: AWS handles provisioning, patching, and scaling Pay-Per-Use: Only pay for actual compute time and storage used Auto-Scaling: Automatically handles traffic spikes from 0 to thousands of requests Built-in Security: Multiple layers including IAM, Cognito, VPC, and secrets management Developer Productivity: Focus on code, not infrastructure Workshop Duration Estimated Time: 3-4 hours\nVPC and networking setup: 20 minutes RDS database creation: 30 minutes Secrets Manager configuration: 15 minutes Lambda function development: 45 minutes API Gateway setup: 40 minutes Cognito configuration: 45 minutes Integration and testing: 45 minutes Workshop Structure This workshop is divided into the following sections:\nPart 1: VPC and Network Setup Create VPC for database isolation "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/5-workshop/5.1-frontend-deployment/5.1.2-s3-hosting/",
	"title": "S3 Static Website Hosting",
	"tags": [],
	"description": "",
	"content": "Overview In this section, you\u0026rsquo;ll set up Amazon S3 to host your static website. S3 (Simple Storage Service) provides a cost-effective and highly durable solution for hosting static content like HTML, CSS, JavaScript, and images.\nWhat you\u0026rsquo;ll accomplish:\nClone the sample application repository Build the frontend application Create and configure an S3 bucket Upload your website files to S3 Enable static website hosting Test your website Estimated time: 30 minutes\nCosts Considerations Free-tier: S3 does not have free-tier benefits Paid-tier Even paid tier cost is minimal for our workshop Overall: \u0026lt;$0 (clean up immediately after finish workshop) Step 1: Prepare Your Application 1.1 Clone the Sample Repository Open your terminal or command prompt and run:\ngit clone https://github.com/Icyretsz/fcj-workshop-serverless-frontend.git 1.2 Install Dependencies The sample application uses Node.js and npm. Install the required dependencies:\nnpm install Troubleshooting:\nIf you don\u0026rsquo;t have Node.js installed, download it from https://nodejs.org/ (LTS version recommended) Verify installation: node --version and npm --version Minimum required versions: Node.js 16.x or higher 1.3 Build the Application Navigate to the root directory and build the production-ready version:\nnpm run build The command will:\nThe build process compiled your source code Optimized assets for production (minification, bundling) Created a dist/ directory with all deployable files 1.4 Verify Build Output Check the contents of your build directory:\nls -la dist/ You should see:\ndist/\r‚îú‚îÄ‚îÄ index.html\r‚îú‚îÄ‚îÄ favicon.ico\r‚îú‚îÄ‚îÄ static/\r‚îÇ ‚îú‚îÄ‚îÄ css/\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ main.def456.css\r‚îÇ ‚îî‚îÄ‚îÄ js/\r‚îÇ ‚îî‚îÄ‚îÄ main.abc123.js\r‚îî‚îÄ‚îÄ assets/\r‚îî‚îÄ‚îÄ images/ Step 2: Create an S3 Bucket 2.1 Navigate to S3 Console Log in to the AWS Management Console In the search bar at the top, type \u0026ldquo;S3\u0026rdquo; Click on S3 under Services 2.2 Create New Bucket Click the Create bucket button Configure the following settings: General Configuration:\nBucket name: workshop-frontend-[your-name]-[random-string] Example: workshop-frontend-john-a1b2c3 Must be globally unique across all AWS accounts Use only lowercase letters, numbers, and hyphens Note: Write down your bucket name - you\u0026rsquo;ll need it throughout the workshop 2.3 Configure Bucket Settings Object Ownership:\nKeep default: ACLs disabled (recommended) Block Public Access settings:\nUNCHECK \u0026ldquo;Block all public access\u0026rdquo; Check the acknowledgment box: \u0026ldquo;I acknowledge that the current settings might result in this bucket and the objects within becoming public\u0026rdquo; ‚ö†Ô∏è Security Note: We\u0026rsquo;re making this bucket public for website hosting. In production, you\u0026rsquo;d use CloudFront to access the bucket privately, which we\u0026rsquo;ll configure in Part 2.\nBucket Versioning:\nKeep default: Disable Tags (Optional but recommended):\nKey: Project, Value: ServerlessWorkshop Key: Workshop, Value: Frontend Default encryption:\nKeep default: Server-side encryption with Amazon S3 managed keys (SSE-S3) Advanced settings:\nKeep all defaults 2.4 Create the Bucket Scroll to the bottom and click Create bucket You should see a success message: \u0026ldquo;Successfully created bucket \u0026lsquo;workshop-frontend-[your-name]-[random-string]\u0026rsquo;\u0026rdquo; Step 3: Configure Static Website Hosting 3.1 Enable Website Hosting Click on your newly created bucket name Navigate to the Properties tab Scroll down to Static website hosting section Click Edit 3.2 Configure Hosting Settings Static website hosting:\nSelect Enable Hosting type:\nSelect Host a static website Index document:\nEnter: index.html Error document (Optional):\nEnter: index.html This allows client-side routing to work properly (for React, Vue, Angular apps) Click Save changes 3.3 View website endpoint Scroll back down to Static website hosting section Copy the Bucket website endpoint URL Example: http://workshop-frontend-john-a1b2c3.s3-website-us-east-1.amazonaws.com Save this URL - you\u0026rsquo;ll use it to test the website Step 4: Upload Website Files 4.1 Upload via AWS Console Navigate to the Objects tab in your bucket Click Upload 4.2 Add Files Option A: Drag and Drop\nOpen your file explorer/finder Navigate to your frontend/dist/ directory Select ALL files and folders inside the build directory Drag them into the upload area Option B: Browse Files\nClick Add files and Add folder Navigate to frontend/dist/ Select all contents ‚ö†Ô∏è Important: Upload the contents of the dist folder, not the dist folder itself. Your S3 bucket root should have index.html at the top level.\n4.3 Configure Upload Settings Permissions:\nKeep defaults (inherited from bucket) Properties:\nKeep defaults Storage class:\nKeep default: Standard 4.4 Complete Upload Scroll down and click Upload Wait for the upload to complete You should see \u0026ldquo;Upload succeeded\u0026rdquo; with a list of uploaded files Click Close 4.5 Verify Upload Back in your bucket, you should now see:\nindex.html favicon.ico static/ folder assets/ folder (if applicable) Step 5: Configure Bucket Policy for Public Access 5.1 Create Bucket Policy Navigate to the Permissions tab Scroll down to Bucket policy section Click Edit 5.2 Add Policy JSON Copy and paste the following policy, replacing YOUR-BUCKET-NAME with your actual bucket name:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;PublicReadGetObject\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::YOUR-BUCKET-NAME/*\u0026#34; } ] } Example with actual bucket name:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;PublicReadGetObject\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::workshop-frontend-thien-bucket/*\u0026#34; } ] } What this policy does:\nAllows anyone (\u0026quot;Principal\u0026quot;: \u0026quot;*\u0026quot;) to read (s3:GetObject) any object in your bucket Required for public website hosting 5.3 Save Policy Click Save changes Step 6: Test Your Website 6.1 Access Your Website Use the Bucket website endpoint URL you saved earlier Open it in your web browser Example: http://workshop-frontend-john-a1b2c3.s3-website-us-east-1.amazonaws.com Expected result:\nThe website should load successfully You should see the homepage of the application 6.2 Test Navigation Click through different pages of your application Verify images and styles load correctly Check browser developer console for any errors (F12 or right-click ‚Üí Inspect) 6.3 Test Direct Object Access You can also access individual files directly:\nhttp://your-bucket-endpoint/index.html http://your-bucket-endpoint /css/main.def456.css Troubleshooting Issue: \u0026ldquo;403 Forbidden\u0026rdquo; Error Cause: Bucket policy not configured correctly or bucket not public\nSolution:\nVerify bucket policy is correct and saved Check that \u0026ldquo;Block all public access\u0026rdquo; is disabled Ensure the Resource ARN in the policy matches your bucket name Clear your browser cache and try again Issue: \u0026ldquo;404 Not Found\u0026rdquo; Error Cause: File not uploaded or incorrect URL\nSolution:\nVerify files are in the bucket root (not in a subfolder) Check that index.html exists at the bucket root Verify the website endpoint URL is correct Try accessing http://your-endpoint/index.html directly Issue: Styles or JavaScript Not Loading Cause: Incorrect content types or file paths\nSolution:\nCheck browser console for 404 errors Verify file paths in your HTML match the uploaded structure If using CLI, ensure content types are set correctly Check that all files from the build directory were uploaded Issue: \u0026ldquo;Bucket name already exists\u0026rdquo; Cause: S3 bucket names are globally unique\nSolution:\nChoose a different bucket name Add more random characters or your AWS account ID Example: workshop-frontend-john-20241126-a1b2c3 Summary Congratulations! You\u0026rsquo;ve successfully:\nCloned and built the sample application Created an S3 bucket Configured static website hosting Uploaded your website files Set up public access with bucket policy Tested the live website What You\u0026rsquo;ve Deployed The website is now:\nHosted on AWS S3 Publicly accessible via the S3 website endpoint Serving static files (HTML, CSS, JavaScript) Ready to be integrated with CloudFront in the next section Current Limitations Right now, the website:\nOnly uses HTTP (not HTTPS) Has no CDN/caching for global users Has no DDoS protection or WAF Uses a non-memorable S3 endpoint URL In the next sections, we\u0026rsquo;ll address these by adding CloudFront and WAF.\nNext Steps Proceed to Part 2: CloudFront Distribution Setup to add global content delivery and improve performance.\nQuick Reference Your Resources:\nBucket Name: _______________________________ Website Endpoint: _______________________________ Region: _______________________________ Useful Commands (AWS CLI):\n# Update website content cd frontend npm run build aws s3 sync ./build/ s3://YOUR-BUCKET-NAME/ --delete # List bucket contents aws s3 ls s3://YOUR-BUCKET-NAME/ --recursive # Delete all objects (for cleanup) aws s3 rm s3://YOUR-BUCKET-NAME/ --recursive Ready to continue? Let\u0026rsquo;s move on to adding CloudFront for global content delivery!\n"
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/5-workshop/5.2-serverless-backend/5.2.2-vpc/",
	"title": "VPC and Network Setup",
	"tags": [],
	"description": "",
	"content": "Overview In this section, you\u0026rsquo;ll create the network foundation for your serverless backend. You\u0026rsquo;ll set up a Virtual Private Cloud (VPC) with private subnets to securely host your RDS database and Lambda functions, ensuring your data layer is isolated from direct internet access.\nWhat you\u0026rsquo;ll accomplish:\nUnderstand VPC architecture for serverless applications Create a custom VPC with proper CIDR configuration Set up private subnets in different availability zones Configure security groups for database access Establish internet connectivity for Lambda functions Prepare network infrastructure for RDS and Lambda deployment Estimated time: 30 minutes\nWhy VPC for Serverless Backend? Security Benefits Private Subnets:\nDatabase not accessible from internet Lambda functions communicate privately with RDS Additional layer of defense against attacks Network Isolation:\nSeparate your database from public internet Control traffic flow with security groups Meet compliance requirements for data protection Architecture Pattern Internet\r‚Üì\rNAT Gateway (for Lambda outbound)\r‚Üì\rPrivate Subnet (Lambda Functions)\r‚Üì\rPrivate Subnet (RDS Database)\r‚îî‚îÄ‚îÄ Isolated, no direct internet access Lambda and VPC:\nBy default, Lambda functions run in an AWS-managed VPC with internet access. When you attach Lambda to your custom VPC:\nLambda can access resources in private subnets (like RDS)\nLambda loses direct internet access by default\nRequired for connecting to databases in VPC\nUnderstanding the Network Architecture What We\u0026rsquo;ll Build VPC (10.0.0.0/16)\r‚îú‚îÄ‚îÄ Private Subnet 1 (10.0.128.0/20) - AZ: ap-southeast-1a\r‚îÇ ‚îî‚îÄ‚îÄ For Lambda Functions\r‚îÇ\r‚îú‚îÄ‚îÄ Private Subnet 2 (10.0.144.0/20) - AZ: ap-southeast-1b\r‚îÇ ‚îî‚îÄ‚îÄ No use for this subnet\r‚îÇ\r‚îú‚îÄ‚îÄ Private Subnet 3 (10.0.160.0/20) - AZ: ap-southeast-1a\r‚îÇ ‚îî‚îÄ‚îÄ For RDS Database (multi-AZ requirement)\r‚îÇ\r‚îú‚îÄ‚îÄ Private Subnet 4 (10.0.176.0/20) - AZ: ap-southeast-1b\r‚îÇ ‚îî‚îÄ‚îÄ For RDS Database (multi-AZ requirement) Why 4 subnets across 2 Availability Zones? Private Subnet 1 is preserved for a Lambda function\nPrivate Subnet 3 and Private Subnet 4 is preserved for an RDS instance.\nIt is required that your VPC must have at least two subnets. These subnets must be in two different Availability Zones in the AWS Region where you want to deploy your DB instance. This ensures high availability and allows RDS to create a Multi-AZ deployment or automatic failover setup.\nWe won\u0026rsquo;t use Private Subnet 2\nStep 1: Create VPC 1.1 Navigate to VPC Console Log in to AWS Management Console In the search bar, type \u0026ldquo;VPC\u0026rdquo; Click on VPC under Services 1.2 Create VPC Click Create VPC VPC settings: Select VPC and more\nName tag: workshop-backend\nIPv4 CIDR block: 10.0.0.0/16\nIPv6 CIDR block: No IPv6 CIDR block\nTenancy: Default\nNumber of Availability Zones (AZs): 2\nNumber of public subnets: 0\nNumber of private subnets: 4\nNAT gateways ($) - updated: None\nVPC Endpoints: None\nVerify your VPC structure under the Preview Click Create VPC\nWait a few seconds for the system to initialize your VPC. When finished, click View VPC\nYou will be redirected to your new VPC console Step 2: Create Security Groups Security groups act as virtual firewalls controlling inbound and outbound traffic.\n2.1 Create Security Group for VPC endpoint This security group is for a VPC endpoint required for our Lambda to access to Secrets Manager to fetch RDS secret. We will set up the endpoint in 5.2.4 AWS Secrets Manager Configuration\nIn the VPC console left navigation, click Security Groups Click Create security group Security group name: workshop-endpoint-sm-sg\nDescription: Security group for Secret Manager endpoint\nVPC: Select workshop-backend-vpc\nInbound rules:\nClick Add rule Type: HTTPS Protocol: TCP Port range: 443 Source: Custom Source: type in our VPC\u0026rsquo;s CIDR 10.0.0.0/16 Outbound rules:\nKeep default: All traffic (0.0.0.0/0) allowed Click Create security group 2.1 Create Security Group for Lambda In the VPC console left navigation, click Security Groups Click Create security group Security group name: workshop-lambda-sg\nDescription: Security group for Lambda functions\nVPC: Select workshop-backend-vpc\nInbound rules:\nClick Add rule Type: HTTPS Protocol: TCP Port range: 443 Source: Custom Source: Select the endpoint security group: workshop-endpoint-sm-sg Outbound rules:\nKeep default: All traffic (0.0.0.0/0) allowed This allows Lambda to: Connect to RDS Call AWS services (Secrets Manager, CloudWatch) Click Create security group 6.2 Create Security Group for RDS Click Create security group Security group name: workshop-rds-sg\nDescription: Security group for RDS database\nVPC: Select workshop-backend-vpc\nInbound rules:\nClick Add rule Type: PostgreSQL Protocol: TCP Port range: 5432 (auto-filled) Source: Custom Source: Select the Lambda security group: workshop-lambda-sg Start typing \u0026ldquo;workshop\u0026rdquo; to filter Select the SG ID (sg-xxxxxxxxxxxxx) What this does: Only allows PostgreSQL connections (port 5432) from resources that have the Lambda security group attached.\nOutbound rules:\nKeep default: All traffic allowed (for database maintenance) Click Create security group Security Group Best Practice:\nWe\u0026rsquo;re using security group references instead of IP addresses:\nMore flexible (no need to update if Lambda IP changes)\nMore secure (only Lambda with the correct SG can access)\nEasier to manage (add more Lambda functions without updating rules)\nStep 3: Verify Network Configuration Verify you have:\nVPC created with 10.0.0.0/16 CIDR 4 private subnets created: Private subnet 1 (10.0.128.0/20) in ap-southeast-1a Private subnet 2 (10.0.144.0/20) in ap-southeast-1b Private subnet 3 (10.0.160.0/20) in ap-southeast-1a Private subnet 4 (10.0.176.0/20) in ap-southeast-1b Lambda security group created (outbound all traffic) RDS security group created (inbound PostgreSQL from Lambda SG) Summary Congratulations! You\u0026rsquo;ve successfully:\nCreated a custom VPC with 4 private subnets with proper CIDR configuration Set up security groups for Lambda and RDS with proper rules Established secure network foundation for serverless backend Next Steps Proceed to Part 2: RDS Database Setup to create your PostgreSQL database within this secure VPC infrastructure.\nReady to continue? Your network foundation is now ready to host secure database and serverless compute resources! üéâ\n"
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: Define and scope the initial typing-game project (key features, microservice boundaries, future matchmaking plans). Build team foundations: create a shared repository, seed the backlog, refine ER diagrams, choose tech stack, and assign ownership. Standardize formulas for WPM and accuracy. Prototype a FastAPI microservice for text generation, sentence assembly, and chat; verify the feasibility of integrating with Bedrock. Configure AWS Budgets and alerting. Gain hands-on experience with core AWS components: Lambda (function URLs), VPC (subnets, gateways, peering vs transit), VPC Flow Logs, and load-balancing concepts. Provision an Amazon RDS instance and design/seed a schema for text retrieval. Refactor the text service to use DB-backed retrieval and benchmark it against the prior API-based approach. Introduce early operational practices: define roles, run benchmarks, and set up basic monitoring for scalability. Tasks carried out this week: Day Task Start Date Completion Date Reference Material 2 - Conducted a team brainstorming session to shape and prioritize the first project + Migrated canvas notes into an initial backlog on the project board + Researched and documented consistent WPM and accuracy calculation methods + Initialized the shared code repository and set up the basic project layout for chosen languages and microservices + Iterated on ER diagram sketches and began drafting database schemas + Assigned lead owners for each microservice to clarify responsibilities 09/15/2025 09/15/2025 3 - Configure AWS Budgets: + Reviewed budget types (Cost, Usage, RI, etc.) + Set a monthly cost threshold + Created the budget in the AWS console + Enabled email/SNS alerts - Build a small web app with AWS Lambda: + Studied Lambda and function URL basics + Implemented a simple \u0026ldquo;Hello World\u0026rdquo; function + Configured the function and associated IAM role + Enabled and tested the function URL endpoint - Experiment with FastAPI for microservices: + Followed the FastAPI tutorial + Set up a local dev environment + Built a proof-of-concept API + Implemented and tested endpoints using the built-in Swagger UI 09/16/2025 09/16/2025 AWS Lambda: https://ap-southeast-1.console.aws.amazon.com/lambda AWS Budgets: https://us-east-1.console.aws.amazon.com/costmanagement/ FastAPI: https://www.coursera.org/learn/packt-mastering-rest-apis-with-fastapi-1xeea/ 4 - Studied AWS networking and security fundamentals: + Reviewed Amazon VPC as an isolated network in AWS + Distinguished public and private subnets and their roles + Learned when to use an Internet Gateway vs. a NAT Gateway + Explored VPC Flow Logs for monitoring and troubleshooting traffic + Compared options for connecting on-premises to AWS: Site-to-Site VPN vs. Direct Connect + Evaluated VPC Peering vs. Transit Gateway use cases + Reviewed Elastic Load Balancing concepts for distributing traffic and ensuring availability 09/17/2025 09/17/2025 Module 02-(01 to 03): https://www.youtube.com/watch?v=O9Ac_vGHquM https://www.youtube.com/watch?v=BPuD1l2hEQ4 https://www.youtube.com/watch?v=CXU8D3kyxIc 5 - Explored the Amazon Bedrock playground: + Reviewed available foundation models (e.g., Claude, Titan) + Selected a candidate model for text generation + Experimented with prompts and tuning parameters + Generated and analyzed sample outputs - Built a microservice prototype: + Designed the service flow and integrated multiple text-generation APIs + Implemented functions for random sentence creation and a chat capability + Reviewed limitations of the initial implementation and defined next steps + Validated the chosen technical approach 09/18/2025 09/18/2025 Amazon Bedrock:https://ap-southeast-1.console.aws.amazon.com/bedrock 6 - Provisioned and configured an Amazon RDS database: + Selected an appropriate engine for the project + Set up instance configuration, credentials, VPC and security group rules + Launched the instance, monitored creation, and recorded the connection endpoint securely - Implemented a DB-driven TextService prototype: + Designed a simple schema with tables for words and sentences + Wrote a one-time script to seed the RDS instance with initial data + Refactored the TextService to retrieve content from the database instead of external APIs and ran a benchmark to compare performance + Measured response-time improvements between the prior API approach and the new DB-backed method 09/19/2025 09/19/2025 Aurora and RDS: https://ap-southeast-1.console.aws.amazon.com/rds Week 2 Achievements: Scoped the first typing game: defined core functionality, microservice boundaries, populated an initial backlog, and assigned ownership. Documented standardized WPM and accuracy calculation methods. Initialized a shared repository with a baseline multi-language project layout. Refined ER diagrams and drafted the initial relational schema. Delivered a FastAPI prototype (text generation, sentence assembly, chat) and validated endpoints through Swagger UI. Configured AWS Budgets with a monthly threshold and alert notifications. Gained practical knowledge of core AWS building blocks: Lambda (function URLs), VPC (subnets, IGW, NAT, Flow Logs), peering vs transit gateway, and load-balancing basics. Evaluated Amazon Bedrock models and validated a candidate model plus prompting strategy. Launched an RDS instance, created the schema, and loaded a seed dataset. Refactored the TextService to use DB-based retrieval and observed an initial performance improvement in benchmarks. Established early operational practices: role assignments, benchmarking focus, and initial scalability considerations. "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 12 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 12 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/4-eventparticipated/4.3-event3/",
	"title": "Reinventing DevOps with AWS Generative AI",
	"tags": [],
	"description": "",
	"content": "Summary Report Event Objectives Share the current context of DevOps and the impact of Generative AI. Present real-world case studies and practical frameworks for integrating AI into DevOps. Provide a live demonstration of AWS Generative AI tools that enhance the development lifecycle. Discuss the evolving role of DevOps engineers and the skills needed for the future. Speakers Le Thanh Duc ‚Äì Cloud Delivery Manager, CMC Global Du Quoc Thanh ‚Äì Technical Leader, CMC Global Van Hoang Kha ‚Äì Cloud Engineer, AWS Community Builder Key Highlights The Evolution from DevOps to DevSecOps The modern DevOps mindset integrates security from the start, making it a shared responsibility across development, security, and operations teams. Shifting from a reactive, post-development security model to a proactive approach where security is embedded in every phase. This cultural shift is the core of DevSecOps, aiming to balance development speed with system security. Phases of a Secure DevOps Lifecycle A comprehensive, seven-phase approach to embedding security throughout the pipeline:\nPlan: Define security requirements and perform threat modeling. Code: Use static analysis (SAST) tools to detect vulnerabilities early. Build: Automate security checks, dependency scans, and configuration validation. Test: Integrate penetration testing and compliance auditing. Deploy: Scan Infrastructure as Code (IaC) to ensure secure environments. Operate: Automate patching, incident response, and remediation. Monitor: Use real-time analytics and AI-powered anomaly detection for proactive defense. Leveraging AI in the DevOps Toolchain Automation: AI automates repetitive tasks like code review, log analysis, vulnerability scanning, and filtering false positives. Enhanced Security: AI-driven tools can prioritize critical risks, suggest fixes, and detect anomalous behavior in runtime environments. Efficiency: AI assists in generating documentation, reports, and compliance policies, reducing manual workload. Tooling Examples: The session highlighted tools like SonarQube, Checkov, Prometheus, and GitHub Actions, along with AI\u0026rsquo;s role in enhancing their capabilities. AWS Tools for AI-Enhanced DevOps Amazon CodeGuru: A service demonstrated to scan code for vulnerabilities (e.g., SQL injection, secret leaks) and provide actionable recommendations for fixes. AWS Managed Control Plane (MCP) \u0026amp; Base (MCB): Tools for automating security compliance and updates for Terraform and Kubernetes (EKS) configurations. Cost Optimization: AI/ML services like AWS Cost Anomaly Detection and Compute Optimizer help predict resource needs and reduce waste. Key Takeaways Security Mindset Proactive Integration: Always start with security in mind, embedding it into the earliest stages of planning and development, not as an afterthought. Shared Responsibility: Foster a culture where developers, operations, and security teams are collectively responsible for security. Continuous Improvement: Use feedback loops from monitoring and incidents to continuously refine security processes. Technical Architecture Automated Security Pipeline: Embed automated security checks at every stage of the CI/CD pipeline, from code scanning to deployment. Observability: Implement robust monitoring, logging, and alerting systems (e.g., Prometheus, Grafana, Loki) to gain real-time insights into system health and security. Infrastructure as Code (IaC) Security: Utilize tools to scan IaC configurations to prevent misconfigurations before they reach production. AI Integration Strategy Phased Approach: Select and adopt AI tools that fit specific project needs to avoid performance overhead and unnecessary complexity. Human-in-the-Loop: View AI as a powerful assistant that enhances human capabilities, not as a replacement. Human oversight and judgment remain critical. Measure ROI: The integration of AI should be measured by its ability to increase development velocity, improve security posture, and reduce manual effort. Applying to Work Enhance CI/CD: Integrate automated static analysis (SAST) and dependency scanning tools into current pipelines. Adopt IaC Scanning: Implement tools like Checkov to validate Terraform or other IaC scripts. Pilot AWS AI Tools: Experiment with Amazon CodeGuru on a small-scale project to review code quality and security. Improve Monitoring: Leverage AI-powered anomaly detection to get proactive alerts on potential issues. Automate Documentation: Use AI to assist in generating and maintaining project documentation and reports. Event Experience Attending the \u0026ldquo;Reinventing DevOps with AWS Generative AI\u0026rdquo; session was highly valuable, offering a comprehensive overview of how AI is reshaping security and efficiency in software development. Key experiences included:\nLearning from highly skilled speakers Experts from CMC Global and AWS Vietnam shared deep insights from their extensive experience in cloud and DevOps. Through real-world case studies from clients in the Philippines and Singapore, I gained a practical understanding of implementing secure CI/CD pipelines. Hands-on technical exposure The live demonstration of Amazon CodeGuru was particularly insightful, showing how AI can concretely identify vulnerabilities and suggest code fixes in real-time. Leveraging modern tools Explored a modern DevOps toolchain, including SonarQube, Checkov, Prometheus, and GitLab CI, and understood how AI integrates with them. Learned how to use AI for infrastructure management and compliance with tools like AWS MCP and MCB. Networking and discussions The interactive Q\u0026amp;A session offered a chance to dive deeper into specific topics, such as the evolution of DevOps roles, AI\u0026rsquo;s limitations, and career advice for cloud architects. Discussions reinforced the importance of balancing AI automation with human expertise and critical thinking. Lessons learned Shifting to a DevSecOps culture is essential for building secure and reliable applications at speed. AI tools like Amazon CodeGuru can significantly boost productivity and security, but they require human oversight to verify and implement suggestions effectively. Modernization requires a clear strategy; a phased approach to adopting new tools and processes is less risky and more effective. Overall, the event provided not only technical knowledge but also reshaped my thinking about the future of DevOps, the indispensable role of integrated security, and the collaborative potential between AI and human engineers.\n"
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives: Complete essential AWS hands-on labs, including Site-to-Site VPN and core EC2 operations. Work through and finish all four modules of the AWS Cloud Technical Essentials course. Improve proficiency with AWS Console and CLI (credentials, key pairs, region/service navigation). Collaborate with product and design teams to analyze and document TypeRush UI/UX from Figma. Assess storage options and finalize the decision to adopt a scalable NoSQL model for TextService. Prototype MongoDB integration (environment setup, data seeding, service refactoring, validation). Establish consistent and effective communication routines with the First Cloud Journey team. Tasks carried out this week: Day Task Start Date Completion Date Reference Material 2 - Lab 03: AWS Site-to-Site VPN: + Built a complete Site-to-Site VPN setup, including a new VPC, a customer gateway EC2 instance, a Virtual Private Gateway, and the VPN connection. + Configured the tunnel and validated successful end-to-end connectivity. - Lab 04: Amazon EC2 Fundamentals: + Launched and connected to both Windows Server and Amazon Linux EC2 instances. + Deployed a sample ‚ÄúAWS User Management‚Äù CRUD application on both platforms. + Explored core EC2 capabilities such as instance resizing, EBS snapshot management, and building custom AMIs. 09/22/2025 09/22/2025 VPN Lab (Lab 03): https://000003.awsstudygroup.com/ EC2 Lab (Lab 04): https://000004.awsstudygroup.com/ 3 - Started the AWS Cloud Technical Essentials course and completed the first 2 modules: + Module 1: Cloud Foundations \u0026amp; IAM - Defined cloud computing and its value proposition. - Compared on-premises workloads with cloud workloads. - Created an AWS account and explored different interaction methods (Console, CLI, SDK). - Studied the AWS Global Infrastructure (Regions, Availability Zones). - Learned and applied IAM best practices. + Module 2: Compute \u0026amp; Networking - Reviewed EC2 architecture components. - Differentiated containers vs virtual machines. - Explored serverless technologies and their use cases. - Studied core VPC networking concepts and created a custom VPC. 09/23/2025 09/23/2025 AWS Cloud Technical Essentials: https://www.coursera.org/learn/aws-cloud-technical-essentials 4 - Collaborated with design to document the TypeRush UI/UX: + Participated in a cross-functional review session to evaluate the latest Figma flows. + Analyzed major screens (login, game, score summary, settings) to understand layout, hierarchy, and interaction patterns. + Listed technical feasibility questions and UI considerations for further alignment. + Began translating the designs into early component requirements and user stories. - Discussed TextService storage approach with the team lead: + Compared relational and non-relational storage models for word/sentence data. + Presented pros and cons for each approach based on usage patterns. + Finalized the decision to adopt a NoSQL solution due to dynamic schema needs and scalability. 09/24/2025 09/24/2025 5 - Integrated and tested MongoDB for the TextService prototype: + Set up a MongoDB environment using Docker. + Updated the data seeding script to insert word/sentence documents into MongoDB collections. + Refactored TextService logic to read/write data through MongoDB queries. + Performed full integration testing to ensure connectivity and correct data operations. 09/25/2025 09/25/2025 6 - Completed the remaining AWS Cloud Technical Essentials modules: + Module 3: Storage \u0026amp; Databases - Compared file, block, and object storage types. - Studied Amazon S3 concepts and created an S3 bucket. - Reviewed EBS usage with EC2. - Explored AWS database services and created a DynamoDB table. + Module 4: Monitoring \u0026amp; High Availability - Understood CloudWatch monitoring and alarm basics. - Learned cost/performance optimization techniques. - Studied Elastic Load Balancing for traffic routing. - Compared vertical vs horizontal scaling and built a high-availability setup. 09/26/2025 09/26/2025 AWS Cloud Technical Essentials: https://www.coursera.org/learn/aws-cloud-technical-essentials Week 3 Achievements: AWS Hands-on Labs Completed:\nBuilt a full Site-to-Site VPN setup (VPC, customer gateway EC2, virtual private gateway, tunnel configuration). Completed EC2 fundamentals including Windows/Linux instances, CRUD app deployment, snapshot handling, and custom AMIs. Completed all 4 modules of AWS Cloud Technical Essentials\n(Cloud Foundations/IAM, Compute \u0026amp; Networking, Storage \u0026amp; Databases, Monitoring \u0026amp; High Availability).\nImproved AWS Console \u0026amp; CLI Practice:\nAccount setup and IAM credential management. Navigated services and regions efficiently. Worked with key pairs and resource inspection commands. TypeRush UI/UX Documentation:\nReviewed Figma flows and major interface screens. Captured component behaviors and feasibility questions. Drafted early user stories and component specifications. TextService Storage Strategy:\nEvaluated SQL vs NoSQL approaches. Chose NoSQL for improved flexibility and scalability. MongoDB Prototype Implementation:\nSet up MongoDB via Docker. Updated seeding scripts. Refactored service logic to use MongoDB. Verified smooth end-to-end read/write operations. "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/5-workshop/5.1-frontend-deployment/5.1.3-cloudfront-setup/",
	"title": "CloudFront Distribution Setup",
	"tags": [],
	"description": "",
	"content": "Overview In this section, you\u0026rsquo;ll configure Amazon CloudFront as a Content Delivery Network (CDN) in front of your S3 bucket. CloudFront will cache your content at edge locations around the world, providing faster load times for users regardless of their geographic location.\nWhat you\u0026rsquo;ll accomplish:\nCreate a CloudFront distribution Configure your S3 bucket as the origin Set up cache behaviors and optimization Configure SSL/TLS with HTTPS Update S3 bucket to restrict direct access Test your CloudFront distribution Estimated time: 45 minutes\nWhy CloudFront? Benefits over direct S3 access:\nPerformance: Content served from edge locations near your users (200+ locations worldwide) Security: HTTPS support, DDoS protection, and integration with AWS WAF Cost: Reduced S3 data transfer costs through caching Scalability: Handles traffic spikes automatically Custom Domains: Use your own domain name with SSL certificate Costs Considerations Free-tier: $0/month Paid-tier $15/month Overall: \u0026lt;$0 or \u0026lt;$1 if Pro (clean up immediately after finish workshop) Step 1: Create CloudFront Distribution 1.1 Navigate to CloudFront Console In the AWS Console search bar, type \u0026ldquo;CloudFront\u0026rdquo; Click on CloudFront under Services Click Create distribution 1.2 Step 1: Choose a plan For this workshop, we will continue with the free plan. Click Next 1.3 Step 2: Get started On this step, we will configure the name of the distribution Distribution name:\nSet as: workshop-frontend-cf Route 53 managed domain (optional):\nIf you already have a Route 53 managed domain, you can specify it here to use the domain instead of CloudFront\u0026rsquo;s generated domain\nLeave the rest as default\nClick Next\n1.4 Step 3: Specify origin Origin type:\nSelect Amazon S3 as origin Origin:\nClick on the Browse S3 button On the appeared modal, select the S3 bucket you created earlier in S3 Static Website Hosting tutorial. Then click Choose You will notice that there will be a warning appear. This is because we have configured our S3 Bucket for static website hosting. You should ignore it as we will disable S3 static website hosting in later steps Understanding the Warning:\nIn S3 Static Website Hosting, we configured the bucket for public access to enable S3 static website hosting. CloudFront has detected this configuration. That is why when you click the Use website endpoint, the URL in the textbox will appear in this format: [your-bucket-name].s3-website-[region].amazonaws.com, this is called the S3 website endpoint.\nWhile S3 static website hosting works well for direct access, it requires public bucket permissions that expose your content to anyone on the internet. A more secure approach is to keep your S3 bucket private and serve content exclusively through CloudFront using Origin Access Control (OAC). Using OAC requires the origin URL as the bucket endpoint, which has this format: [your-bucket-name].s3.[region].amazonaws.com\nWhy OAC is better:\nS3 bucket remains private (no public access) Content only accessible via CloudFront Better security posture Same functionality with improved protection We\u0026rsquo;ll implement this secure configuration in the following steps by switching from the S3 website endpoint to OAC, then disabling public access to the bucket.\nConfirm that the S3 origin URL has this format:\nFormat: [your-bucket-name].s3.[region].amazonaws.com Example: workshop-frontend-thien-bucket.s3.ap-southeast-1.amazonaws.com Origin path: Leave empty\nSettings:\nBe sure to check Allow private S3 bucket access to CloudFront - Recommended as this will allow OAC Leave the rest as default Click Next\n1.5 Step 4: Enable security By default, Web Application Firewall (WAF) is enabled. You should leave the settings in this step as they are. We will configure WAF in more details in later part of this workshop.\nClick Next\n1.6 Step 5: Review and create Review your CloudFront settings Click Create distribution when you are ready You will be redirected to the results page ‚è±Ô∏è Wait Time: CloudFront distribution deployment takes 5-15 minutes. While waiting, let\u0026rsquo;s proceed with configuring our S3 Bucket to support OAC\nStep 2: Secure Your S3 Bucket (Restrict Direct Access) Now that CloudFront is serving your content, let\u0026rsquo;s prevent users from bypassing CloudFront and accessing S3 directly.\n2.1 Navigate to Your S3 Bucket Go to S3 console Click on your bucket name Go to Permissions tab 2.2 Block Public Access Scroll to Block public access (bucket settings) Click Edit Check all four options: Block public access to buckets and objects granted through new access control lists (ACLs) Block public access to buckets and objects granted through any access control lists (ACLs) Block public access to buckets and objects granted through new public bucket or access point policies Block public access to buckets and objects granted through any public bucket or access point policies Click Save changes Type confirm when prompted Click Confirm 2.3 Update Bucket Policy Scroll to Bucket policy You will notice that permissions for CloudFront-only access policy was automatically created. Those permissions allow CloudFront to access our S3 Bucket We should now remove the PublicReadGetObject permission we created in S3 Static Website Hosting\nClick Edit\nDelete this statement:\n{ \u0026#34;Sid\u0026#34;: \u0026#34;PublicReadGetObject\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::[your-bucket-name]/*\u0026#34; } Click Save changes 2.4 Disable S3 Static Website Hosting From your Bucket, go to Properties tab Scroll down to Static website hosting section, click Edit Under Static website hosting, check Disable Click Save changes With this change, we will no longer be able to access our website via S3 website endpoint in S3 Static Website Hosting\nStep 3: View CloudFront Origin Settings (Optional) If you want, you can view CloudFront\u0026rsquo;s origin settings to better understand how CloudFront connect to our S3 Bucket\nGo to CloudFront console -\u0026gt; Distributions Select your newly-created distribution in the list Go to Origins tab Under Origins, select your origin S3 bucket. Then click Edit In the next screen, you will see the Origin\u0026rsquo;s settings. There are several notable settings: Origin domain: your S3 Bucket Endpoint Origin access: access method to the origin. Here you should see the option Origin access control settings (recommended) selected Origin access control: you should see an OAC created by CloudFront pre-selected. It should have name follow this format: oac-[your-bucket-name].s3.[region]-[random-string] You may also see the message \u0026ldquo;You must allow access to CloudFront using this policy statement. Learn more about giving CloudFront permission to access the S3 bucket\u0026rdquo;. This message means that you must add required policy statements in your S3 Bucket for CloudFront to access the S3 bucket you created in S3 Static Website Hosting tutorial to host the sample website. But as of 26/11/2025, in the CloudFront creation process, particularly 1.4 Step 3: Specify origin, we selected Allow private S3 bucket access to CloudFront - Recommended. This option will automatically insert required policy statements for CloudFront to access the S3 bucket, as we already saw in 2.3 Update Bucket Policy. So you can ignore this message.\nStep 3: Test Your CloudFront Distribution 3.1 Access via CloudFront Domain Go back to your Distribution\u0026rsquo;s page, on the top panel you will see your website\u0026rsquo;s endpoint under Distribution domain name. You can access this endpoint to go to you website Open a new browser tab Navigate to: https://[your-cloudfront-domain].cloudfront.net Example: https://d1234abcd.cloudfront.net Note: Use HTTPS, not HTTP Expected result:\nYour website loads successfully Browser shows connection is secure (HTTPS) Content loads from CloudFront, not S3 directly 3.2 Verify Direct S3 Access is Blocked Try accessing your old S3 website endpoint:\nExample: http://workshop-frontend-john-a1b2c3.s3-website-us-east-1.amazonaws.com Expected result:\n404 Not Found error This confirms S3 is now protected, the S3 Website Endpoint is no longer exists and our website is now only accessible via CloudFront 3.3 Test HTTPS Redirect Try accessing with HTTP: http://[your-cloudfront-domain].cloudfront.net Expected result:\nAutomatically redirects to HTTPS Browser URL changes to https://... 3.4 Check Response Headers Open browser Developer Tools (F12) Go to Network tab Refresh the page Click on the first request (usually the document) Look at Response Headers You should see:\nx-amz-cf-id: CloudFront request ID x-cache: Shows cache status (Miss from cloudfront, Hit from cloudfront, etc.) age: Time in seconds the object has been in the cache 3.5 Test From Different Locations (Optional) Use an online tool to test your site from multiple global locations:\nTools:\nhttps://www.webpagetest.org/ https://tools.pingdom.com/ https://www.dotcom-tools.com/website-speed-test What to look for:\nFast load times from various geographic locations CloudFront serving from nearby edge locations Step 4: Configure Custom Domain with Route 53 (Optional) Skip if you\u0026rsquo;re not using a custom domain.\n4.1 Update DNS Records If the domain name is purchased from an external DNS provider:\nGo to Route 53 console Click on your hosted zone Click Create hosted zone Configure: Domain name: your domain name Type: Public hosted zone Click Create hosted zone You will be redirected to the hosted zone page. Under Records, note the NS type nameservers (total 4 of them) Go to your external DNS provider. Find the Nameservers settings, then add 4 nameservers you got from Route 53 earlier Wait up to 24 hours for changes to propagate. In the meantime, let\u0026rsquo;s set up CloudFront to use the custom domain Go to your newly-created distribution page, under Alternate domain names, click Add domain Under Domains to serve, input the domain name that you purchased. Click Next On the next screen, click on Create certificate. A new SSL certificate will be created for you On the next screen, review changes then click Add domains Now you can access the sample website using the custom domain 4.2 Test Custom Domain Wait 5-10 minutes for DNS propagation Navigate to: https://www.yourdomain.com Expected result:\nYour website loads via your custom domain HTTPS works with your SSL certificate No certificate warnings 4.3 Verify DNS Propagation Use a DNS checker tool:\nhttps://dnschecker.org/ Enter your domain name Check that it resolves to your CloudFront distribution Step 5: Cache Invalidation When you update your website, CloudFront caches the old version. Learn how to clear the cache.\n5.1 Create an Invalidation Go to CloudFront console Click on your distribution ID Go to Invalidations tab Click Create invalidation 5.2 Specify Paths to Invalidate Object paths:\nFor all files:\n/* For specific files:\n/index.html\r/css/*\r/js/* For a single file:\n/index.html Click Create invalidation Note: Invalidations usually complete in 1-2 minutes\n5.3 Invalidation Costs First 1,000 invalidation paths per month: FREE After that: $0.005 per path Best practices:\nUse versioned filenames (e.g., main.abc123.js) to avoid frequent invalidations Invalidate only specific files when possible For complete rebuilds, /* is acceptable Step 6: Performance Verification 6.1 Test Loading Speed Open your website in an incognito/private window Open Developer Tools (F12) Go to Network tab Refresh the page Check the DOMContentLoaded and Load times at the bottom Good benchmarks:\nDOMContentLoaded: \u0026lt; 1 second Full page load: \u0026lt; 2 seconds First Contentful Paint: \u0026lt; 1 second 6.2 Check CloudFront Cache Hit Ratio Go to CloudFront console Click on your distribution Go to Monitoring tab Check the Cache hit rate graph Target: 80%+ cache hit rate after initial traffic\n6.3 Verify Compression In Developer Tools Network tab Click on a CSS or JS file request Look at Response Headers Verify content-encoding: gzip or content-encoding: br (brotli) File size comparison:\nWithout compression: ~100 KB With compression: ~25 KB (75% reduction) Troubleshooting Issue: CloudFront serves old/cached content after update Solution:\nCreate a cache invalidation for affected paths Or, implement cache-busting with versioned filenames Verify your build process generates unique filenames Issue: \u0026ldquo;The request could not be satisfied\u0026rdquo; error Causes:\nCloudFront can\u0026rsquo;t reach S3 origin Origin configuration incorrect S3 bucket policy blocking CloudFront Solution:\nCheck CloudFront origin settings point to correct S3 endpoint Verify S3 bucket policy allows CloudFront access Ensure Origin Access Control is configured correctly Check S3 bucket exists and has content Issue: SSL certificate not showing in CloudFront Solution:\nVerify certificate is in us-east-1 region Check certificate status is Issued (not Pending) Wait a few minutes and refresh the CloudFront page Ensure certificate covers the domains in CNAME settings Issue: Custom domain not working Solution:\nVerify DNS records are correct (CNAME pointing to CloudFront) Check DNS propagation with dnschecker.org Ensure SSL certificate includes your custom domain Wait up to 48 hours for full DNS propagation (usually much faster) Issue: \u0026ldquo;Access Denied\u0026rdquo; when accessing via CloudFront Solution:\nCheck S3 bucket policy includes correct distribution ARN Verify Origin Access Control is created and associated Ensure bucket policy allows CloudFront service principal Try creating a new cache invalidation Issue: Slow initial load, then fast subsequent loads This is expected behavior:\nFirst request: Cache miss, CloudFront fetches from S3 (slower) Subsequent requests: Cache hit, served from edge (fast) This is normal and improves with more traffic Summary Congratulations! You\u0026rsquo;ve successfully:\nCreated a CloudFront distribution Configured S3 as the origin Enabled HTTPS with SSL/TLS Secured S3 to only allow CloudFront access (Optional) Set up a custom domain Tested cache performance Learned cache invalidation What You\u0026rsquo;ve Achieved Your website now has:\nGlobal Distribution: Served from 200+ edge locations worldwide HTTPS Security: Encrypted traffic with SSL/TLS Better Performance: Reduced latency through caching DDoS Protection: Built-in AWS Shield Standard Cost Optimization: Reduced S3 data transfer costs Scalability: Automatic handling of traffic spikes Architecture So Far Internet Users\r‚Üì\rCloudFront (HTTPS)\r‚Üì\rS3 Bucket (Private) What\u0026rsquo;s Next In Part 3, we\u0026rsquo;ll configure AWS WAF to protect your application from:\nSQL injection attacks Cross-site scripting (XSS) Bot traffic and scraping Geographic restrictions Rate limiting Useful AWS CLI Commands:\n# Invalidate entire cache aws cloudfront create-invalidation \\ --distribution-id YOUR-DISTRIBUTION-ID \\ --paths \u0026#34;/*\u0026#34; # Invalidate specific paths aws cloudfront create-invalidation \\ --distribution-id YOUR-DISTRIBUTION-ID \\ --paths \u0026#34;/index.html\u0026#34; \u0026#34; /css/*\u0026#34; # Get distribution status aws cloudfront get-distribution \\ --id YOUR-DISTRIBUTION-ID \\ --query \u0026#34;Distribution.Status\u0026#34; # Update website and invalidate cd frontend npm run build aws s3 sync ./build/ s3://YOUR-BUCKET-NAME/ --delete aws cloudfront create-invalidation \\ --distribution-id YOUR-DISTRIBUTION-ID \\ --paths \u0026#34;/*\u0026#34; Ready to continue? Let\u0026rsquo;s proceed to Part 3: AWS WAF Configuration to add security protection!\n"
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/5-workshop/5.2-serverless-backend/5.2.3-rds/",
	"title": "RDS Database Setup",
	"tags": [],
	"description": "",
	"content": "Overview In this section, you\u0026rsquo;ll create an Amazon RDS PostgreSQL database instance within your VPC. The database will be deployed in a private subnet, making it inaccessible from the internet while still allowing your Lambda functions to connect securely.\nWhat you\u0026rsquo;ll accomplish:\nUnderstand RDS architecture and configuration options Create a DB subnet group for RDS deployment Configure and launch an RDS PostgreSQL instance Set up database parameters and options Create initial database schema and tables Verify database connectivity Understand RDS security and backup settings Estimated time: 35-40 minutes (includes 10-15 min database creation wait time)\nWhy Amazon RDS? Benefits Over Self-Managed Databases Managed Service:\nAutomated backups and point-in-time recovery Automated software patching and updates Monitoring and metrics built-in High availability options (Multi-AZ) Read replicas for scaling reads Operational Simplicity:\nNo server provisioning or OS management Easy scaling of compute and storage Automated failover for Multi-AZ deployments CloudWatch integration for monitoring Cost Efficiency:\nPay only for what you use Reserved instances for production workloads Storage auto-scaling available Understanding RDS Configuration Costs considerations For this workshop, we\u0026rsquo;ll use db.t3.micro:\n2 vCPUs, 1 GB RAM Burstable performance (suitable for dev/test) Free Tier eligible (750 hours/month for first 12 months) $0.017/hour (~$12.41/month) outside Free Tier Overall: $0 (free-tier) or \u0026lt;$2 (if clean up immediately after workshop) Single-AZ vs Multi-AZ Single-AZ (Workshop Setup):\nDatabase in one Availability Zone Lower cost Good for development/testing ~5 minutes downtime for maintenance Multi-AZ (Production):\nSynchronous replication to standby in different AZ Automatic failover (~1-2 minutes) Higher cost (~2x) Better for production workloads Storage Configuration Storage Type: General Purpose SSD (gp3)\nBalance of price and performance Burstable IOPS Suitable for most workloads Allocated Storage: 20 GB\nMinimum for PostgreSQL Free Tier includes 20 GB Can be increased later without downtime Step 1: Create DB Subnet Group RDS requires a DB subnet group that defines which subnets the database can be deployed in.\n1.1 Navigate to RDS Console In AWS Console search bar, type \u0026ldquo;RDS\u0026rdquo; Click on RDS under Services In the left navigation, click Subnet groups 1.2 Create DB Subnet Group Click Create DB subnet group Subnet group details:\nName: workshop-db-subnet-group\nDescription: Subnet group for workshop RDS database\nVPC: Select workshop-backend-vpc\n1.3 Add Subnets Availability Zones:\nSelect 2 availability zones that you create the subnets in the VPC setup in the previous section Select: ap-southeast-1a and ap-southeast-1b Subnets:\nSelect 2 private subnets, each from different Availability Zone: workshop-backend-subnet-private3-ap-southeast-1a workshop-backend-subnet-private4-ap-southeast-1b Why Two Subnets?\nEven though we\u0026rsquo;re deploying a single-AZ database, RDS requires the subnet group to have at least two subnets in different Availability Zones. This is an AWS requirement that:\nAllows easy migration to Multi-AZ later Provides flexibility for read replicas Ensures consistent configuration practices The database will only use one subnet (we\u0026rsquo;ll specify which one during instance creation).\n1.4 Create Subnet Group Click Create You should see success message Subnet group appears in the list with status Complete Step 2: Create RDS PostgreSQL Instance 2.1 Start Database Creation In RDS console left navigation, click Databases Click Create database 2.2 Choose Database Creation Method Database creation method:\nSelect Full configuration Provides full configuration options More control than Easy create 2.3 Engine Options Engine type:\nSelect PostgreSQL Engine Version:\nSelect PostgreSQL 17. (or latest available) Use default unless you need a specific version PostgreSQL Version:\nWe recommend PostgreSQL 17 or later for this workshop as it includes:\nBetter JSON support Improved performance Enhanced security features For production, choose an LTS (Long-Term Support) version and test your application thoroughly before upgrading.\n2.4 Templates Templates:\nSelect Sandbox (or Free tier if applicable) Availability and durability\nSelect Single-AZ DB instance deployment (1 instances) Multi-AZ for Production:\nFor production workloads, enable Multi-AZ deployment:\nAutomatic failover to standby instance\nSynchronous replication\n~1-2 minute failover time\n~2x cost of Single-AZ\nChange to Multi-AZ later with minimal downtime via database modification.\n2.5 Settings DB instance identifier: workshop-postgres-db\nThis is the name of your RDS instance Must be unique in your AWS account per region Credentials Settings:\nMaster username: postgres\nDefault PostgreSQL admin user Cannot be changed after creation Credentials management:\nSelect Managed in AWS Secrets Manager - most secure Under Select the encryption key, select aws/secretmanager(default) AWS Secrets Manager will generate and store our database credentials so that we don\u0026rsquo;t have to hardcode our secrets in the Lambda function that query our database\n2.6 Instance Configuration DB instance class:\nSelect Burstable classes (includes t classes) Select db.t3.micro 2 vCPUs, 1 GB RAM Free Tier eligible Sufficient for workshop and small applications 2.7 Storage Storage type:\nSelect General Purpose SSD (gp3) (if available) Or General Purpose SSD (gp2) (Free Tier default) Allocated storage: 20 GB\nMinimum for PostgreSQL Free Tier includes 20 GB Storage autoscaling:\nKeep Enable storage autoscaling checked Maximum storage threshold: 100 GB Database automatically grows if needed (charged for additional storage) 2.8 Connectivity Compute resource:\nSelect Don\u0026rsquo;t connect to an EC2 compute resource We\u0026rsquo;ll manually configure VPC settings Network type:\nKeep IPv4 selected Virtual private cloud (VPC):\nSelect workshop-backend-vpc DB subnet group:\nSelect workshop-db-subnet-group Public access:\nSelect No Database will not be accessible from internet Only accessible from within VPC VPC security group:\nSelect Choose existing Remove the default security group Select workshop-rds-sg (created in Part 1) Availability Zone:\nSelect ap-southeast-1a This matches where workshop-private-subnet-3 is located Why Specify Availability Zone?\nAlthough our DB subnet group includes both AZs, we\u0026rsquo;re explicitly choosing us-east-1b to ensure the database is created in workshop-private-subnet-3. This provides:\nPredictable placement for troubleshooting\nSame availability zone with our Lambda\u0026rsquo;s subnet Better organization of resources\n2.10 Database Authentication Database authentication:\nKeep Password authentication selected We\u0026rsquo;ll use username/password for simplicity Other options (not using in workshop):\nPassword and IAM database authentication: More secure, uses IAM roles Password and Kerberos authentication: For enterprise Active Directory integration 2.11 Monitoring Database Insights: select standard\nPerformance history will be retained for 7 days Turn on Performance Insights:\nKeep Enabled for this workshop This monitoring option shows the source of database load like SQL queries, so you can tune SQL statements or increase system resources. Enable Enhanced monitoring:\nKeep disabled for this workshop Provides OS-level metrics For production: Enable both for better observability.\n2.12 Additional Configuration Click Additional configuration to expand settings.\nDatabase options:\nInitial database name: workshopdb\nCreates a database upon instance creation If left empty, no database is created (you\u0026rsquo;d have to create it manually) DB parameter group:\nKeep default: default.postgres17 Option group:\nKeep default: default:postgres-17 Backup:\nEnable automated backups:\nKeep enabled Free within retention period Backup retention period: 7 days\nFree Tier includes backups up to DB instance storage size Sufficient for development/testing Backup window:\nSelect No preference (AWS chooses optimal time) Or select specific time if you have preferences Enable Backup replication:\nKeep disabled Replicates backups to another region (additional cost) Encryption:\nEnable encryption:\nKeep enabled (selected by default) Uses AWS KMS for encryption at rest No additional cost (uses AWS managed key) AWS KMS key:\nSelect (default) aws/rds AWS-managed key, no key management required Encryption Best Practice:\nAlways enable encryption for databases containing sensitive data:\nEncrypts data at rest\nEncrypts automated backups\nEncrypts read replicas\nCannot be disabled after creation\nMinimal performance impact\nMaintenance:\nEnable auto minor version upgrade:\nKeep enabled PostgreSQL minor version updates applied automatically Applied during maintenance window Recommended for security patches Maintenance window:\nSelect No preference Or choose specific time (e.g., weekends for production) Deletion protection:\nKeep disabled for this workshop Prevents accidental deletion Enable in production 2.13 Estimate Costs Before creating, review the estimated monthly cost:\nLocated at the bottom right of the page Free Tier estimate: $0 (within 750 hours/month) Outside Free Tier: ~$20-25/month for db.t3.micro 2.14 Create Database Review all settings Click Create database You\u0026rsquo;ll see a success banner Database creation time: 10-15 minutes\nStatus progression:\nCreating ‚Üí Backing-up ‚Üí Available Step 3: Monitor Database Creation 3.1 Check Status Stay in RDS console ‚Üí Databases Find your database: workshop-postgres-db Monitor the Status column Status indicators:\nCreating: Initial provisioning Backing-up: Initial automated backup Available: Ready to use 3.2 View Database Details Once status is Available:\nClick on the database identifier: workshop-postgres-db You\u0026rsquo;ll see detailed information Important information to note:\nEndpoint \u0026amp; port:\nEndpoint: workshop-postgres-db.xxxxxxxxxx.[region].rds.amazonaws.com Port: 5432 Copy the endpoint - you\u0026rsquo;ll need it for Lambda connections Connectivity \u0026amp; security:\nVPC: workshop-backend-vpc Subnets: Both private subnets Security groups: workshop-rds-sg Public accessibility: No Configuration:\nDB instance class: db.t3.micro Storage: 20 GB gp3 (or gp2) Multi-AZ: No RDS Best Practices Security Implemented in this workshop:\nDatabase in private subnet (no public access)\nSecurity group restricting access to Lambda only\nEncryption at rest enabled\nSecrets Manager for storing database credentials\nFor production, also implement:\nIAM database authentication\nSecrets Manager with automatic rotation\nEnhanced monitoring and logging\nRegular security patches (auto minor version upgrade enabled)\nCloudWatch alarms for anomalies\nPerformance Implemented:\nAppropriate instance class for workload\ngp3 storage for balanced performance\nIndexes on commonly queried columns\nFor production, also consider:\nRead replicas for read-heavy workloads\nConnection pooling (RDS Proxy)\nQuery performance insights\nProper database configuration tuning\nAvailability Implemented:\nAutomated backups (7-day retention)\nDB subnet group in multiple AZs\nFor production, also implement:\nMulti-AZ deployment for automatic failover\nCross-region read replicas for DR\nLonger backup retention (30 days)\nBackup replication to another region\nCost Optimization Implemented:\nFree tier (if applicable)\nRight-sized instance (db.t3.micro)\nSingle-AZ deployment\nStandard backup retention\nFor production, also consider:\nReserved instances for predictable workloads (up to 69% savings)\nStop database instances during non-business hours (dev/test)\nStorage autoscaling instead of over-provisioning\nRegular review of CloudWatch metrics\nSummary Congratulations! You\u0026rsquo;ve successfully:\nCreated a DB subnet group for RDS deployment Launched an RDS PostgreSQL database instance Configured the database in a private subnet Set up security groups for Lambda access Enabled encryption and automated backups Prepared database schema for application Obtained database endpoint for Lambda connections Next Steps Proceed to Part 3: AWS Secrets Manager Configuration to securely store and manage your database credentials.\nReady to continue? Your database is now ready to receive connections from Lambda functions! üéâ\n"
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nThis section will list and introduce the blogs you have translated. For example:\nBlog 1 - Getting started with healthcare data lakes: Using microservices This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 2 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 3 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 4 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 5 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 6 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\n"
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/4-eventparticipated/",
	"title": "Events Participated In",
	"tags": [],
	"description": "",
	"content": "During the internship, I attended 5 events. Each event offered a memorable experience, providing profound insights, valuable takeaways, and excellent networking opportunities.\nKick-off AWS FCJ Workforce Event Name: Kick-off AWS FCJ Workforce - FPTU OJT FALL 2025\nTime: 08:30, September 6, 2025\nLocation: 26th Floor, Bitexco Financial Tower, No. 02 Hai Trieu Street, Ben Nghe Ward, Ho Chi Minh City\nRole: Attendee\nSummary: Strategic orientation sessions covering AWS career paths (Cloud, DevOps, AI/ML, Security, Data), featuring alumni sharing, networking activities, and interactive Q\u0026amp;A.\nKey Takeaways \u0026amp; Value: Refined long-term development mindset; gained clarity on the DevOps/Cloud roadmap; committed to engaging with the AWS Builders community and sharing knowledge within the team.\nData Science On AWS Event Name: Data Science On AWS\nTime: 09:30, October 16, 2025\nLocation: FPT University HCMC, High Tech Park, Thu Duc City, Ho Chi Minh City\nRole: Attendee\nSummary: Demonstration of an end-to-end Data Science pipeline on AWS (S3 ‚Üí Glue ‚Üí SageMaker), featuring live demos (IMDb ETL, Sentiment analysis), and an overview of managed AI services (Transcribe, Comprehend, Rekognition, Personalize).\nKey Takeaways \u0026amp; Value: Acquired new skills in Glue ETL and SageMaker training/deployment; reinforced a data-first mindset; Contributions include automating ETL processes, piloting sentiment analysis models, and streamlining MLOps workflows.\nReinventing DevOps with AWS Generative AI Event Name: Reinventing DevOps with AWS Generative AI\nTime: 19:30, October 16, 2025\nLocation: Online via Microsoft Teams (Hosted by CMC Global)\nRole: Attendee\nSummary: Transitioning from DevOps to DevSecOps utilizing a seven-phase secure lifecycle and AI-augmented toolchains (including Amazon CodeGuru live demos), with a strong emphasis on IaC security and observability.\nKey Takeaways \u0026amp; Value: Mastered SAST/dependency scanning, IaC security scanning (e.g., Checkov), and AI-powered anomaly detection; Contributions involve implementing CI/CD security gates, piloting CodeGuru, and enhancing monitoring documentation.\nGenerative AI with Amazon Bedrock Event Name: Generative AI with Amazon Bedrock\nTime: 08:30, November 15, 2025\nLocation: 26th Floor, Bitexco Financial Tower, No. 02 Hai Trieu Street, Ben Nghe Ward, Ho Chi Minh City\nRole: Attendee\nSummary: High-level overview of Generative AI on AWS using Amazon Bedrock and Foundation Models. Covered advanced Prompt Engineering (Zero-Shot, Few-Shot, Chain-of-Thought), RAG implementation with Amazon Titan and vector databases, and an introduction to Agentic AI and Amazon Bedrock AgentCore for production-ready agents.\nKey Takeaways \u0026amp; Value: Understood how to architect RAG-powered applications, optimize prompt designs, and deploy scalable, secure AI prototypes to production. Defined next steps to utilize aws-samples and Bedrock_AgentCore for building internal AI assistants grounded in proprietary data.\nDevOps on AWS Event Name: DevOps on AWS\nTime: 08:30, November 17, 2025\nLocation: 26th Floor, Bitexco Financial Tower, No. 02 Hai Trieu Street, Ben Nghe Ward, Ho Chi Minh City\nRole: Attendee\nSummary: End-to-end DevOps practices on AWS: Infrastructure as Code (IaC) via CloudFormation and CDK; CI/CD pipelines using the CodeSuite (CodeCommit, CodeBuild, CodeDeploy, CodePipeline); Containerization with Docker and the AWS ecosystem (ECR, ECS, EKS, Fargate, App Runner); and Observability via CloudWatch and X-Ray.\nKey Takeaways \u0026amp; Value: Solidified the DevOps mindset and practical skills to replace manual \u0026ldquo;ClickOps\u0026rdquo; with automated IaC, standardized CI/CD workflows, and adopted a container-first architecture for microservices. Action items include codifying existing infrastructure, containerizing applications, and enhancing observability with custom dashboards and traces.\n"
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/4-eventparticipated/4.4-event4/",
	"title": "Generative AI with Amazon Bedrock",
	"tags": [],
	"description": "",
	"content": "Summary Report Event Vision To provide a strategic guide to Generative AI, from the foundational principles of large language models to the sophisticated art of prompt engineering. To unlock the power of Retrieval Augmented Generation (RAG) for building AI that is grounded in proprietary, real-time data. To introduce the next evolutionary step in AI: autonomous, goal-oriented agents that can reason and act. To unveil Amazon Bedrock AgentCore, a comprehensive platform designed to bridge the chasm between AI prototypes and secure, scalable production systems. Speakers Lam Tuan Kiet ‚Äì Sr. DevOps Engineer, FPT Software Danh Hoang Hieu Nghi ‚Äì AI Engineer, Renova Cloud Dinh Le Hoang Anh ‚Äì Cloud Engineer Trainee, First Cloud AI Journey Key Highlights A Glimpse into the Future \u0026ldquo;The journey of AI is an evolution in autonomy.\u0026rdquo; This central theme framed the entire workshop, charting a course from simple AI assistants that follow rules to fully autonomous Agentic AI systems that can reason, plan, and execute complex workflows with minimal human oversight.\nPart 1: Mastering the Fundamentals with Amazon Bedrock The Foundation Model Revolution: The session began by contrasting traditional ML models with the broad, general-purpose Foundation Models (FMs). Amazon Bedrock was positioned as the secure gateway to a diverse range of cutting-edge FMs. The Art of the Prompt: Prompt Engineering was deconstructed as the skill of communicating effectively with an AI. The session covered a spectrum of techniques from Zero-Shot and Few-Shot to advanced Chain-of-Thought prompting. Retrieval Augmented Generation (RAG): Grounding AI in Reality: RAG was presented as the critical technology for enterprise AI. By using embedding models like Amazon Titan to convert private data into vector representations, RAG allows the AI to retrieve relevant, up-to-date information before generating an answer, combating hallucinations and tailoring responses to specific business contexts. Part 2: The Evolution into Autonomous Agents Bridging the Prototype-to-Production Chasm: The speakers identified the critical hurdles that prevent promising AI prototypes from becoming valuable business tools: Performance, Scalability, Security, and Governance. The Rise of Agentic AI: The workshop then pivoted to the future: AI Agents. These are not just chatbots; they are systems designed to achieve specific goals by automating entire workflows, leveraging open-source frameworks like LangChain, LlamaIndex, and Crew.AI. Part 3: Introducing Amazon Bedrock AgentCore The Production-Ready Platform for Agents: The highlight of the event was the introduction of Amazon Bedrock AgentCore, a comprehensive platform designed to solve the production chasm. AgentCore provides all the foundational services needed to run agents securely and at scale: Runtime \u0026amp; Identity: Securely execute and manage agent operations. Memory: Provide agents with context for coherent interactions. Tools: Equip agents with a Browser Tool to access live web data and a Code Interpreter to perform calculations. Gateway: A secure entry point for agents to interact with company APIs. Observability: Gain deep operational insights into agent performance, cost, and behavior. From Theory to Practice: The Live Demos and Code Blueprints The workshop transcended theory by grounding every concept in practical, hands-on code, referencing two key GitHub repositories as invaluable resources for attendees.\nThe Foundational Cookbook: aws-samples/amazon-bedrock-samples This official AWS repository was presented as the essential \\\u0026ldquo;cookbook\\\u0026rdquo; for mastering the core components of Bedrock. The demos showcased how this collection of Jupyter notebooks and code samples provides a flight simulator for AI development, allowing engineers to:\nExperiment with Prompting: Explore dozens of examples for Zero-shot, Few-shot, and Chain-of-Thought prompting across different models like Claude and Llama. Build a RAG Pipeline from Scratch: Step-by-step guides demonstrated how to use Amazon Titan to create embeddings, store them in a vector database, and build a complete, functional RAG pipeline for Q\u0026amp;A over private documents. Master the APIs: Provide clear, reusable code snippets for interacting with virtually every feature of the Bedrock service, from text generation to image creation. The Capstone Project: ihatesea69/Bedrock_AgentCore This repository served as the capstone demo, illustrating how to assemble the individual components from the \\\u0026ldquo;cookbook\\\u0026rdquo; into a sophisticated, functioning AI agent using the new AgentCore platform. The live demonstration walked through this repository to showcase how to:\nDefine an Agent: Structure the agent\u0026rsquo;s identity, instructions, and goals in code. Equip the Agent with Tools: Grant the agent the ability to perform actions beyond text generation, such as calling external APIs or interpreting code. Orchestrate a Mission: Tie everything together in a live example where the agent receives a complex request, autonomously chooses the right tools, retrieves necessary information, and executes a multi-step plan to achieve its goal. This made the abstract concept of \\\u0026ldquo;Agentic AI\\\u0026rdquo; tangible and achievable. Key Takeaways Your Quick-Start Blueprint: Applying This to Your Work Build a RAG-Powered Expert: Clone the amazon-bedrock-samples repository and adapt the RAG notebooks to connect to your own internal documentation, creating a powerful, context-aware internal search engine. Prototype Your First Agent: Use the Bedrock_AgentCore repository as a template. Define a simple agent that automates a multi-step business process unique to your team, such as generating a daily sales report by calling an internal API and summarizing the results. Refine Prompts with the Cookbook: Take an existing AI workflow and use the diverse examples in the amazon-bedrock-samples repo to upgrade your prompts from simple instructions to sophisticated, few-shot examples, dramatically improving output quality. Explore AgentCore Tools: Investigate how the tools within Bedrock AgentCore, like the Browser Tool or Code Interpreter, could solve a specific business problem that requires live data or dynamic calculations. Event Experience This workshop was a fascinating journey through the entire landscape of modern AI. The inclusion of comprehensive GitHub repositories transformed it from a theoretical lecture into an actionable masterclass. Attendees left not just with knowledge, but with the specific code blueprints needed to begin building the next generation of AI-driven applications immediately. The introduction of Amazon Bedrock AgentCore, backed by a practical demo, provided a clear and compelling roadmap for moving beyond simple AI prototypes to create secure, scalable, and truly autonomous enterprise agents.\nSome event photos "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives: Gain hands-on experience configuring Amazon RDS, including VPC setup, security groups, and backup management. Learn how to deploy scalable web applications using Auto Scaling Groups and Application Load Balancers. Strengthen monitoring skills with CloudWatch metrics, logs, alarms, and custom dashboards. Explore hybrid DNS architectures with Route 53 Resolver for enterprise environments. Improve proficiency with AWS CLI for managing resources across S3, EC2, VPC, and IAM. Build end-to-end CI/CD pipelines using CodeCommit, CodeBuild, CodeDeploy, and CodePipeline. Implement automated backup strategies with AWS Backup and lifecycle policies. Deploy containerized applications using Docker and container registries on AWS. Study virtual machine migration workflows, including import and export between environments. Build serverless applications using AWS Lambda and API Gateway with proper IAM configuration. Understand centralized security monitoring using AWS Security Hub and its service integrations. Tasks Completed This Week: Day Task Start Date Completion Date Reference Material 2 - Configure and Manage Amazon RDS: + Configure VPC, security groups, and DB subnet group + Launch EC2 and RDS instances + Deploy a sample app on EC2 connected to RDS + Perform backup and restore operations + Clean up all created resources - Deploy a Scalable Web Application using Auto Scaling: + Set up VPC, subnets, and security groups + Create an EC2 Launch Template + Configure Target Group and Application Load Balancer + Create Auto Scaling Group with manual, scheduled, and dynamic policies + Clean up all AWS resources - Monitor Resources with CloudWatch: + Analyze metrics using search and math expressions + Query logs with CloudWatch Logs Insights + Create Metric Filters + Configure CloudWatch Alarms + Build dashboards for visualization + Clean up alarms and dashboards 29/09/2025 29/09/2025 RDS, Auto Scaling, CloudWatch Labs 3 - Implement Hybrid DNS with Route 53 Resolver: + Deploy base infrastructure with CloudFormation + Set up Microsoft AD for on-prem DNS simulation + Create outbound and inbound Resolver endpoints + Configure forwarding rules + Test bidirectional name resolution and clean up resources - Manage AWS Services via CLI: + Install and configure AWS CLI + Use CLI to manage S3, SNS, IAM resources + Perform S3 bucket/object operations + Create and manage VPC components + Launch and terminate EC2 instances with CLI + Clean up all resources 30/09/2025 30/09/2025 Route 53 Resolver, AWS CLI Labs 4 - Build a CI/CD Pipeline for Automated Deployment: + Store source code in CodeCommit + Configure CodeBuild for compiling and packaging + Set up CodeDeploy for automated deployments + Orchestrate the full workflow with CodePipeline + Test automated deployment with a code push + Clean up all pipeline resources - Automate EC2 Backups with AWS Backup: + Deploy infrastructure using CloudFormation + Create backup plans with lifecycle rules + Configure backup notifications + Test backup and restore operations + Remove all created backups and stacks 01/10/2025 01/10/2025 CodeCommit, CodeBuild, CodeDeploy, AWS Backup Labs 5 - Deploy a Dockerized Application on AWS: + Set up VPC, security groups, IAM roles + Launch RDS instance as backend + Deploy application on EC2 using Docker image + Redeploy using Docker Compose + Push image to container registry (ECR or Docker Hub) + Clean up resources - Migrate Virtual Machines with Import/Export: + Export an on-prem VM + Upload VM image to S3 + Import VM to create AMI + Launch EC2 from imported AMI + Export EC2 instance back to S3 + Clean up all resources 02/10/2025 02/10/2025 Docker on AWS, VM Import/Export Labs 6 - Deploy a Serverless Application using Lambda and API Gateway: + Package and zip Lambda function with dependencies + Create IAM execution role + Deploy Lambda function + Create HTTP API in API Gateway and integrate with Lambda + Deploy and test endpoint + Clean up Lambda, API, and IAM role - Centralize Security Monitoring with Security Hub: + Enable Security Hub + Review aggregated findings and dashboards + Analyze detections from GuardDuty, Inspector, and Macie + Explore risk summaries and charts 03/10/2025 03/10/2025 Lambda \u0026amp; Security Hub Labs Week 4 Achievements: Successfully deployed Amazon RDS with full networking and backup configuration. Built scalable application infrastructure using Auto Scaling and ALB. Implemented comprehensive CloudWatch monitoring with metrics, logs, alarms, and dashboards. Configured hybrid DNS architecture using Route 53 Resolver and Microsoft AD. Strengthened AWS CLI proficiency across multiple services. Developed automated CI/CD pipelines with AWS developer tools. Implemented lifecycle-based backup strategies with AWS Backup. Deployed Dockerized applications and published container images to ECR. Completed VM migration workflows using import/export features. Built serverless applications with Lambda and API Gateway. Centralized security monitoring using Security Hub with service integrations. "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/5-workshop/5.2-serverless-backend/5.2.4-secrets-manager/",
	"title": "AWS Secrets Manager Configuration",
	"tags": [],
	"description": "",
	"content": "Overview In this section, you\u0026rsquo;ll configure AWS Secrets Manager to securely store your database credentials. Instead of hardcoding passwords in Lambda functions, you\u0026rsquo;ll retrieve them dynamically from Secrets Manager, following security best practices.\nWhat you\u0026rsquo;ll accomplish:\nUnderstand AWS Secrets Manager and its benefits Store RDS database credentials as a secret Configure secret structure for database connections Set up IAM permissions for Lambda to access secrets Test secret retrieval Understand secret rotation (optional configuration) Estimated time: 15-20 minutes\nWhy AWS Secrets Manager? Security Benefits Never Hardcode Credentials:\nCredentials stored encrypted at rest Access controlled via IAM policies Audit trail of who accessed secrets No credentials in application code or environment variables Automatic Rotation:\nPeriodic credential rotation without downtime Reduces risk of credential compromise Lambda functions automatically use new credentials Centralized Management:\nSingle source of truth for credentials Easy to update across multiple applications Version history of secret values Cost Considerations Pricing:\n$0.40 per secret per month $0.05 per 10,000 API calls VPC Endpoint (PrivateLink): $0.013/hour For this workshop:\n1 secret for database credentials Minimal API calls from Lambda Estimated cost: \u0026lt;$1 (assume clean up immediately after workshop) Step 1: View Database Secret In previous section - RDS Database Setup, during database creation, we have selected the option Managed in AWS Secrets Manager - most secure and used the default (aws/secretmanager(default)). This option create a RDS-managed secret for us. Let\u0026rsquo;s view it\n1.1 Navigate to Secrets Manager Console In AWS Console search bar, type \u0026ldquo;Secrets Manager\u0026rdquo; Click on Secrets Manager under Services 1.2 View Secret In the Secrets Manager dashboard, you will see the RDS-managed secret in the list 1.3 View Secret Details Click on your secret name You\u0026rsquo;ll see detailed information Secret ARN:\nCopy this ARN - you\u0026rsquo;ll need it for IAM policies Secret value (encrypted):\nClick Retrieve secret value to view 1.4 View rotation behavior Go to the Rotation tab You can see the rotation behaviors Rotation status: true - enabled Rotation schedule: 7 days - the secret will be rotated every 7 days Last rotated date and Next rotation data: dates indicate the last rotation and the next rotation Click Edit rotation to modify behavior On the Edit rotation configuration modal, you can see several notable settings: Automatic rotation: Enables or disables automatic password/secret rotation. When turned on, Secrets Manager will rotate the secret based on the schedule you define. Rotation schedule: Controls when and how often the secret is rotated. You can choose time unit and amount of time units for rotation You can select Schedule expression for complex rotation schedules Windows duration - optional: The amount of time (in hours) during which Secrets Manager is allowed to perform the rotation. This is often used to avoid maintenance windows or prevent rotations during peak hours. Rotate immediately when the secret is stored (checkbox) If checked, Secrets Manager will rotate the secret immediately after it is created or stored. After that, it follows the regular rotation schedule. Step 2: Create IAM Role for Lambda Access Lambda functions need permission to read this secret. We\u0026rsquo;ll create a custom IAM role for our Lambda function to assume.\n2.1 Navigate to IAM Console In AWS Console search bar, type \u0026ldquo;IAM\u0026rdquo; Click on IAM under Services In left navigation, click Roles 2.2 Create Role Click Create role 2.3 Select trusted entity Trusted entity type: select AWS service Service or use case: select Lambda Click Next 2.4 Add permissions Permission policies: In the search field, search and select AWSLambdaVPCAccessExecutionRole and SecretsManagerReadWrite Click Next 2.5 Name, review and create Role name: workshop-lamda-secretsmng-role Description: Allows Lambda functions to call Secrets Manager to fetch secrets. Understanding the IAM Role AWSLambdaVPCAccessExecutionRole { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34;, \u0026#34;ec2:CreateNetworkInterface\u0026#34;, \u0026#34;ec2:DescribeNetworkInterfaces\u0026#34;, \u0026#34;ec2:DeleteNetworkInterface\u0026#34;, \u0026#34;ec2:AssignPrivateIpAddresses\u0026#34;, \u0026#34;ec2:UnassignPrivateIpAddresses\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:*:*:*\u0026#34; } Create and write to CloudWatch Log Groups Essential for debugging and monitoring Create ENIs (Elastic Network Interfaces) in VPC Required for Lambda to connect to RDS in private subnet Manages network connectivity SecretsManagerReadWrite This policy includes all permissions related to Secrets Manager and allow Lambda to reach Secrets Manager to connect to RDS instance\nHow Lambda Uses These Permissions Lambda Function Execution\r‚Üì\r1. Create ENI in VPC (VPCAccessExecutionRole)\r‚Üì\r2. Retrieve DB credentials (WorkshopLambdaSecretsPolicy)\r‚Üì\r3. Connect to RDS via ENI\r‚Üì\r4. Execute database queries\r‚Üì\r5. Write logs to CloudWatch (BasicExecutionRole)\r‚Üì\r6. Return response Security Best Practices Principle of Least Privilege:\nPolicy only grants read access (not write/delete)\nScoped to specific secret (not all secrets)\nNo wildcard permissions\nResource-Based Restrictions:\nUses specific ARN pattern\nCan\u0026rsquo;t accidentally access other secrets\nEasier to audit and troubleshoot\nStep 3: Create VPC endpoint for Lambda function For a Lambda function reside inside a private subnet to access Secrets Manager secrets, it must go through a VPC endpoint. We\u0026rsquo;ll create a VPC endpoint now in VPC console\n3.1 Navigate to VPC Console In AWS Console search bar, type \u0026ldquo;VPC\u0026rdquo; Click on VPC under Services In left navigation, under PrivateLink and Lattice, click Endpoints Click Create Endpoint 3.2 Endpoint creation Name tag - optional: workshop-lambda-secretsmng-endpoint Type:: AWS services Services: in the search field, type secrets. You will see a service named com.amazonaws.[region].secretsmanager. Select it Network settings:\nVPC: select workshop-backend-vpc Subnet: Select the subnet that is inside the same availability zone as our RDS\u0026rsquo;s subnet Recall that in VPC and Network Setup) we decided to put Lambda in private subnet 1, RDS in private subnet 3, both reside in ap-southeast-1a availability zone In this case, we will select apse1-az2 (ap-southeast-1a) Subnet ID: select workshop-backend-subnet-private1-ap-southeast-1a Security groups: select workshop-endpoint-sm-sg security group that we created in VPC and Network Setup Policy: full access. This allows Lambda to use all Secrets Manager operations (allowed in the role we just created) through the endpoint Click Create endpoint\n3.3 Verify Endpoint Once status is Available:\nClick on the endpoint ID Note the DNS names - Lambda will use these automatically Verify Subnets shows workshop-private-subnet-1 Verify Security groups shows workshop-lambda-sg Cost Savings with VPC Endpoints:\nInterface VPC Endpoint costs: ~$7.20/month + $0.01/GB data processed\nWithout VPC Endpoint:\nLambda ‚Üí NAT Gateway ‚Üí Internet ‚Üí Secrets Manager NAT Gateway cost: ~$32/month + $0.045/GB With VPC Endpoint:\nLambda ‚Üí VPC Endpoint ‚Üí Secrets Manager (private) Lower latency, more secure, lower cost for high-traffic applications For this workshop with minimal traffic, the difference is small, but it demonstrates production best practices.\nSummary Congratulations! You\u0026rsquo;ve successfully:\nView RDS secret in detail in AWS Secrets Manager Created IAM role for Lambda to access the secret Created VPC endpoint for Lambda to read secrets in AWS Secrets Manager Next Steps Proceed to Part 4: Lambda Functions Development to create Lambda functions and connect the pieces together (VPC endpoint, Lambda role) that will use these secrets to connect to your RDS database.\nReady to continue? Your credentials are now securely stored and your Lambda functions will have proper permissions to access them! üîê\n"
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/5-workshop/5.1-frontend-deployment/5.1.4-waf/",
	"title": "AWS WAF Configuration",
	"tags": [],
	"description": "",
	"content": "Overview In this section, you\u0026rsquo;ll configure AWS WAF (Web Application Firewall) to protect your CloudFront distribution from common web exploits and malicious traffic. WAF provides a layer of security that inspects incoming requests and blocks those that match defined rules.\nWhat you\u0026rsquo;ll accomplish:\nUnderstand AWS WAF concepts and rule types View and understand WAF included with your CloudFront distribution Test and verify WAF protection Monitor blocked requests Estimated time: 45 minutes\nWhy AWS WAF? Protection against:\nSQL injection attacks: Prevents database compromise Cross-site scripting (XSS): Blocks malicious script injection Bot traffic: Filters automated requests and scrapers DDoS attacks: Rate limiting to prevent resource exhaustion Geographic restrictions: Block/allow traffic from specific countries Known bad inputs: Protects against common attack patterns Benefits:\nReal-time protection at the edge (CloudFront) Highly customizable rules Detailed logging and monitoring Pay only for what you use No infrastructure to manage Costs Considerations Free-tier: $0/month Paid-tier Resource Type Price Web ACL $5.00 per month (prorated hourly) Rule $1.00 per month (prorated hourly) Request $0.60 per 1 million requests Overall: $0 or \u0026lt; $1 if Paid-tier (assume immediate clean up after workshop) Understanding WAF Concepts Web ACL (Access Control List) A Web ACL is the core resource that contains all your security rules. It\u0026rsquo;s associated with your CloudFront distribution to inspect and filter traffic.\nRule Types 1. Managed Rules\nPre-configured rule sets maintained by AWS or AWS Marketplace sellers Regularly updated for new threats Easy to implement (no configuration needed) 2. Custom Rules\nRules you create for specific requirements Full control over match conditions and actions Useful for application-specific protection 3. Rule Groups\nCollections of related rules Can be managed rules or custom rule groups Evaluated in priority order Rule Actions Allow: Request passes through Block: Request is blocked (returns 403 Forbidden) Count: Request is counted but allowed (useful for testing) CAPTCHA: Presents a CAPTCHA challenge Challenge: Presents a silent browser challenge Rule Capacity Units (WCU) Each rule consumes a certain number of WCUs Maximum 1,500 WCUs per Web ACL Simple rules: 1-5 WCUs Complex managed rules: 50-100+ WCUs Step 1: Navigate to AWS WAF Console 1.1 Access WAF Service In the AWS Console search bar, type \u0026ldquo;WAF\u0026rdquo; Click on WAF \u0026amp; Shield under Services 1.2 Understand the Dashboard You\u0026rsquo;ll see:\nWeb ACLs: Your firewall configurations IP sets: Reusable IP address lists Regex pattern sets: Reusable regex patterns Rule groups: Custom rule collections Application integration Add-on protection Step 2: View a Web ACL 2.1 View the Web ACL created by CloudFront In the left navigation, click Protection packs (web ACLs) You can see a Web ACL created by CloudFront that you set up in Cloudfront Distribution Setup This WAF configuration is included as part of the CloudFront pricing Free plan for the distribution we created in Cloudfront Distribution Setup\nClick the Web ACL name to open a sidebar to view actions You can view which CloudFront distribution created the Web ACL in the sidebar Click Manage details, here we can see the properties and behavior of this protection pack Click Download protection pack (web ACL) as JSON. Then open it to see the web ACL You can view the sample Web ACL JSON and explanation in this section Under Protection pack (web ACL) behavior, you can modify Default action, if set to allow, allow all requests that are not caught by web ACL rules, and vice versa Under Challenge configurations is for adjusting challenges (CAPTCHA) behaviors like default immunity time (time before having to solve next CAPTCHA) or token domains (for persisting challenge immunity between domains) Custom response: customize what is returned in the response when requests are blocked Back to the main page, click Manage rules On this page, you can see existing rules which was created along with CloudFront Click on a rule to manage: Inspection: apply rule to All requests or requests that Match statement All requests: the Override rule groups checkbox if checked, the entire rule group will primarily operate in Count mode. Rules that in Block mode will stop blocking, instead, they only count requests that match criteria Match statement: define criteria to block requests, such as IP block or Geo-block Click the Add rule button to add a rule You can choose between Custom rule and AWS Managed rule groups In custom rule, you can create criteria such as IP-based rule (IP block), Geo-based rule (Location block), Rate-based rule (Rate limiting) and Custom rule (combining with logical operators) For example, let\u0026rsquo;s create a Rate-based rule, if there are more than 1000 requests within 5 minutes, block all subsequent requests after the 1000th Click the Edit rule order to modify order of rules Higher rules will block all requests without them to access lower rules Let\u0026rsquo;s modify our new Rate-based rule to be the highest Drag the new rule to the top, then click Save rule order We will test this rule in the next step (Step 3) Back to the main page, click Manage resources\nOn this section, we can modify which AWS resources for this web ACL to protect. Free-tier CloudFront associated web ACL can not be assigned to other resources or un-assigned from its designated CloudFront distribution We will cover View dashboard, logs and sampled requests and Configure logging and sampled requests sections later\nStep 3: Test WAF Protection 3.1 Test Normal Access Open your website: https://[your-cloudfront-domain].cloudfront.net Navigate through different pages Verify the site works normally Expected result: Site loads without issues\n3.2 Test SQL Injection Protection Try accessing your site with a SQL injection pattern in the URL:\nhttps://[your-cloudfront-domain].cloudfront.net/?id=1\u0026#39; OR \u0026#39;1\u0026#39;=\u0026#39;1 Expected result:\n403 Forbidden error Request blocked by WAF 3.3 Test XSS Protection Try accessing with a cross-site scripting pattern:\nhttps://[your-cloudfront-domain].cloudfront.net/?search=\u0026lt;script\u0026gt;alert(\u0026#39;xss\u0026#39;)\u0026lt;/script\u0026gt; Expected result:\n403 Forbidden error Request blocked by WAF 3.4 Test Rate Limiting You can test rate limiting rule we created earlier with a simple script or tool:\nUsing curl (bash/terminal):\n$url = \u0026#34;https://d3b2qa4f4hqtdb.cloudfront.net/\u0026#34; $count = 1100 # Loop from 1 to 1100 for ($i = 1; $i -le $count; $i++) { # Invoke-WebRequest sends the request # -Uri specifies the URL $response = Invoke-WebRequest -Uri $url -Method GET -MaximumRedirection 0 -TimeoutSec 10 -ErrorAction SilentlyContinue | Select-Object -First 1 # Check if a response object was returned if ($response -ne $null) { # Output the HTTP Status Code $response.StatusCode } else { # Output a message if the request failed \u0026#34;Request failed or timed out\u0026#34; } } Expected result:\nFirst ~1,000 requests: HTTP 200 After 1,000 requests: HTTP 403 Rate limit triggered Testing Rate Limits:\nThe rate limit counts requests over a 5-minute window. After being blocked, wait 5 minutes for the counter to reset. In production, set appropriate limits based on your expected traffic patterns:\nPublic websites: 2,000-10,000 requests per 5 minutes\nAPIs: 100-1,000 requests per 5 minutes (depends on use case)\nAdmin panels: 50-100 requests per 5 minutes\n3.5 Alternative: Use Browser Developer Tools For simpler testing without scripts:\nOpen Developer Tools (F12) Go to Network tab Rapidly refresh the page multiple times (Hold Ctrl+R or Cmd+R) After 2,000+ requests, you should see 403 responses Step 8: Monitor WAF Activity 8.1 View Web ACL Overview Go to AWS WAF -\u0026gt; Protection packs (web ACLs) Click on View under the Dashboard column Or you can click the Web ACL name and in the sidebar, click View dashboard, logs and sampled requests You\u0026rsquo;ll see the Dashboard Metrics shown:\nTotal: total requests received Allowed requests: Requests that passed all rules Blocked requests: Requests blocked by rules CAPTCHA: Requests that were matched by a rule and presented with a visual or audio puzzle requiring human interaction to solve. Challenged: Requests that were matched by a rule and subjected to a silent browser interrogation 8.2 Analyze Protection pack (web ACL) activity Scroll down to Protection pack (web ACL) activity section You\u0026rsquo;ll see a graph displaying requests handled by each rule. Hover on each section to see detailed count of allowed, blocked\u0026hellip; requests of each rule 8.3 Analyze overview Scroll down to the bottom, you can see charts, you can select criteria to filter data for the chart such as: Request locations Type of attacks Client devices Rules characteristics\u0026hellip; Useful for:\nIdentifying attack patterns Understanding traffic trends Setting up alarms for unusual activity Step 9: Create CloudWatch Alarms (Optional) To follow this section, you must upgrade your CloudFront to Pro-tier. You will be charged $15/month in Pro-tier and you can only switch back to free-tier after 5 days. Furthermore, you can only delete your Pro-tier distribution after the first billing cycle\n9.1 Enable logging destination in WAF Go to WAF console -\u0026gt; Protection packs (web ACLs) In your web ACL, under Logging \u0026amp; metrics, you should see that it is Not enabled, click on Not enabled -\u0026gt; Configure to configure logging On the next screen, under Logging, click Enable, then select Logging destination On the sidebar that appeared, under Amazon Cloudwatch Logs log group, you can use existing log groups and create a new one. We will create and use a log group called aws-waf-logs-workshop1, then click Create Back to Logging destination sidebar, select you newly-created log groups Then click Save 9.1 Set Up Blocked Request Alarm Go to CloudWatch console Click Alarms -\u0026gt; All alarms in the left navigation Click Create alarm Configure alarm:\nSelect metric:\nClick Select metric You should see WAFV2 in the list, click on it to show list of metrics Select any metrics you want to create an alarm for. I will create one for XSS Blocked attacks: click ManagedRuleGroup, ManagedRuleGroupRule, WebACL, then select AWSManagedRulesCommonRuleSet, then click Select metric Click Select metric Specify conditions:\nStatistic: Sum Period: 5 minutes Threshold type: Static Whenever BlockedRequests is: Greater than 2 This triggers when more than 2 requests are blocked in 5 minutes Configure actions:\nChoose to create In alarm Click Create new topic (for SNS notification) Topic name: Default_CloudWatch_Alarms_Topic Email: Enter your email address Click Create topic You will now see you topic selected Click Next Alarm name:\nName: WAF-High-Blocked-Requests Description: Alert when WAF blocks more than 100 requests in 5 minutes Click Create alarm 9.2 Confirm SNS Subscription Check your email Click the confirmation link in the SNS subscription email You\u0026rsquo;ll start receiving alerts when the alarm triggers 9.3 Test alarm Now let\u0026rsquo;s simulate a XSS attack 2 more times to trigger the alarm\nTry accessing with a cross-site scripting pattern:\nhttps://[your-cloudfront-domain].cloudfront.net/?search=\u0026lt;script\u0026gt;alert(\u0026#39;xss\u0026#39;)\u0026lt;/script\u0026gt; Step 10: Fine-Tune Rules (Optional) 10.1 Handle False Positives If legitimate requests are being blocked:\nGo to WAF console Click on your Web ACL Click Rules tab Find the rule causing false positives (check sampled requests) Options:\nOption 1: Set rule to Count mode\nClick Edit on the rule group Click Override all rule actions Select Count This logs matches without blocking (testing mode) Option 2: Exclude specific rules\nClick Edit on the rule group Expand Rules Find the problematic rule Select Override to Count for that specific rule Option 3: Add scope-down statement\nClick Edit on the rule group Add conditions to narrow when the rule applies Example: Only apply to specific paths or query parameters Troubleshooting Issue: Legitimate requests being blocked (False Positives) Solution:\nCheck sampled requests to identify which rule is blocking Set that specific rule to Count mode temporarily Add scope-down statements to narrow rule application Or exclude specific sub-rules causing issues Issue: WAF not blocking test attacks Causes:\nWAF still deploying to edge locations (wait 5 minutes) Rule priority incorrect Rule set to Count instead of Block Solution:\nVerify Web ACL status is Active Check rule actions are set to Block Verify CloudFront association is complete Check rule priority order Clear CloudFront cache and test again Issue: Cannot see Web ACL in CloudFront Solution:\nEnsure you created WAF in Global (CloudFront) region Regional WAF (for ALB/API Gateway) won\u0026rsquo;t appear for CloudFront Recreate Web ACL in correct region if needed Issue: Rate limit not working Solution:\nVerify rate limit rule is enabled and priority is correct Check you\u0026rsquo;re testing from same IP (different IPs have separate counters) Remember rate limit is per 5-minute window Test with enough requests (e.g., 2,100+ for 2,000 limit) Issue: High WAF costs Solution:\nReview which managed rules you actually need Consider using fewer rule groups Use custom rules instead of multiple managed rules where possible Remove Web ACL when not actively using Summary Congratulations! You\u0026rsquo;ve successfully:\nCreated an AWS WAF Web ACL Configured AWS managed rule groups for protection Set up custom rate limiting rules Associated WAF with CloudFront Tested and verified WAF protection Learned to monitor and troubleshoot WAF activity (Optional) Set up logging and CloudWatch alarms What You\u0026rsquo;ve Achieved Your application is now protected against:\nSQL injection attacks: Database exploitation attempts blocked Cross-site scripting (XSS): Malicious script injection prevented Known bad inputs: Invalid and malicious patterns filtered Rate-based attacks: Automated abuse and DDoS mitigated IP reputation threats: Known malicious IPs blocked Complete Architecture Internet Users\r‚Üì\rAWS WAF (Security Rules)\r‚Üì\rCloudFront (HTTPS + Caching)\r‚Üì\rS3 Bucket (Private, Static Content) Security Layers Now in Place Network Layer: CloudFront with DDoS protection (AWS Shield Standard) Application Layer: AWS WAF with managed rules and rate limiting Transport Layer: HTTPS encryption with SSL/TLS Storage Layer: Private S3 bucket with OAC Best Practices Summary For Production:\nStart with managed rule groups for baseline protection Use Count mode to test rules before blocking Monitor sampled requests regularly for false positives Set up CloudWatch alarms for unusual activity Enable logging for audit and compliance Review and update rules based on traffic patterns Use IP sets for dynamic allow/block lists Implement appropriate rate limits for your use case Document rule changes and exceptions Security Monitoring:\nCheck WAF dashboard weekly Review blocked requests for attack patterns Investigate spikes in blocked traffic Update rules as new threats emerge Keep managed rules enabled for auto-updates Next Steps You\u0026rsquo;ve completed the frontend deployment! Your static website is now:\nGlobally distributed with CloudFront Protected by AWS WAF Served over HTTPS Secured with private S3 access Proceed to Workshop 2 to build the backend:\nAPI Gateway for RESTful APIs Lambda functions for business logic RDS for database storage Cognito for user authentication Secrets Manager for credentials Useful Commands:\n# Get Web ACL details aws wafv2 get-web-acl \\ --scope CLOUDFRONT \\ --id YOUR-WEBACL-ID \\ --name workshop-frontend-waf # List Web ACLs aws wafv2 list-web-acls \\ --scope CLOUDFRONT # Get sampled requests aws wafv2 get-sampled-requests \\ --web-acl-arn YOUR-WEBACL-ARN \\ --rule-metric-name RateLimitRule \\ --scope CLOUDFRONT \\ --time-window StartTime=1234567890,EndTime=1234567900 \\ --max-items 100 # Update rule action to Count (testing) aws wafv2 update-web-acl \\ --scope CLOUDFRONT \\ --id YOUR-WEBACL-ID \\ --name workshop-frontend-waf \\ --default-action Allow={} \\ # ... (additional parameters) WAF Dashboard URLs:\nWeb ACL Overview: https://console.aws.amazon.com/wafv2/homev2/web-acl/workshop-frontend-waf/ CloudWatch Metrics: https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#metricsV2:query=~(metricName~'BlockedRequests) Congratulations! You\u0026rsquo;ve successfully deployed and secured a production-ready serverless frontend application on AWS! üéâ\nUnderstanding Web ACL JSON { \u0026#34;ARN\u0026#34;: \u0026#34;arn:aws:wafv2:us-east-1:362324939369:global/webacl/CreatedByCloudFront-cf37def6/78a29ce4-287f-47a4-b108-886bfc3ae748\u0026#34;, \u0026#34;Capacity\u0026#34;: 925, \u0026#34;DefaultAction\u0026#34;: { \u0026#34;Allow\u0026#34;: {} }, \u0026#34;Description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Id\u0026#34;: \u0026#34;78a29ce4-287f-47a4-b108-886bfc3ae748\u0026#34;, \u0026#34;LabelNamespace\u0026#34;: \u0026#34;awswaf:362324939369:webacl:CreatedByCloudFront-cf37def6:\u0026#34;, \u0026#34;ManagedByFirewallManager\u0026#34;: false, \u0026#34;Name\u0026#34;: \u0026#34;CreatedByCloudFront-cf37def6\u0026#34;, \u0026#34;OnSourceDDoSProtectionConfig\u0026#34;: { \u0026#34;ALBLowReputationMode\u0026#34;: \u0026#34;ACTIVE_UNDER_DDOS\u0026#34; }, \u0026#34;RetrofittedByFirewallManager\u0026#34;: false, \u0026#34;Rules\u0026#34;: [ { \u0026#34;Name\u0026#34;: \u0026#34;AWS-AWSManagedRulesAmazonIpReputationList\u0026#34;, \u0026#34;OverrideAction\u0026#34;: { \u0026#34;None\u0026#34;: {} }, \u0026#34;Priority\u0026#34;: 0, \u0026#34;Statement\u0026#34;: { \u0026#34;ManagedRuleGroupStatement\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;AWSManagedRulesAmazonIpReputationList\u0026#34;, \u0026#34;VendorName\u0026#34;: \u0026#34;AWS\u0026#34; } }, \u0026#34;VisibilityConfig\u0026#34;: { \u0026#34;CloudWatchMetricsEnabled\u0026#34;: true, \u0026#34;MetricName\u0026#34;: \u0026#34;AWS-AWSManagedRulesAmazonIpReputationList\u0026#34;, \u0026#34;SampledRequestsEnabled\u0026#34;: true } }, { \u0026#34;Name\u0026#34;: \u0026#34;AWS-AWSManagedRulesCommonRuleSet\u0026#34;, \u0026#34;OverrideAction\u0026#34;: { \u0026#34;None\u0026#34;: {} }, \u0026#34;Priority\u0026#34;: 1, \u0026#34;Statement\u0026#34;: { \u0026#34;ManagedRuleGroupStatement\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;AWSManagedRulesCommonRuleSet\u0026#34;, \u0026#34;VendorName\u0026#34;: \u0026#34;AWS\u0026#34; } }, \u0026#34;VisibilityConfig\u0026#34;: { \u0026#34;CloudWatchMetricsEnabled\u0026#34;: true, \u0026#34;MetricName\u0026#34;: \u0026#34;AWS-AWSManagedRulesCommonRuleSet\u0026#34;, \u0026#34;SampledRequestsEnabled\u0026#34;: true } }, { \u0026#34;Name\u0026#34;: \u0026#34;AWS-AWSManagedRulesKnownBadInputsRuleSet\u0026#34;, \u0026#34;OverrideAction\u0026#34;: { \u0026#34;None\u0026#34;: {} }, \u0026#34;Priority\u0026#34;: 2, \u0026#34;Statement\u0026#34;: { \u0026#34;ManagedRuleGroupStatement\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;AWSManagedRulesKnownBadInputsRuleSet\u0026#34;, \u0026#34;VendorName\u0026#34;: \u0026#34;AWS\u0026#34; } }, \u0026#34;VisibilityConfig\u0026#34;: { \u0026#34;CloudWatchMetricsEnabled\u0026#34;: true, \u0026#34;MetricName\u0026#34;: \u0026#34;AWS-AWSManagedRulesKnownBadInputsRuleSet\u0026#34;, \u0026#34;SampledRequestsEnabled\u0026#34;: true } } ], \u0026#34;VisibilityConfig\u0026#34;: { \u0026#34;CloudWatchMetricsEnabled\u0026#34;: true, \u0026#34;MetricName\u0026#34;: \u0026#34;CreatedByCloudFront-cf37def6\u0026#34;, \u0026#34;SampledRequestsEnabled\u0026#34;: true } } ARN (Amazon Resource Name) \u0026#34;ARN\u0026#34;: \u0026#34;arn:aws:wafv2:us-east-1:362324939369:global/webacl/CreatedByCloudFront-cf37def6/78a29ce4-287f-47a4-b108-886bfc3ae748\u0026#34; What it is: A unique identifier for your Web ACL across all of AWS.\nBreaking down the ARN:\nwafv2: Service (AWS WAF version 2) us-east-1: Region where metadata is stored (all CloudFront WAF uses us-east-1) 362324939369: Your AWS Account ID global: Scope (CloudFront resources are global) webacl: Resource type CreatedByCloudFront-cf37def6: Web ACL name 78a29ce4-287f-47a4-b108-886bfc3ae748: Unique Web ACL ID Capacity \u0026#34;Capacity\u0026#34;: 925 What it means: This Web ACL is using 925 out of the maximum 1,500 Web ACL Capacity Units (WCUs).\nCapacity breakdown:\nAmazon IP Reputation List: ~25 WCUs Common Rule Set: ~700 WCUs Known Bad Inputs: ~200 WCUs Total: 925 WCUs Remaining capacity: 575 WCUs (you can add more rules) About WCU Capacity: Each rule and managed rule group consumes WCUs based on its complexity. The 1,500 WCU limit ensures optimal performance. If you need more capacity, you can:\nRemove unused rules\nCreate multiple Web ACLs for different distributions\nUse custom rule groups to optimize capacity usage\nDefault Action \u0026#34;DefaultAction\u0026#34;: { \u0026#34;Allow\u0026#34;: {} } What it means: Requests that don\u0026rsquo;t match any rules are allowed by default.\nThis is the recommended approach because:\nRules explicitly block malicious traffic Legitimate traffic passes through by default Reduces risk of blocking valid users Alternative: Setting default action to \u0026ldquo;Block\u0026rdquo; would require explicit Allow rules for all legitimate traffic (not recommended for most use cases).\nID and Name \u0026#34;Id\u0026#34;: \u0026#34;78a29ce4-287f-47a4-b108-886bfc3ae748\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;CreatedByCloudFront-cf37def6\u0026#34; ID: Unique identifier used in API calls Name: Human-readable name (auto-generated if created via CloudFront console) Label Namespace \u0026#34;LabelNamespace\u0026#34;: \u0026#34;awswaf:362324939369:webacl:CreatedByCloudFront-cf37def6:\u0026#34; What it is: A prefix for labels applied by rules in this Web ACL.\nLabels are tags that rules can add to requests. Other rules can then match on these labels for advanced logic. For example:\nOne rule labels a request as \u0026ldquo;suspicious\u0026rdquo; Another rule blocks all \u0026ldquo;suspicious\u0026rdquo; labeled requests Firewall Manager Settings \u0026#34;ManagedByFirewallManager\u0026#34;: false, \u0026#34;RetrofittedByFirewallManager\u0026#34;: false What it means: This Web ACL is not managed by AWS Firewall Manager.\nAWS Firewall Manager is a service for centrally managing security policies across multiple AWS accounts. Since these are false:\nYou have full control to modify this Web ACL Changes won\u0026rsquo;t be overridden by organizational policies You\u0026rsquo;re managing security at the individual account level DDoS Protection Configuration \u0026#34;OnSourceDDoSProtectionConfig\u0026#34;: { \u0026#34;ALBLowReputationMode\u0026#34;: \u0026#34;ACTIVE_UNDER_DDOS\u0026#34; } What it is: Configuration for how WAF handles traffic from low-reputation sources during DDoS attacks.\nACTIVE_UNDER_DDOS mode:\nDuring normal operations: All traffic processed normally During active DDoS attack: WAF applies additional scrutiny to requests from IPs with low reputation scores This provides an extra layer of protection when your application is under attack About AWS Shield: This configuration works in conjunction with AWS Shield Standard, which is automatically included with CloudFront at no extra cost. Shield Standard provides:\nDDoS protection at network and transport layers\nAutomatic detection and mitigation\nAlways-on protection\nFor enhanced protection, you can upgrade to AWS Shield Advanced (additional cost).\nRules Configuration Your Web ACL contains 3 managed rule groups, evaluated in priority order (0 ‚Üí 1 ‚Üí 2).\nRule 1: Amazon IP Reputation List (Priority 0) { \u0026#34;Name\u0026#34;: \u0026#34;AWS-AWSManagedRulesAmazonIpReputationList\u0026#34;, \u0026#34;Priority\u0026#34;: 0, \u0026#34;Statement\u0026#34;: { \u0026#34;ManagedRuleGroupStatement\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;AWSManagedRulesAmazonIpReputationList\u0026#34;, \u0026#34;VendorName\u0026#34;: \u0026#34;AWS\u0026#34; } }, \u0026#34;OverrideAction\u0026#34;: { \u0026#34;None\u0026#34;: {} }, \u0026#34;VisibilityConfig\u0026#34;: { \u0026#34;CloudWatchMetricsEnabled\u0026#34;: true, \u0026#34;MetricName\u0026#34;: \u0026#34;AWS-AWSManagedRulesAmazonIpReputationList\u0026#34;, \u0026#34;SampledRequestsEnabled\u0026#34;: true } } What it does:\nBlocks requests from IP addresses known for malicious activity AWS maintains a constantly updated list of bad IPs based on threat intelligence Includes IPs associated with: Botnets Spam campaigns Malware distribution DDoS attacks Priority 0: Evaluated first - if a request comes from a known bad IP, it\u0026rsquo;s blocked immediately without checking other rules.\nOverrideAction: None:\nUses the default action defined in the managed rule group (Block) No overrides applied All rules within this group remain active VisibilityConfig:\nCloudWatchMetricsEnabled: true - Metrics sent to CloudWatch MetricName - Used in CloudWatch for filtering/alerting SampledRequestsEnabled: true - Stores sample requests for analysis in WAF console Rule 2: Common Rule Set (Priority 1) { \u0026#34;Name\u0026#34;: \u0026#34;AWS-AWSManagedRulesCommonRuleSet\u0026#34;, \u0026#34;Priority\u0026#34;: 1, \u0026#34;Statement\u0026#34;: { \u0026#34;ManagedRuleGroupStatement\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;AWSManagedRulesCommonRuleSet\u0026#34;, \u0026#34;VendorName\u0026#34;: \u0026#34;AWS\u0026#34; } }, \u0026#34;OverrideAction\u0026#34;: { \u0026#34;None\u0026#34;: {} }, \u0026#34;VisibilityConfig\u0026#34;: { \u0026#34;CloudWatchMetricsEnabled\u0026#34;: true, \u0026#34;MetricName\u0026#34;: \u0026#34;AWS-AWSManagedRulesCommonRuleSet\u0026#34;, \u0026#34;SampledRequestsEnabled\u0026#34;: true } } What it does:\nProvides broad protection against common web exploits Based on OWASP Top 10 vulnerabilities Protects against: Cross-Site Scripting (XSS): Malicious script injection Local File Inclusion (LFI): Unauthorized file access Remote File Inclusion (RFI): Loading external malicious files Command Injection: OS command execution attempts Path Traversal: Directory traversal attacks (e.g., ../../etc/passwd) SQL Injection: Basic SQL injection patterns Session Fixation: Session hijacking attempts Priority 1: Evaluated after IP reputation check. If the request passes the IP check, it\u0026rsquo;s then inspected for common attack patterns.\nWhy it\u0026rsquo;s important:\nThis is the most comprehensive rule group Covers the majority of common web attacks Regularly updated by AWS security team Uses 700 WCUs (the largest rule group) Rule 3: Known Bad Inputs (Priority 2) { \u0026#34;Name\u0026#34;: \u0026#34;AWS-AWSManagedRulesKnownBadInputsRuleSet\u0026#34;, \u0026#34;Priority\u0026#34;: 2, \u0026#34;Statement\u0026#34;: { \u0026#34;ManagedRuleGroupStatement\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;AWSManagedRulesKnownBadInputsRuleSet\u0026#34;, \u0026#34;VendorName\u0026#34;: \u0026#34;AWS\u0026#34; } }, \u0026#34;OverrideAction\u0026#34;: { \u0026#34;None\u0026#34;: {} }, \u0026#34;VisibilityConfig\u0026#34;: { \u0026#34;CloudWatchMetricsEnabled\u0026#34;: true, \u0026#34;MetricName\u0026#34;: \u0026#34;AWS-AWSManagedRulesKnownBadInputsRuleSet\u0026#34;, \u0026#34;SampledRequestsEnabled\u0026#34;: true } } What it does:\nBlocks requests with patterns that are known to be invalid or exploits Focuses on malformed inputs that should never occur in legitimate traffic Protects against: Malformed request patterns: Requests that violate HTTP standards Invalid characters: Special characters in unexpected places Known exploit patterns: Signatures of well-known vulnerabilities CVE exploits: Patterns matching published Common Vulnerabilities and Exposures Priority 2: Last line of defense. If a request passes IP reputation and common rule checks, this rule group catches any remaining known malicious patterns.\nWhy it\u0026rsquo;s useful:\nVery low false positive rate (rarely blocks legitimate traffic) Catches exploit attempts targeting specific vulnerabilities Complements the Common Rule Set with more specific patterns How Rules are Evaluated Evaluation Flow Request arrives\r‚Üì\rPriority 0: IP Reputation Check\r‚îú‚îÄ Match ‚Üí Block (403 Forbidden)\r‚îî‚îÄ No match ‚Üí Continue\r‚Üì\rPriority 1: Common Rule Set\r‚îú‚îÄ Match ‚Üí Block (403 Forbidden)\r‚îî‚îÄ No match ‚Üí Continue\r‚Üì\rPriority 2: Known Bad Inputs\r‚îú‚îÄ Match ‚Üí Block (403 Forbidden)\r‚îî‚îÄ No match ‚Üí Continue\r‚Üì\rDefault Action: Allow\r‚Üì\rRequest forwarded to CloudFront Key Points First match wins: When a rule matches, its action is taken immediately (if it\u0026rsquo;s a Block action) Priority matters: Lower numbers are evaluated first (0 before 1 before 2) Managed rules are efficient: Even with 700 WCUs, the Common Rule Set evaluates very quickly Default action only applies if no rules match: Most requests will either match a rule or reach the default Allow Visibility and Monitoring \u0026#34;VisibilityConfig\u0026#34;: { \u0026#34;CloudWatchMetricsEnabled\u0026#34;: true, \u0026#34;MetricName\u0026#34;: \u0026#34;CreatedByCloudFront-cf37def6\u0026#34;, \u0026#34;SampledRequestsEnabled\u0026#34;: true } CloudWatch Metrics Enabled Metrics automatically sent to CloudWatch:\nTotal requests Allowed requests Blocked requests Counted requests (if any rules use Count action) Per-rule metrics Access metrics:\nCloudWatch console ‚Üí Metrics ‚Üí WAF Namespace: AWS/WAFV2 Dimensions: WebACL, Rule, Region Sampled Requests Enabled What it captures:\nUp to 100 recent requests per rule Includes both allowed and blocked requests Request details: IP, URI, headers, action taken View sampled requests:\nWAF Console ‚Üí Your Web ACL Scroll to \u0026ldquo;Sampled requests\u0026rdquo; section Click on any request to see full details Use cases:\nTroubleshooting false positives Understanding attack patterns Verifying rule effectiveness "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/4-eventparticipated/4.5-event5/",
	"title": "DevOps on AWS",
	"tags": [],
	"description": "",
	"content": "Summary Report Event Objectives Instill the DevOps mindset, covering its culture, principles, and key performance metrics. Provide a deep dive into building CI/CD pipelines using native AWS DevOps services. Teach Infrastructure as Code (IaC) principles using AWS CloudFormation and the AWS CDK. Explore the AWS container ecosystem, including Docker, Amazon ECR, ECS, EKS, and App Runner. Demonstrate how to implement comprehensive monitoring and observability using CloudWatch and AWS X-Ray. Speakers Truong Quang Tinh - Platform Engineer (TymeX), AWS Community Builder Bao Huynh - AWS Community Builder Thinh Nguyen - AWS Community Builder Vi Tran - AWS Community Builder Van Hoang Kha ‚Äì Cloud Engineer, AWS Community Builder Long Huynh - AWS Community Builder Quy Pham - AWS Community Builder Nghiem Le - AWS Community Builder Key Highlights From Manual Operations to Infrastructure as Code (IaC) The workshop highlighted the pitfalls of \u0026ldquo;ClickOps\u0026rdquo; (manual console-based management), such as being slow, error-prone, and difficult to replicate. AWS CloudFormation: Introduced as the native IaC solution, using YAML/JSON templates to define and manage AWS resources in \u0026ldquo;Stacks\u0026rdquo; and its ability to detect configuration drift. AWS Cloud Development Kit (CDK): Presented as a developer-centric IaC framework that allows defining infrastructure in familiar programming languages (e.g., Python, TypeScript), using reusable \u0026ldquo;Constructs\u0026rdquo; to accelerate development. Building a Full CI/CD Pipeline A complete, automated pipeline was demonstrated using the suite of AWS developer tools: AWS CodeCommit: For secure source control. AWS CodeBuild: For automated builds and testing. AWS CodeDeploy: For managing complex deployments like Blue/Green and Canary releases. AWS CodePipeline: To orchestrate the entire release process from source to deployment. Containerization and Orchestration The session covered the fundamentals of containerization with Docker and the importance of a container registry like Amazon ECR for storing and scanning images. A detailed comparison of orchestration services was provided: Amazon ECS: An AWS-native, simpler solution deeply integrated with the AWS ecosystem, ideal for teams wanting lower operational overhead. Amazon EKS: A managed Kubernetes service that aligns with the open-source standard, offering greater flexibility and multi-cloud portability at the cost of higher complexity. AWS Fargate \u0026amp; App Runner: Serverless compute options that remove the need to manage underlying servers for containers, simplifying deployment and operations. Monitoring and Observability The importance of full-stack observability was emphasized for maintaining and debugging distributed systems. Amazon CloudWatch: Used for collecting metrics, logs, and setting up alarms and dashboards. AWS X-Ray: Demonstrated for distributed tracing to analyze and debug performance bottlenecks in microservices architectures. Key Takeaways Design Mindset Automate Everything: Transition from manual \u0026ldquo;ClickOps\u0026rdquo; to a fully automated IaC approach to ensure consistency, speed, and reliability. Infrastructure as Code is Non-Negotiable: IaC is the foundation for modern DevOps, enabling collaboration, versioning, and reproducibility of environments. Choose the Right Tool for the Team: The choice between CloudFormation, CDK, ECS, and EKS should be based on team skills, ecosystem needs, and the desired balance between simplicity and control. Technical Architecture CI/CD Pipelines: Every project should have an automated pipeline that handles code integration, testing, and deployment to ensure rapid and safe releases. Container-First for Microservices: Use containers to package applications and their dependencies, and an orchestrator (ECS or EKS) to manage them at scale. Full-Stack Observability: Implement a robust monitoring strategy with metrics, logs (CloudWatch), and distributed tracing (X-Ray) to gain deep insights into application performance and health. Modernization Strategy Phased Adoption: Introduce DevOps practices incrementally. Start by converting one manual process to IaC or building a CI/CD pipeline for a single service. Leverage Serverless: Use serverless options like AWS Fargate and App Runner to reduce operational complexity and allow teams to focus on application logic rather than infrastructure management. Measure What Matters: Focus on key DevOps metrics like Deployment Frequency, Lead Time for Changes, and Mean Time to Recovery (MTTR) to drive continuous improvement. Applying to Work Automate a Deployment: Convert a manually deployed application to use an AWS CodePipeline workflow. Codify Infrastructure: Define an existing S3 bucket or EC2 instance using an AWS CloudFormation template or a CDK application. Containerize an Application: Create a Dockerfile for a web application and push the image to Amazon ECR. Pilot a Container Service: Deploy a simple containerized application using AWS App Runner or ECS with the Fargate launch type. Improve Observability: Create a CloudWatch Dashboard for a critical application and configure alarms for key metrics like CPU utilization and error rates. Event Experience Attending the \u0026ldquo;DevOps on AWS\u0026rdquo; workshop was extremely valuable, offering a comprehensive and practical guide to implementing modern DevOps practices on the cloud.\nLearning from highly skilled speakers The AWS Community Builders provided deep, practical knowledge, breaking down complex topics into understandable concepts. Hands-on technical exposure The multiple live demos, including a full CI/CD pipeline walkthrough and a microservices deployment on ECS, provided a clear, real-world context for the tools and services discussed. Leveraging modern tools The workshop provided a thorough exploration of the modern AWS DevOps toolkit, from advanced IaC with the CDK to serverless containers with App Runner. Networking and discussions The Q\u0026amp;A sessions offered opportunities to discuss career pathways and the AWS certification roadmap, providing valuable guidance for professional development. Lessons learned Adopting IaC is the single most impactful step towards achieving a mature DevOps practice. AWS provides a complete, integrated toolset to build a sophisticated DevOps platform, with options suitable for teams of all sizes and skill levels. Effective monitoring and observability are not optional; they are critical for operating reliable and performant applications in the cloud. Some event photos "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives: Implement advanced networking techniques with VPC Peering and Transit Gateway to interconnect multiple VPCs. Deploy a full-stack application using EC2, RDS, Auto Scaling and integrate with CloudFront. Build a cost-optimization solution using serverless patterns with AWS Lambda to automate EC2 management. Establish a CI/CD pipeline using AWS Developer Tools for automated deployments. Configure hybrid cloud storage with AWS Storage Gateway to connect on-premises environments. Manage enterprise file systems with Amazon FSx and strengthen web security with AWS WAF. Organize AWS resources effectively using Tags and Resource Groups. Improve operational skills using both the AWS Management Console and the AWS CLI. Tasks carried out this week: Day Task Start Date Completion Date Reference Material 2 - Set up VPC Peering between two VPCs:\n+ Provision the environment using CloudFormation\n+ Create Security Groups for EC2\n+ Launch EC2 instances in each VPC to verify connectivity\n+ Update Network ACLs\n+ Create \u0026amp; accept the Peering connection\n+ Configure Route Tables for routing\n+ Enable Cross-Peer DNS to resolve hostnames\n- Deploy a hub-and-spoke network using Transit Gateway:\n+ Create Key Pair\n+ Provision environment using CloudFormation\n+ Create a Transit Gateway as the central connector\n+ Attach VPCs to the Transit Gateway\n+ Configure Transit Gateway Route Tables\n+ Update VPC Route Tables 10/06/2025 10/06/2025 VPC Peering: https://000019.awsstudygroup.com/ Transit Gateway: https://000020.awsstudygroup.com/ 3 - Deploy WordPress on AWS:\n+ Prepare VPC/Subnets\n+ Create Security Groups for EC2 and RDS\n+ Launch EC2 host for WordPress\n+ Provision RDS for the database\n+ Install and configure WordPress\n+ Set up Auto Scaling\n+ Perform database backup/restore\n+ Integrate CloudFront to improve performance\n- Optimize EC2 costs with Lambda:\n+ Tag EC2 instances for cost management\n+ Create IAM Role for Lambda\n+ Write Lambda to automatically stop/start EC2 instances\n+ Test Lambda behavior 10/07/2025 10/07/2025 WordPress: https://000021.awsstudygroup.com/ Lambda Optimization: https://000022.awsstudygroup.com/ 4 - Automate application deployment with a CI/CD pipeline:\n+ Prepare required resources\n+ Install the CodeDeploy Agent on EC2\n+ Create a CodeCommit repository for source code\n+ Configure CodeBuild to build the application\n+ Set up CodeDeploy for automated deployments\n+ Build CodePipeline to orchestrate the pipeline\n+ Troubleshoot pipeline execution issues\n- Use Storage Gateway for hybrid cloud storage:\n+ Create an S3 Bucket\n+ Launch an EC2 host for the Storage Gateway appliance\n+ Activate the Storage Gateway\n+ Create file shares\n+ Mount file shares on on-premises machines 10/08/2025 10/08/2025 CI/CD: https://000023.awsstudygroup.com/ Storage Gateway: https://000024.awsstudygroup.com/ 5 - Manage Amazon FSx for Windows File Server:\n+ Create the environment\n+ Provision SSD and HDD Multi-AZ file systems\n+ Create file shares\n+ Run performance tests\n+ Enable Data Deduplication \u0026amp; Shadow Copies\n+ Manage sessions, open files, and quotas\n+ Enable Continuous Access shares\n+ Scale throughput and storage\n+ Delete the environment when finished\n+ Reference AWS CLI for FSx management\n- Deploy AWS WAF:\n+ Create an S3 bucket and deploy a sample web app\n+ Use Managed Rules\n+ Create advanced Custom Rules\n+ Test rules\n+ Enable logging\n+ Clean up resources 10/09/2025 10/09/2025 Amazon FSx: https://000025.awsstudygroup.com/ AWS WAF: https://000026.awsstudygroup.com/ 6 - Manage resources with Tags \u0026amp; Resource Groups:\n+ Understand and apply tags in the Console\n+ Launch EC2 with tags\n+ Add/remove tags on resources\n+ Filter resources by tags\n+ Use tags with the AWS CLI\n+ Tag EC2 via CLI\n+ Tag resources at creation via CLI\n+ List tagged resources via CLI\n+ Create Resource Groups based on tags\n+ Manage resources within Resource Groups 10/10/2025 10/10/2025 Tags \u0026amp; Resource Groups: https://000027.awsstudygroup.com/ Week 5 Achievements: Completed advanced networking on AWS:\nVPC Peering to connect VPCs directly Transit Gateway as a centralized connectivity hub Configured routing and DNS across multiple VPCs Deployed and optimized cloud applications:\nWordPress integrated with RDS Auto Scaling + CloudFront for improved performance and availability Lambda-based automation for EC2 cost optimization Built a complete DevOps workflow:\nFull CI/CD pipeline using CodeCommit ‚Äì CodeBuild ‚Äì CodeDeploy ‚Äì CodePipeline Automated deployment process Hybrid storage solution via Storage Gateway Enterprise file management and web security:\nFSx Multi-AZ setup, quotas management, and deduplication AWS WAF with advanced rule management Applied effective resource governance:\nCost optimization \u0026amp; resource management using Tags Centralized resource grouping with Resource Groups Advanced operations via both Console \u0026amp; CLI Gained hands-on experience with Infrastructure as Code using CloudFormation to create consistent environments.\n"
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/5-workshop/5.2-serverless-backend/5.2.5-lambda/",
	"title": "Lambda Functions Development",
	"tags": [],
	"description": "",
	"content": "Overview In this section, you\u0026rsquo;ll create AWS Lambda functions that serve as the business logic layer of your serverless backend. These functions will handle API requests, retrieve database credentials from Secrets Manager, interact with your RDS database, and return responses to API Gateway.\nWhat you\u0026rsquo;ll accomplish:\nUnderstand Lambda function architecture and execution Develop Lambda functions for CRUD operations Connect Lambda to RDS PostgreSQL database Retrieve credentials securely from Secrets Manager Configure VPC settings for database access Package and deploy Lambda functions with dependencies Test Lambda functions directly Set up CloudWatch Logs for debugging Initialize database schema via Lambda Estimated time: 60-75 minutes\nLambda Function Architecture What We\u0026rsquo;ll Build API Gateway\r‚Üì\rLambda Functions (in VPC)\r‚îú‚îÄ‚îÄ initDB - Initialize database schema\r‚îú‚îÄ‚îÄ createUser - Create new user\r‚îú‚îÄ‚îÄ getUsers - List all users\r‚îú‚îÄ‚îÄ createTask - Create new task\r‚îú‚îÄ‚îÄ getTasks - Get user\u0026#39;s tasks\r‚îú‚îÄ‚îÄ updateTask - Update task\r‚îî‚îÄ‚îÄ deleteTask - Delete task\r‚Üì\rVPC Endpoint (Secrets Manager)\r‚Üì\rSecrets Manager (get DB credentials)\r‚Üì\rRDS PostgreSQL (execute queries) Costs Considerations Free-tier: 1 million free requests per month 400,000 GB-seconds of compute time per month Applies to both x86 and Graviton2 Lambda functions 100 GiB of HTTP response streaming per month The first 6 MB per request is always free Paid-tier Even paid tier cost is minimal for our workshop Overall: \u0026lt;$1 (clean up immediately after finish workshop) Step 1: Prepare Lambda Development Environment (optional) In this step, you\u0026rsquo;ll set up a local development environment for building and packaging your Lambda function. However, this step is optional. If you prefer not to set up the environment manually, you can skip this step and download source code below. You can download source code for this part here:\nGithub repository: https://github.com/Icyretsz/fcj-workshop-serverless-backend-ver1 Only the zip file for lambda: https://fcj-workshop-files.s3.ap-southeast-1.amazonaws.com/userHandler.zip 1.1 View Project Directory The source code is provided in the above git repository\nThe project structure:\nfcj-workshop-serverless-backend-ver1\r‚îî‚îÄ‚îÄ backend/\r‚îî‚îÄ‚îÄ src/\r‚îî‚îÄ‚îÄ userHandler.ts\r‚îî‚îÄ‚îÄ types.ts\r‚îú‚îÄ‚îÄ package.json\r‚îú‚îÄ‚îÄ tsconfig.json You can choose to organize your Lambda functions in different ways. In production, it is recommended to create separate Lambda functions for each user operation (CREATE, READ, UPDATE, DELETE) to improve maintainability, scalability, and security.\nHowever, for the sake of simplicity in this workshop, we will place all user-related operations into a single Lambda function (userHandler.js).\nThe userHandler.ts Key features of this handler:\nDB Initialization: creates users table and insert mock data automatically if it doesn‚Äôt exist. TypeScript types: uses your User interface and ApiResponse. Secrets Manager: retrieves database credentials at runtime. No connection pool: safe for workshops, simple enough. This Lambda function handles CRUD operations for users in a PostgreSQL database. It\u0026rsquo;s designed to work with API Gateway Lambda Proxy Integration, which provides a specific event structure and expects a specific response format. Supported operations:\nGET /users - Get all users GET /users/{id} - Get single user POST /users - Create new user PUT /users/{id} - Update user DELETE /users/{id} - Delete user You can modify it as you like. When you are ready, move on to next steps to build and deploy our source code to Lambda\nStep 2: Build Lambda Deployment Package 2.1 Install dependencies From root folder, run\nnpm install 2.1 Build TypeScript Code Build\nnpm run build This creates dist folder in root directory\nNow find node_modules folder then copy it to dist folder\nWhen complete, select all files in dist folder, compress them to zip and name it userHandler\nConfirm contents of userHandler.zip\nStep 3: Deploy Lambda Functions Now we\u0026rsquo;ll create Lambda functions in AWS\n3.1 Navigate to Lambda console Navigate to Lambda console Click Create function 3.2 Create Lambda function Function options: Select Author from scratch Basic information: Function name: workshop-lambda-sm-rds Runtime: Nodejs 24.x Architecture: x86_64 Expand Change default execution role Select Use an existing role Select workshop-lamda-secretsmng-role Advanced settings: Expand Advanced settings Check Enable VPC VPC:\nSelect workshop-backend-vpc Subnets:\nSelect workshop-private-subnet-1 (10.0.1.0/24) Security groups:\nSelect workshop-lambda-sg Click Create function Wait a few minutes for the system to create the Lambda function Create successful 3.3 Upload Deployment Package In the function page, in Code tab Click Upload from dropdown Select .zip file Click Upload Select /fcj-serverless-workshop/backend/userHandler.zip Click Save Lambda will start importing our source code (userHandler.zip) The result: you will see the source code files in the left sidebar 3.4 Configure environment variables Go to the Configuration tab On the left sidebar, click Environment variables Click Edit On the next screen, click Add environment variable Add the following variables: RDS-HOST: your RDS endpoint DB_NAME: your RDS database name SECRET_NAME: the secret name of the RDS-managed secret (refer to 5.2.4) REGION: your current AWS region Go back to Code tab, click Deploy 3.5 Test Lambda function Now let\u0026rsquo;s test the Lambda function. We will test the get all users route\nIn the Code tab of your Lambda function, click the Test button or Create new test event on the left sidebar A drawer on the right will appear Configure the test event: Event name: test-get-users Event JSON: { \u0026#34;httpMethod\u0026#34;: \u0026#34;GET\u0026#34;, \u0026#34;pathParameters\u0026#34;: null, \u0026#34;body\u0026#34;: null } Click Save You will see your new test event in the left sidebar, hover on it and then click the play button to start the test If everything goes well, you will see the response with status 200 and a body contains our users Step 4: Monitor Lambda with CloudWatch Logs 4.1 View CloudWatch Logs In Lambda console, click Monitor tab Click View CloudWatch logs Click on the latest Log stream 4.2 Analyze Log Entries You should see detailed logs for each invocation:\n2024-11-29T10:00:00.000Z INFO [users-handler] process start.\r2024-11-29T10:00:00.100Z INFO Failed to get secret: Error: Could not retrieve secret\r2024-11-29T10:00:00.150Z INFO Connected to RDS PostgreSQL successfully.\r2024-11-29T10:00:00.200Z INFO Inserting demo users...\r2024-11-29T10:00:00.250Z INFO Demo users inserted.\r2024-11-29T10:00:00.300Z INFO [users-handler] process end. Key things to observe:\nSuccessful database connection Table initialization Demo data insertion (first run only) Request method and path Query execution and results 4.3 Understanding Log Output Successful Create User:\nSTART RequestId: abc-123-def\r[INFO] [users-handler] process start.\r[INFO] Connected to RDS PostgreSQL successfully.\r[INFO] Demo users inserted.\rEND RequestId: abc-123-def\rREPORT RequestId: abc-123-def Duration: 1250.34 ms Billed Duration: 1251 ms Memory Size: 512 MB Max Memory Used: 128 MB Init Duration: 2345.67 ms What each metric means:\nDuration: Actual execution time (1250.34 ms) Billed Duration: Rounded up time you\u0026rsquo;re charged for (1251 ms) Memory Size: Allocated memory (512 MB) Max Memory Used: Peak memory usage (128 MB) Init Duration: Cold start initialization time (2345.67 ms, first invocation only) Error Example:\nSTART RequestId: xyz-789-abc\r[INFO] [users-handler] process start.\r[ERROR] Failed to get secret: ResourceNotFoundException: Secret not found\r[ERROR] RDS connection failed: Error: Connection timeout\r[INFO] [users-handler] process end.\rEND RequestId: xyz-789-abc\rREPORT RequestId: xyz-789-abc Duration: 5000.12 ms Billed Duration: 5001 ms Memory Size: 512 MB Max Memory Used: 95 MB 4.4 Filter Logs by Pattern CloudWatch Logs Insights allows you to query logs with SQL-like syntax.\nIn CloudWatch Logs, click Logs Insights in left navigation Select your log group: /aws/lambda/workshop-lambda-sm-rds Enter a query Click Run query Example queries:\nFind all errors:\nfields @timestamp, @message | filter @message like /ERROR/ | sort @timestamp desc | limit 20 Find slow requests (\u0026gt; 2 seconds):\nfields @timestamp, @duration | filter @type = \u0026#34;REPORT\u0026#34; | filter @duration \u0026gt; 2000 | sort @duration desc | limit 20 Count requests by HTTP method:\nfields @timestamp, @message | filter @message like /httpMethod/ | parse @message \u0026#39;\u0026#34;httpMethod\u0026#34;:\u0026#34;*\u0026#34;\u0026#39; as method | stats count() by method Find database connection errors:\nfields @timestamp, @message | filter @message like /RDS connection failed/ | sort @timestamp desc | limit 20 Get average execution time:\nfields @timestamp, @duration | filter @type = \u0026#34;REPORT\u0026#34; | stats avg(@duration) as avg_duration, max(@duration) as max_duration, min(@duration) as min_duration Summary Congratulations! You\u0026rsquo;ve successfully:\nCloned and explored the TypeScript Lambda project Built and packaged the Lambda deployment Deployed Lambda function to AWS Configured VPC, security groups, and environment variables Tested CRUD operations Set up CloudWatch monitoring and logging What You\u0026rsquo;ve Built Your Lambda function now provides:\nComplete CRUD API for user management Database connectivity with automatic initialization Secure credentials via Secrets Manager VPC isolation for security Comprehensive logging for debugging Performance monitoring via CloudWatch Type-safe code with TypeScript Architecture So Far Client Request\r‚Üì\rAPI Gateway (to be created in Part 5)\r‚Üì\rLambda Function (workshop-userHandler)\r‚îú‚Üí VPC Endpoint ‚Üí Secrets Manager ‚Üí Get DB Credentials\r‚îî‚Üí VPC Private Subnet ‚Üí RDS PostgreSQL ‚Üí Execute Queries\r‚Üì\rCloudWatch Logs (monitoring \u0026amp; debugging) Next Steps Proceed to Part 5: API Gateway Setup to create REST API endpoints that will trigger your Lambda function and expose it to the internet.\nReady to continue? Your Lambda function is now fully functional and ready to be exposed via API Gateway! üöÄ\n"
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/5-workshop/5.1-frontend-deployment/5.1.5-clean-up/",
	"title": "Part 4: Clean up",
	"tags": [],
	"description": "",
	"content": "Overview This section covers how to properly clean up the resources you created during frontend deployment. Cleaning up is important to avoid ongoing AWS charges, especially for resources that have monthly costs like WAF Web ACLs.\nWhat you\u0026rsquo;ll learn:\nHow to safely delete resources in the correct order Understanding resource dependencies Cost implications of keeping vs. deleting resources How to preserve configuration for future use Partial cleanup options Estimated time: 15-20 minutes\nShould You Clean Up? Keep Resources If: ‚úÖ You\u0026rsquo;re proceeding immediately to Part 2: Serverless Backend ‚úÖ You want to maintain the working frontend for reference ‚úÖ You\u0026rsquo;re using this for a real project ‚úÖ Costs are acceptable for your use case Clean Up If: ‚úÖ You\u0026rsquo;ve completed the workshop and don\u0026rsquo;t need the resources ‚úÖ You want to minimize AWS costs ‚úÖ You\u0026rsquo;re practicing and will recreate later ‚úÖ You\u0026rsquo;re approaching Free Tier limits Complete Cleanup (Step-by-Step) Follow this order to avoid dependency errors:\nStep 1: Disable and Delete CloudFront Distribution CloudFront distributions must be disabled before deletion.\n1.1 Disable Distribution Go to CloudFront console Select your distribution (check the box) Click Disable Confirm by clicking Disable in the modal Status changes:\nDeploying: Distribution is being disabled Deployed: Disabled successfully Wait time: 5-15 minutes\n1.2 Wait for \u0026ldquo;Deployed\u0026rdquo; Status Stay on the CloudFront distributions page Refresh periodically (every 2-3 minutes) Wait until Status column shows Deployed Last modified field shows a date 2.3 Delete Distribution Select your disabled distribution (check the box) Click Delete Confirm by clicking Delete in the modal Expected result: Distribution is removed from list\nNote: If you upgraded your distribution to Pro, you must wait until the next billing cycle to delete it.\nStep 3: Delete SSL/TLS Certificate (Optional) Only if you created a custom SSL certificate in ACM for custom domains.\n3.1 Check Certificate Usage Before deleting, verify the certificate isn\u0026rsquo;t used elsewhere:\nGo to Certificate Manager console Ensure you\u0026rsquo;re in us-east-1 region Find your certificate Check the In use? column If \u0026ldquo;Yes\u0026rdquo;: Don\u0026rsquo;t delete (still associated with resources) If \u0026ldquo;No\u0026rdquo;: Safe to delete\n3.2 Delete Certificate Select your certificate (check the box) Click Delete Confirm by clicking Delete in the modal Expected result: Certificate is removed from list\nFree Service: ACM certificates are free, so deleting them doesn\u0026rsquo;t save costs. You might want to keep the certificate if:\nYou\u0026rsquo;ll recreate the distribution later\nYou use the same domain for other AWS services\nValidation took a long time (you\u0026rsquo;d have to repeat it)\nStep 4: Delete S3 Bucket S3 buckets must be empty before deletion.\n4.1 Empty the Bucket Go to S3 console Click on your bucket name: workshop-frontend-[your-name]-[random] If there are files, click Empty Type permanently delete to confirm Click Empty 4.2 Delete the Bucket Go back to the S3 buckets list Select your bucket (check the box) Click Delete Type your bucket name to confirm Click Delete bucket Step 5: Delete CloudWatch Alarms Only if you created CloudWatch alarms in the optional section.\nGo to CloudWatch console Click Alarms in left navigation Select alarm(s): WAF-High-Blocked-Requests Click Actions ‚Üí Delete Confirm deletion Step 6: Delete CloudWatch log group Click Logs -\u0026gt; Log groups Select log groups you created in Part 3 aws-waf-logs-workshop1 and select delete Step 7: Delete SNS Topics Go to SNS console Click Topics in left navigation Select topic: Default_CloudWatch_Alarms_Topic Click Delete Type delete me to confirm Click Delete Summary Cleanup Completion Checklist If performing complete cleanup:\nCloudFront distribution disabled and deleted SSL certificate deleted (if created) S3 bucket(s) emptied and deleted CloudWatch alarms deleted SNS topics deleted Verified no remaining charges in Billing Dashboard What\u0026rsquo;s Next? If continuing to Part 2: Serverless backend:\nKeep existing resources OR Proceed with backend deployment Backend will integrate with this frontend infrastructure If finished with workshop:\nAll resources cleaned up No ongoing charges Knowledge and skills gained! üéâ Congratulations! You\u0026rsquo;ve successfully completed Part 1: Frontend Deployment, including proper resource cleanup. You now understand how to deploy, secure, and manage a serverless frontend on AWS! üéâ\n"
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "Building and Deploying Fullstack Serverless Applications on AWS Workshop Overview Welcome to this comprehensive hands-on workshop on building and deploying production-ready fullstack serverless applications on AWS. Over the course of three parts, you\u0026rsquo;ll learn how to architect, secure, and automate the deployment of a modern web application using AWS\u0026rsquo;s serverless services.\nWhat You\u0026rsquo;ll Build By the end of this series, you\u0026rsquo;ll have built a complete serverless application featuring:\nSecure Frontend: A globally distributed, high-performance static website protected by AWS WAF Scalable Backend: RESTful APIs powered by Lambda functions with secure database access User Authentication: Complete user management with sign-up, login, and authorization Automated Deployment: A full CI/CD pipeline that automatically builds and deploys your application Workshop Structure Part 1: Frontend Deployment with CloudFront, WAF, and S3 Learn to deploy and secure a static frontend application using AWS\u0026rsquo;s content delivery network. You\u0026rsquo;ll configure CloudFront for global distribution, implement S3 for reliable storage, and protect your application with AWS WAF rules.\nKey Topics:\nS3 bucket configuration for static website hosting CloudFront distribution setup and optimization AWS WAF rule configuration for security Custom domain and SSL certificate management Part 2: Backend Deployment with API Gateway, Lambda, and RDS Build a secure, scalable backend infrastructure. You\u0026rsquo;ll create RESTful APIs, implement serverless functions, set up a managed database, and integrate user authentication.\nKey Topics:\nAPI Gateway REST API design and deployment Lambda function development and configuration RDS database setup and connection management AWS Secrets Manager for credential security Amazon Cognito for authentication and authorization Securing APIs with Cognito authorizers Prerequisites Required Knowledge:\nBasic understanding of web application architecture Familiarity with JavaScript/Node.js or Python Basic command line/terminal usage Understanding of HTTP and REST APIs Required Tools:\nAWS Account with administrative access AWS CLI installed and configured Text editor or IDE (VS Code recommended) Git installed locally Recommended:\nBasic understanding of SQL Familiarity with JSON Experience with version control (Git) Architecture Overview Learning Outcomes By completing this workshop series, you will be able to:\nDesign and implement serverless architectures on AWS Secure web applications using industry best practices Implement user authentication and authorization flows Manage application secrets and database credentials securely Build and maintain automated deployment pipelines Optimize applications for performance and cost Troubleshoot common serverless deployment issues Cost Considerations This workshop uses AWS services that may incur costs. We\u0026rsquo;ll use AWS Free Tier eligible services where possible, but you should:\nMonitor your AWS billing dashboard regularly Delete resources after completing each workshop if not continuing immediately Set up billing alerts to avoid unexpected charges Estimated cost for completing all workshops: $5-$15 (assuming no existing Free Tier usage)\nGetting Help Throughout the workshops, you\u0026rsquo;ll find:\nStep-by-step instructions with screenshots Code samples and configuration templates Common troubleshooting tips Links to AWS documentation for deeper dives Ready to Start? Let\u0026rsquo;s begin with Part 1: Frontend Deployment and build the foundation of your serverless application!\nContent Part 1: Frontend Deployment with CloudFront, WAF, and S3 Part 2: Serverless Backend: Backend Deployment with API Gateway, Lambda, RDS, and Cognito "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives: Learn to manage EC2 access control using IAM services with resource tags and permission boundaries. Set up and configure monitoring tools including Grafana and AWS CloudWatch. Implement AWS Systems Manager for patch management and remote command execution. Optimize EC2 instances through right-sizing practices and AWS Compute Optimizer. Apply encryption to S3 data using AWS KMS and configure audit logging. Analyze AWS costs and usage patterns with Cost Explorer. Build a data pipeline and lake using S3, Kinesis, Glue, Athena, and QuickSight. Automate infrastructure provisioning with AWS CloudFormation templates. Tasks carried out this week: Day Task Start Date Completion Date Reference Material 2 - Manage access to EC2 services with resource tags through IAM services: + Create an IAM user for preparation + Create a custom IAM Policy to define specific permissions + Set up an IAM Role to be assumed by users or services + Verify the policy by switching roles and testing access - Getting started with Grafana basic: + Create a VPC and subnet to establish a network environment + Configure a Security Group to control inbound and outbound traffic + Launch an EC2 instance to host the monitoring application + Create an IAM User and Role for secure access to AWS resources + Assign the IAM Role to the EC2 instance + Install Grafana on the EC2 instance + Set up monitoring dashboards within Grafana 10/13/2025 10/13/2025 IAM services: https://000028.awsstudygroup.com/ Grafana basic: https://000029.awsstudygroup.com/ 3 - Limit user permissions with IAM Permission Boundary: + Perform preparatory steps for the exercise + Create a restriction policy to define the maximum allowable permissions + Create a new IAM user with limited permissions + Test the IAM user\u0026rsquo;s limits to verify the permission boundaries - Manage patches and run commands on multiple servers with AWS System Manager: + Create a VPC and Subnet for the network environment + Launch a public Windows EC2 instance + Create an IAM Role with necessary permissions + Assign the IAM Role to the EC2 instance + Configure and use Patch Manager to handle server patching + Use Run Command to execute commands on the servers 10/14/2025 10/14/2025 IAM permission boundary: https://000030.awsstudygroup.com/ AWS Systems Manager: https://000031.awsstudygroup.com/ 4 - Implement right-sizing practices for Amazon EC2: + Get acquainted with Amazon CloudWatch for monitoring + Create and attach an IAM Role for the CloudWatch Agent + Install the CloudWatch Agent on an EC2 instance + Use AWS Compute Optimizer to analyze and optimize EC2 configurations - Encrypt data at rest in S3 using AWS KMS: + Create necessary IAM policies, roles, groups, and users + Set up a Key Management Service (KMS) key + Create an S3 bucket and upload data + Configure AWS CloudTrail for logging and Amazon Athena for querying data + Test and share encrypted data stored in S3 10/15/2025 10/15/2025 EC2 right-sizing: https://000032.awsstudygroup.com/ S3 encryption with KMS: https://000033.awsstudygroup.com/ 5 - Visualize and analyze costs with AWS Cost Explorer: + View cost and usage data by service and by account + Analyze the scope and effectiveness of Savings Plans and Reserved Instances + Evaluate cost elasticity + Create custom reports for EC2 instances + Use Cost Explorer for in-depth cost analysis + Review data transfer costs for common architectures - Build a data lake on AWS: + Create an IAM Role and Policy for necessary permissions + Set up an S3 bucket for data storage + Create a Kinesis Data Firehose delivery stream for data collection + Use a Glue Crawler to create a data catalog + Perform data transformation + Analyze data using Amazon Athena + Visualize data with Amazon QuickSight 10/16/2025 10/17/2025 AWS Cost Explorer: https://000034.awsstudygroup.com/ Data lake on AWS: https://000035.awsstudygroup.com/ 6 - Study AWS CloudWatch for monitoring and observability: + Explore CloudWatch Metrics, including viewing, searching, and using expressions + Work with CloudWatch Logs, Logs Insights, and Metric Filters + Configure CloudWatch Alarms to trigger notifications + Create CloudWatch Dashboards for visualizing data - Automate infrastructure with AWS CloudFormation: + Create IAM Users and Roles for preparation + Develop a basic CloudFormation template to provision resources + Explore advanced features like Custom Resources with Lambda + Use Mappings, Stacksets, and Drift Detection for complex deployments 10/17/2025 10/17/2025 AWS CloudWatch: https://000036.awsstudygroup.com/ AWS CloudFormation: https://000037.awsstudygroup.com/ Week 6 Achievements: Successfully managed EC2 access control through IAM services: Configured resource-based access policies using tags Implemented permission boundaries to limit user capabilities Set up monitoring and observability infrastructure: Deployed Grafana on EC2 for custom dashboards Configured CloudWatch Metrics, Logs, and Alarms for resource monitoring Implemented AWS Systems Manager capabilities: Used Patch Manager for automated server updates Executed remote commands across multiple instances with Run Command Applied EC2 optimization and cost management practices: Installed and configured CloudWatch Agent for detailed metrics Analyzed instance configurations using AWS Compute Optimizer Reviewed cost patterns and trends with Cost Explorer Secured data storage with encryption: Created and managed KMS keys for S3 encryption Set up CloudTrail and Athena for audit logging and analysis Built a complete data lake pipeline: Configured Kinesis Data Firehose for streaming data ingestion Created data catalogs with Glue Crawler Performed data analysis with Athena and visualization with QuickSight Automated infrastructure deployment with CloudFormation: Developed templates for resource provisioning Explored advanced features including Lambda-backed custom resources and StackSets "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/5-workshop/5.2-serverless-backend/5.2.6-api-gateway/",
	"title": "API Gateway Setup",
	"tags": [],
	"description": "",
	"content": "Overview In this section, you\u0026rsquo;ll create an Amazon API Gateway REST API that serves as the entry point for your serverless backend. API Gateway will receive HTTP requests from clients, route them to your Lambda function, and return responses.\nWhat you\u0026rsquo;ll accomplish:\nUnderstand API Gateway concepts and integration types Create a REST API in API Gateway Define resources and methods for user operations Configure Lambda proxy integration Enable CORS for frontend integration Deploy API to a stage Test API endpoints with various tools Monitor API usage and performance Understand API Gateway pricing Estimated time: 40-50 minutes\nAPI Gateway Architecture What We\u0026rsquo;ll Build Client (Browser/Mobile/Postman)\r‚Üì\rHTTPS Request\r‚Üì\rAPI Gateway REST API\r‚îú‚îÄ‚îÄ POST /users ‚Üí Lambda (Create User)\r‚îú‚îÄ‚îÄ GET /users ‚Üí Lambda (Get All Users)\r‚îú‚îÄ‚îÄ GET /users/{id} ‚Üí Lambda (Get Single User)\r‚îú‚îÄ‚îÄ PUT /users/{id} ‚Üí Lambda (Update User)\r‚îî‚îÄ‚îÄ DELETE /users/{id} ‚Üí Lambda (Delete User)\r‚Üì\rLambda Function (workshop-userHandler)\r‚Üì\rRDS PostgreSQL Costs Considerations Free-tier: 1M REST API CALLS RECEIVED | 1M HTTP API CALLS RECEIVED | 1M MESSAGES | 750,000 CONNECTION MINUTES per month\nPaid-tier REST API\nAPI Calls (per month) Price (per million) First 333 million $4.25 Next 667 million $3.53 Next 19 billion $3.00 Over 20 billion $1.91 Caching: 0.5GB -\u0026gt; $0.028/hour\nAdditional costs: CloudWatch logs (free-tier eligible)\nOverall: \u0026lt;$5 (clean up immediately after finish workshop)\nAPI Gateway Concepts REST API:\nResource-based API (e.g., /users, /users/{id}) Supports all HTTP methods (GET, POST, PUT, DELETE, etc.) Request/response transformation Built-in throttling and caching Resources:\nURL paths (e.g., /users) Can be nested (e.g., /users/{id}/tasks) Methods:\nHTTP operations on resources (GET, POST, PUT, DELETE) Each method can have different integration Integration Types:\nLambda Proxy: Passes entire request to Lambda (recommended) Lambda: Custom request/response mapping HTTP: Proxy to HTTP endpoint Mock: Returns static response AWS Service: Direct AWS service integration Stages:\nDeployment environments (e.g., dev, staging, prod) Each stage has unique URL Can have different settings per stage Step 1: Create REST API 1.1 Navigate to API Gateway Console Go to AWS Console Search for \u0026ldquo;API Gateway\u0026rdquo; Click API Gateway under Services 1.2 Create API Click Create an API You\u0026rsquo;ll see several API types:\nHTTP API: Simpler, cheaper, faster (70% cost reduction) REST API: Full features, request/response transformation WebSocket API: Real-time bidirectional communication REST API (Private): VPC-only access Under REST API, click Build 1.3 Configure API Settings Choose the protocol: Keep REST selected Create new API: Select New API API details:\nAPI name: workshop-user-api\nDescription: REST API for user management in serverless workshop\nEndpoint Type:\nSelect Regional Deployed in current region Lower latency for users in same region Can add CloudFront later for global distribution Endpoint Types:\nRegional: Deployed in a single region, recommended for most use cases\nEdge Optimized: Automatically distributed via CloudFront (adds latency for regional traffic)\nPrivate: Only accessible within VPC\nFor this workshop, Regional is best. You already have CloudFront from Part 1: Frontend Deployment if you want global distribution.\nSecurity policy Select SecurityPolicy_TLS13_1_2_2021_06 This option protects data in transit between a client and server with TLS 1.3 Click Create API You\u0026rsquo;ll be taken to the API Gateway console showing your new API.\nStep 2: Create Resources and Methods 2.1 Create /users Resource A resource represents a REST API endpoint path.\nIn the API Gateway console, select Resources in left navigation (if not already selected) Click Create Resource Proxy resource:\nEssentially a catch-all resource A proxy resource (often created as {proxy+}) is a special type of resource in API Gateway that forwards all requests to a single backend (such as a Lambda function) ‚Äî regardless of the URL path or HTTP method. We won\u0026rsquo;t be using this for our workshop since our API is simple Resource Path: /\nResource Name: users\nThis becomes the URL path CORS:\nCheck this box Automatically adds OPTIONS method with CORS headers Enable CORS on all child methods Required for browser-based frontends Click Create Resource You\u0026rsquo;ll see /users appear in the resource tree.\n2.2 Create /users/{id} Resource Create a child resource with path parameter for single user operations.\nSelect /users resource (click on it) Click Create method New Child Resource:\nResource Path: user\nSingular, represents a single user Resource Name: {id}\nCurly braces indicate a path parameter CORS:\nCheck this box Click Create Resource Your resource tree now shows:\n2.3 Create POST Method on /users Methods define HTTP operations on resources.\nClick on /users resource Click Create Method Select POST Setup - POST:\nIntegration type:\nSelect Lambda Function Use Lambda Proxy integration:\nCheck this box Passes entire request to Lambda as-is Lambda returns API Gateway-formatted response Lambda Region:\nSelect your region (e.g., ap-southeast-1) Lambda Function:\nType: workshop-lambda-sm-rds Should auto-complete Permission prompt: You\u0026rsquo;ll see a popup: \u0026ldquo;Add Permission to Lambda Function\u0026rdquo;\nThis grants API Gateway permission to invoke your Lambda function.\nClick Create method 2.4 Create GET Method on /users Get all users.\nClick on /users resource Click Create Method Method: GET Setup - GET:\nIntegration type: Lambda Function Use Lambda Proxy integration: Checked Lambda Function: workshop-lambda-sm-rds Click Create method 2.5 Create GET Method on /users/{id} Get single user.\nClick on /users/{id} resource Click Create Method Method: GET Setup - GET:\nIntegration type: Lambda Function Use Lambda Proxy integration: Checked Lambda Function: workshop-lambda-sm-rds Click Create method 2.6 Create PUT Method on /users/{id} Update user.\nClick on /users/{id} resource Click Create Method Method: PUT Setup - PUT:\nIntegration type: Lambda Function Use Lambda Proxy integration: Checked Lambda Function: workshop-lambda-sm-rds Click Create method 2.7 Create DELETE Method on /users/{id} Delete user.\nClick on /users/{id} resource Click Create Method Method: DELETE Setup - DELETE:\nIntegration type: Lambda Function Use Lambda Proxy integration: Checked Lambda Function: workshop-lambda-sm-rds Click Create method 2.8 Verify Resource Structure Your API structure should now look like:\n2.9 Enable CORS on each resources Click /users resource Click Enable CORS Select the methods: GET, POST Access-Control-Allow-Origin: input your CloudFront endpoint to restrict origin, or * for debug Click Save Do the same for /{id} resource (methods: GET, PUT, DELETE) After enabling CORS for those two resources, you will see OPTION method in each resource Click on OPTION method, go to Integration response to see the details Step 3: Deploy API APIs must be deployed to a stage before they\u0026rsquo;re accessible.\n3.1 Create Deployment Click Deploy API Deployment stage:\nSelect [New Stage] Stage name: dev\nShort for development Other common names: prod, staging, test Stage description: Development stage for workshop\nDeployment description: Initial deployment\nClick Deploy 3.2 Get API Endpoint After deployment, you\u0026rsquo;ll see the Stage Editor.\nInvoke URL is your API\u0026rsquo;s base URL:\nhttps://abc123xyz.execute-api.[region].amazonaws.com/dev Step 5: Test API Endpoints 5.1 Test with API Gateway Console API Gateway provides a built-in testing tool.\nTest POST /users (Create User):\nIn the left navigation, click Resources Click on /users ‚Üí POST method Go to *Test tab Request Body:\n{ \u0026#34;cognitoSub\u0026#34;: \u0026#34;test-sub-123\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;Test User\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;test@example.com\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;phoneNumber\u0026#34;: \u0026#34;1234567890\u0026#34; } Click Test Expected Response:\nStatus: 201\nResponse Body:\n{ \u0026#34;success\u0026#34;: true, \u0026#34;data\u0026#34;: { \u0026#34;id\u0026#34;: 3, \u0026#34;cognito_sub\u0026#34;: \u0026#34;test-sub-123\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;Test User\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;test@example.com\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;phone_number\u0026#34;: \u0026#34;1234567890\u0026#34; } } Response Headers:\nContent-Type: application/json Logs: Shows Lambda execution logs inline\nTest GET /users (Get All Users):\nClick on /users ‚Üí GET method Click Test No request body needed Click Test Expected Response:\nStatus: 200\nResponse Body:\n{ \u0026#34;success\u0026#34;: true, \u0026#34;data\u0026#34;: [ { \u0026#34;id\u0026#34;: 1, \u0026#34;cognito_sub\u0026#34;: \u0026#34;demo-sub-1\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;Alice\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;alice@example.com\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;phone_number\u0026#34;: \u0026#34;1234567890\u0026#34; }, { \u0026#34;id\u0026#34;: 2, \u0026#34;cognito_sub\u0026#34;: \u0026#34;demo-sub-2\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;Bob\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;bob@example.com\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;phone_number\u0026#34;: \u0026#34;0987654321\u0026#34; }, { \u0026#34;id\u0026#34;: 3, \u0026#34;cognito_sub\u0026#34;: \u0026#34;test-sub-123\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;Test User\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;test@example.com\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;phone_number\u0026#34;: \u0026#34;1234567890\u0026#34; } ] } Test GET /users/{id} (Get Single User):\nClick on /users/{id} ‚Üí GET method Click Test Path Parameters:\nid: 1 Click Test Expected Response:\nStatus: 200\nResponse Body:\n{ \u0026#34;success\u0026#34;: true, \u0026#34;data\u0026#34;: { \u0026#34;id\u0026#34;: 1, \u0026#34;cognito_sub\u0026#34;: \u0026#34;demo-sub-1\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;Alice\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;alice@example.com\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;phone_number\u0026#34;: \u0026#34;1234567890\u0026#34; } } 5.2 Test with curl (Command Line) Test your deployed API from terminal:\nSet your API URL:\nAPI_URL=\u0026#34;https://YOUR-API-ID.execute-api.[region].amazonaws.com/dev\u0026#34; Create User:\ncurl -X POST \u0026#34;${API_URL}/users\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;cognitoSub\u0026#34;: \u0026#34;curl-test-456\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;Curl User\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;curl@example.com\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;phoneNumber\u0026#34;: \u0026#34;5551234567\u0026#34; }\u0026#39; Get All Users:\ncurl -X GET \u0026#34;${API_URL}/users\u0026#34; Get Single User:\ncurl -X GET \u0026#34;${API_URL}/users/1\u0026#34; Update User:\ncurl -X PUT \u0026#34;${API_URL}/users/1\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;username\u0026#34;: \u0026#34;Updated Name\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;updated@example.com\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;phoneNumber\u0026#34;: \u0026#34;9998887777\u0026#34; }\u0026#39; Delete User:\ncurl -X DELETE \u0026#34;${API_URL}/users/3\u0026#34; 5.3 Test with Postman Postman provides a user-friendly interface for API testing.\nImport API to Postman:\nIn API Gateway console, go to Stages ‚Üí dev Click Stage action ‚Üí Export In the Export API modal, leave all settings as default. Click Export API Download the json file Open Postman, click Import On the opened modal, click files On the next modal, click Import Expand workshop-user-api, select GET /users to test get all users route Click Send View response Step 6: Configure Stage Settings 6.1 Enable CloudWatch Logging In API Gateway console, go to Stages Click dev stage Under Logs and racing card, click Edit CloudWatch Settings:\nCloudWatch Logs:\nSelect Errors and info logs This includes Request received Request body (if enabled) Integration request Integration response Execution summary Errors Data tracing:\nCheck this box (for development) Logs request/response bodies Disable in production (may contain sensitive data) Enable Detailed Metrics:\nCheck this box Provides method-level metrics Small additional cost ($0.50/month per metric) Click Save Changes Grant API Gateway Permission:\nIf this is your first API with logging, you\u0026rsquo;ll need to set up an IAM role:\nIn IAM dashboard, create role with: Trusted entity type: AWS Service Use case: API Gateway Policy: AmazonAPIGatewayPushToCloudWatchLogs Role name: api-gw-push-cloudwatch-logs Copy the Role ARN Go back to API Gateway dashboard On the left sidebar, click Settings In Logging, click Edit Paste the Role ARN to the textbox Click Save changes 6.2 Configure Throttling Protect your API from abuse with rate limiting.\nStill in dev stage settings in Stage details card, click Edit Throttling settings:\nEnabled\nRate: 1000 requests per second\nBurst capacity for temporary spikes Burst: 2000 requests\nTotal requests allowed in burst Throttling Limits:\nWhen limits are exceeded:\nClient receives 429 Too Many Requests Requests are rejected before reaching Lambda Protects backend from overload No Lambda costs for throttled requests Default AWS Account Limits:\n10,000 requests per second per region Can request increase via support ticket For this workshop, 1000 req/s is more than sufficient.\n6.3 Enable Caching (Optional) API caching reduces Lambda invocations and improves performance.\nIn dev stage, click Settings tab Scroll to Cache Settings Provision API cache:\nCheck this box Cache capacity: 0.5 GB\nSmallest size Sufficient for workshop Cache time-to-live (TTL): 300 seconds (5 minutes)\nHow long responses are cached Per-key cache invalidation:\nInvalidate certain data in cache based on a key instead of the whole cache Caching Costs:\nAPI Gateway caching is relatively expensive:\n0.5 GB cache: $0.028/hour (~$20.16/month) For this workshop: Skip caching to avoid costs. It\u0026rsquo;s included here for completeness.\nWhen to use caching:\nHigh read traffic (\u0026gt;1000 req/min) Data doesn\u0026rsquo;t change frequently Latency is critical Cost of Lambda invocations \u0026gt; cost of cache For user data that changes frequently, caching may not be appropriate.\nStep 7: Monitor API Performance 7.1 View API Gateway Metrics In API Gateway console, go to Dashboard (left navigation) Select your API: workshop-user-api Select stage: dev Metrics displayed:\nAPI calls:\nTotal number of requests over time period Broken down by hour/day Integration latency:\nTime spent in Lambda function Excludes API Gateway overhead Latency:\nTotal request time (API Gateway + Lambda) Includes network, integration, and processing time 4XX errors:\nClient errors (bad requests, not found, etc.) Should be low in well-designed APIs 5XX errors:\nServer errors (Lambda errors, timeouts, etc.) Should be monitored closely 7.2 View CloudWatch Logs Go to CloudWatch console Click Log groups Find: API-Gateway-Execution-Logs_{api-id}/dev Click on log group Click on latest log stream Sample log entry:\n(abc-123-def) Method request body before transformations: {\r\u0026#34;cognitoSub\u0026#34;: \u0026#34;test-123\u0026#34;,\r\u0026#34;username\u0026#34;: \u0026#34;Test User\u0026#34;,\r\u0026#34;email\u0026#34;: \u0026#34;test@example.com\u0026#34;,\r\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;,\r\u0026#34;phoneNumber\u0026#34;: \u0026#34;1234567890\u0026#34;\r}\r(abc-123-def) Endpoint request URI: https://lambda.us-east-1.amazonaws.com/2015-03-31/functions/arn:aws:lambda:us-east-1:123456789012:function:workshop-userHandler/invocations\r(abc-123-def) Endpoint response body before transformations: {\r\u0026#34;statusCode\u0026#34;: 201,\r\u0026#34;headers\u0026#34;: {\r\u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;\r},\r\u0026#34;body\u0026#34;: \u0026#34;{\\\u0026#34;success\\\u0026#34;:true,\\\u0026#34;data\\\u0026#34;:{\\\u0026#34;id\\\u0026#34;:1,...}}\u0026#34;\r}\r(abc-123-def) Method response body after transformations: {\r\u0026#34;success\u0026#34;: true,\r\u0026#34;data\u0026#34;: {\r\u0026#34;id\u0026#34;: 1,\r\u0026#34;cognito_sub\u0026#34;: \u0026#34;test-123\u0026#34;,\r\u0026#34;username\u0026#34;: \u0026#34;Test User\u0026#34;,\r\u0026#34;email\u0026#34;: \u0026#34;test@example.com\u0026#34;,\r\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;,\r\u0026#34;phone_number\u0026#34;: \u0026#34;1234567890\u0026#34;\r}\r}\r(abc-123-def) Method completed with status: 201 Summary Congratulations! You\u0026rsquo;ve successfully:\nCreated REST API in API Gateway Defined resources and methods for CRUD operations Configured Lambda proxy integration Enabled CORS for frontend compatibility Deployed API to dev stage Tested endpoints with multiple tools Configured logging and monitoring Set up throttling for protection What You\u0026rsquo;ve Built Your API Gateway now provides:\nRESTful endpoints for all user operations HTTPS security by default CORS support for frontend integration Request throttling for protection CloudWatch logging for debugging Metrics for monitoring Next Steps Proceed to Part 6: Amazon Cognito Configuration to secure API endpoints with authorization provided by AWS Cognito\nReady to continue? Your API endpoints are now fully functional and ready! üöÄ\n"
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nDuring my internship at [Company/Organization Name] from [start date] to [end date], I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in [briefly describe the main project or task], through which I improved my skills in [list skills: programming, analysis, reporting, communication, etc.].\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ‚úÖ ‚òê ‚òê 2 Ability to learn Ability to absorb new knowledge and learn quickly ‚òê ‚úÖ ‚òê 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ‚úÖ ‚òê ‚òê 4 Sense of responsibility Completing tasks on time and ensuring quality ‚úÖ ‚òê ‚òê 5 Discipline Adhering to schedules, rules, and work processes ‚òê ‚òê ‚úÖ 6 Progressive mindset Willingness to receive feedback and improve oneself ‚òê ‚úÖ ‚òê 7 Communication Presenting ideas and reporting work clearly ‚òê ‚úÖ ‚òê 8 Teamwork Working effectively with colleagues and participating in teams ‚úÖ ‚òê ‚òê 9 Professional conduct Respecting colleagues, partners, and the work environment ‚úÖ ‚òê ‚òê 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ‚òê ‚úÖ ‚òê 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ‚úÖ ‚òê ‚òê 12 Overall General evaluation of the entire internship period ‚úÖ ‚òê ‚òê Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives: Advance hands-on proficiency with Amazon DynamoDB, covering basic operations, advanced data modeling, and multi-region/global architectures. Set up identity federation and IAM role configurations, and apply cost-optimization techniques across AWS and Azure AD. Deploy and operate applications using Lightsail, containers, Step Functions, and IAM roles to ensure secure access. Use Cloud9, Elastic Beanstalk, and CI/CD tools to automate application delivery pipelines. Improve AWS security posture through core IAM best practices, detective controls, incident response, and infrastructure protection. Design and automate microservices architectures using Lambda, DynamoDB, Step Functions, and CodeStar. Tasks carried out this week: Day Task Start Date Completion Date Reference Material 2 - DynamoDB fundamentals: + Create tables and populate sample records + Use AWS CLI for read/query/scan/insert/update operations + Inspect table data and GSIs in the console + Perform backups and restores (PITR and on-demand) - Advanced DynamoDB patterns: + Review capacity units and partitioning behavior + Compare sequential vs. parallel scans for throughput + Apply GSI write-sharding, key overloading, and sparse GSIs + Use composite keys and adjacency lists for complex access patterns - Change Data Capture: + Enable DynamoDB Streams and process changes + Create a Lambda to consume stream events + Explore CDC with Kinesis Data Streams as an alternative - Global serverless app with DynamoDB: + Provision serverless backend components + Configure Global Tables for multi-region replication + Interact with a sample global application front-end - Game player data modeling: + Design data model from entities and access patterns + Define primary keys and table layout + Use sparse GSIs to find available games + Add inverted indexes for retrieving a user‚Äôs past games - Cost \u0026amp; performance analysis with Glue and Athena: + Build a database using Glue crawlers + Query cost and usage reports with Athena + Apply tagging strategies for cost allocation 10/20/2025 10/20/2025 CDK basic: https://000038.awsstudygroup.com/ Amazon DynamoDB Immersion: https://000039.awsstudygroup.com/ Analysis with Glue and Athena: https://000040.awsstudygroup.com/ 3 - IAM federation from Azure AD: + Prepare Azure AD (tenant, users) + Create an Enterprise Application in Azure to connect to AWS + Configure an Identity Provider and matching IAM roles in AWS + Synchronize roles and grant users federated AWS console access + Validate sign-in from Azure AD to AWS console - Cost optimization (Savings Plans \u0026amp; RIs): + Compare Savings Plans and Reserved Instances + Use AWS recommendations to find savings opportunities + Purchase a Savings Plan to lower EC2 costs + Review RI types and RDS Reserved DB Instances options - Schema conversion \u0026amp; migration: + Prepare an EC2 host and install the Schema Conversion Tool + Configure source DB (Oracle/SQL Server) and convert schema for the target + Create DMS replication instance, endpoints, and tasks + Run migrations and replicate ongoing changes + Test DMS Serverless for auto-scaling during migration + Monitor migration progress with CloudWatch and task logs - IAM roles \u0026amp; condition policies: + Create IAM groups and users + Set up an admin role and enable role switching + Limit role use by IP address and time-based conditions - Deploy \u0026amp; manage apps on Lightsail: + Launch a database and WordPress on Lightsail + Configure networking and application settings + Deploy Prestashop and Akaunting instances + Secure apps, create snapshots, scale instance size, and add alarms 10/21/2025 10/21/2025 IAM Federation with Azure AD: https://000041.awsstudygroup.com/ AWS Cost Optimization: https://000042.awsstudygroup.com/ Database Migration with DMS: https://000043.awsstudygroup.com/ IAM Roles and Conditions: https://000044.awsstudygroup.com/ Amazon Lightsail Applications: https://000045.awsstudygroup.com/ 4 - Lightsail containers: + Create a Lightsail container service and deploy a public image + Provision a Lightsail instance, install Docker and the AWS CLI + Build, push, and deploy a custom container image from the instance - AWS Step Functions: + Launch a Cloud9 environment and deploy sample services + Create Step Functions workflows to orchestrate Lambdas with Task states + Add branching with Choice states, manage state I/O, and include Wait tokens + Implement error handling with retry and catch, and run parallel states - Authorize applications using IAM roles: + Create an EC2 instance and S3 bucket for app testing + Demonstrate drawbacks of long-lived access keys + Create an EC2 IAM role with S3 permissions and attach it to the instance 10/22/2025 10/22/2025 Lightsail Containers: https://000046.awsstudygroup.com/ AWS Step Functions: https://000047.awsstudygroup.com/ IAM Roles for Applications: https://000048.awsstudygroup.com/ 5 - Cloud9 basics: + Create a Cloud9 environment + Use the command line, edit files, and run AWS CLI tasks inside Cloud9 - Deploy monolithic app on Elastic Beanstalk: + Set up key pair, CloudFormation stack, and database + Configure and access the instance, test locally via Eclipse, and deploy to Beanstalk + Update the app and verify API endpoints - Automated release pipeline: + Create a CodeStar project and connect Eclipse to CodeCommit + Replace sample app, trigger pipeline, and deploy a Windows Service with CodeDeploy to EC2 + Monitor deployments from the IDE and pipeline tools - Foundational AWS security practices: + Secure the root account and enforce MFA + Create a dedicated IAM admin user/group for daily tasks + Enforce strong password policies and consider SCP guardrails in Organizations - IAM analysis \u0026amp; validation: + Use IAM Access Analyzer to validate least-privilege permissions + Create and test cross-account roles for temporary access + Inspect resource policies for unintended public/cross-account exposure + Review service last-accessed reports to remove unused permissions 10/23/2025 10/23/2025 AWS Cloud9: https://000049.awsstudygroup.com/ Elastic Beanstalk: https://000050.awsstudygroup.com/ CI/CD Pipeline: https://000051.awsstudygroup.com/ AWS Well-Architected Security Workshop: https://catalog.workshops.aws/well-architected-security 6 - Create a microservice: + Configure Eclipse IDE and develop a Lambda function + Test locally and deploy to AWS Lambda + Implement an ImageManager Lambda and automate with CodeStar CI/CD - Refactor data \u0026amp; workflows: + Provision a CloudFormation stack and a new DynamoDB table with a GSI + Build a Scan \u0026amp; Query microservice, create its API, update IAM policies, and redeploy via CodeStar + Implement a Calculator microservice using Step Functions + Lambda - Detective controls \u0026amp; incident response: + Deploy GuardDuty and review findings + Aggregate and prioritize findings in Security Hub and configure automated remediation + Use Detective for root cause analysis of security incidents - Infrastructure protection: + Design a VPC with segmented subnets and security groups + Deploy Network Firewall to inspect inter-subnet and egress/ingress traffic + Configure WAF rules to shield web apps from common attacks + Review AWS Shield Advanced for DDoS mitigation strategies - Data protection \u0026amp; encryption: + Use Amazon Macie to discover/classify sensitive S3 data + Apply customer-managed KMS keys for S3/EBS encryption + Provision SSL/TLS via ACM for load balancers + Manage and rotate secrets using Secrets Manager 10/24/2025 10/24/2025 Create Microservice: https://000052.awsstudygroup.com/ Refactor Data and Workflows: https://000053.awsstudygroup.com/ AWS Well-Architected Security Workshop: https://catalog.workshops.aws/well-architected-security Week 7 Achievements: Solid practical experience with Amazon DynamoDB: created tables, loaded data, used GSIs and composite keys, enabled backups, and implemented change-data-capture via Streams and Kinesis.\nPerformed cost and performance analysis using DynamoDB metrics alongside AWS Glue and Amazon Athena; experimented with tagging to improve cost allocation.\nImplemented identity federation and IAM governance:\nConfigured Azure AD federation to AWS, synchronized IAM roles, and verified federated console access. Created IAM groups, users, and roles with conditional policies (IP- and time-based restrictions). Improved cost optimization and database migration skills by evaluating Savings Plans and Reserved Instances, and practicing schema conversion and data migration using AWS SCT and DMS (including DMS Serverless and monitoring).\nDeployed and managed applications on Lightsail and container services: ran WordPress/Prestashop/Akaunting, managed snapshots and alarms, and built custom Docker images for deployment.\nOrchestrated workflows and secured application access:\nDeveloped Step Functions workflows (Task, Choice, Wait, Retry/Catch, Parallel). Favored IAM roles over long-lived access keys and attached EC2 roles for secure S3 access. Automated application lifecycle and CI/CD:\nUsed Cloud9, CodeStar, CodeCommit, CodeDeploy, and Elastic Beanstalk to streamline development and automated deployments. Strengthened organizational security posture:\nHardened root account practices (MFA), defined admin user/group usage, enforced password policies, explored SCPs, and used IAM Access Analyzer and last-accessed reports to tighten privileges. Built microservices and layered security controls:\nImplemented Lambda-based microservices with DynamoDB and Step Functions, automated deployment via CodeStar, and deployed monitoring/security tools (GuardDuty, Security Hub, Detective) plus network/data protections (VPC design, Network Firewall, WAF, Shield Advanced, Macie, KMS, ACM, Secrets Manager). "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/5-workshop/5.2-serverless-backend/5.2.7-cognito/",
	"title": "Amazon Cognito Configuration",
	"tags": [],
	"description": "",
	"content": "Overview In this section, you\u0026rsquo;ll configure Amazon Cognito to handle user authentication and authorization for your serverless application. Cognito will manage user sign-up, sign-in, password recovery, and generate JWT tokens that secure your API endpoints.\nWhat you\u0026rsquo;ll accomplish:\nUnderstand Amazon Cognito concepts and architecture Create a Cognito User Pool for user management Configure user attributes and password policies Set up app client for your application Test user registration and authentication flows Integrate Cognito with API Gateway as an authorizer Secure API endpoints with JWT token validation Test authenticated API calls Estimated time: 50-60 minutes\nCognito Architecture What We\u0026rsquo;ll Build User (Browser/Mobile)\r‚Üì\rSign Up / Sign In\r‚Üì\rAmazon Cognito User Pool\r‚îú‚îÄ‚îÄ User Management (sign-up, sign-in, verification)\r‚îú‚îÄ‚îÄ Token Generation (ID Token, Access Token, Refresh Token)\r‚îî‚îÄ‚îÄ User Attributes (email, phone, custom attributes)\r‚Üì\rJWT Token (ID Token)\r‚Üì\rAPI Gateway (with Cognito Authorizer)\r‚îú‚îÄ‚îÄ Validates JWT Token\r‚îú‚îÄ‚îÄ Extracts user claims (sub, email, etc.)\r‚îî‚îÄ‚îÄ Passes to Lambda if valid\r‚Üì\rLambda Function (workshop-lambda-sm-rds)\r‚îú‚îÄ‚îÄ Receives user info from authorizer\r‚îî‚îÄ‚îÄ Performs authorized operations\r‚Üì\rRDS PostgreSQL Costs Considerations Free for up to 10,000 monthly active users (MAUs) Overall: $0 (assume immediate clean up after workshop) Cognito Concepts User Pool:\nDirectory of users for your application Handles authentication (sign-up, sign-in) Manages user attributes and profiles Issues JWT tokens after successful authentication App Client:\nConfiguration for how your app connects to user pool Defines authentication flows Controls token expiration times JWT Tokens:\nID Token: Contains user identity and attributes (used for API authorization) Access Token: Used to access Cognito user pool APIs Refresh Token: Used to get new ID/Access tokens without re-authentication Hosted UI:\nPre-built sign-up/sign-in pages Customizable branding Handles OAuth 2.0 flows Alternative to building custom auth UI Cognito Authorizer:\nAPI Gateway feature Validates JWT tokens automatically Denies requests with invalid/expired tokens Passes user claims to Lambda Step 1: Create Cognito User Pool 1.1 Navigate to Cognito Console Go to AWS Console Search for \u0026ldquo;Cognito\u0026rdquo; Click Amazon Cognito under Services 1.2 Create User directory Click Get started for free in less than five minutes You\u0026rsquo;ll go through a step-by-step configuration wizard.\nCognito user directory setup 1.3 Define your application Application type Since we use Cognito to authenticate users in the frontend we deployed in Part 1, choose Single-page application (SPA) Name you application Enter name: workshop-cognito-SPA Click Next 1.4 Configure options Options for sign-in identifiers The required attribute for user to log-in. Select Email Self-registration Enabled as default This option allows users to initiate sign-up by themselves. If chose Disable, only admin that has access to Cognito can create new account Required attributes for sign-up Select name and phone_number These are the additional required attributes when sign-up 1.5 Add a return URL (optional) When specified a return URL, our application will redirect user back to said URL after successful log-in. Left blank for now. We will configure this later when we integrate our front-end app Click Create user directory 1.6 View resources After you click Create user directory in the previous step, you will be redirected to another page\nOn this page, you can view:\nBuilt-in log-in and sign-up page by clicking View login page Quick setup guide This provides developers guidance on how to integrate authentication in Front-end apps We will leave this page for now and comeback later when we integrate our front-end\nClick Go to overview at the bottom of the page\n1.7 View User Pool Details After creation, you\u0026rsquo;ll see your user pool overview.\nSave these important details:\nUser pool ID:\nFormat: [region]_abcd1234 User pool ARN:\nFormat: arn:aws:cognito-idp:us-east-1:123456789012:userpool/[region]_abcd1234 Get App client ID:\nOn the left sidebar, click App clients under Applications Copy the Client ID Format: 1234567890abcdefghijklmnop In the middle, you can see Authentication flow session duration: the maximum duration when user initiate authentication Refresh token expiration: duration before refresh token expires Access token expiration: duration before access token is expired and user is required (or not, based on settings) to log-in again To edit these durations and more, click Edit in the same card in the right Edit app client information Authentication flow: authentication flows that your app will support Choice-based sign-in: ALLOW_USER_AUTH: allows multiple sign-in options for users (biometric, OPT\u0026hellip;) Sign in with secure remote password (SRP): ALLOW_USER_SRP_AUTH: sign-in with username and password Get new user tokens from existing authenticated sessions: ALLOW_REFRESH_TOKEN_AUTH: when access token expires, refresh token are used to extend and refetch new access token without the user have to initiate login again Duration and expiration times: you can also modify duration and expiration times here You can also view some notable configurations on the app client page Quick setup guilde: provides developers guidance on how to integrate Cognito to Front-end apps Attribute permissions: view and edit attribute permissions that this app client can read and write You can see that email is not writeable since we chose email as the required sign-up and sign-in attribute Login pages: customize login and logout behaviors Threat protection: Configure threat protection for adaptive authentication and compromised credentials. Analytics Step 2: Test User Registration 2.1 Access Hosted UI In Cognito console, go to your user pool On the left sidebar, click App clients under Applications Click View login page This opens the Cognito-hosted authentication page in a new tab.\n2.2 Sign Up a New User On the Hosted UI page, click Create an account\nFill in the registration form:\nEmail: Your actual email address\nYou\u0026rsquo;ll receive a verification code here Name: Your full name (alphanumeric, case-sensitive)\nPhone number: +1234567890\nFormat: +[country code][number] Example: +84 0123456789 for Vietnam Password:\nMust meet password policy: At least 8 characters Uppercase letter Lowercase letter Number Special character Confirm password\nClick Sign up 3.3 Verify Email Address Check your email inbox\nYou should receive an email from no-reply@verificationemail.com\nSubject: \u0026ldquo;Your verification code\u0026rdquo;\nBody contains: 6-digit verification code\nExample: 123456 Go back to the login page\nEnter the 6-digit code in the Confirmation code field\nClick Confirm Account\nDidn\u0026rsquo;t receive the email?\nCheck spam/junk folder Wait 2-3 minutes (can be delayed) Click \u0026ldquo;Resend code\u0026rdquo; on the verification page Verify email address is correct Check Cognito email sending quota (50/day limit) If still not working:\nGo to Cognito console ‚Üí Users Find your user (status: UNCONFIRMED) Click Actions ‚Üí Confirm account (admin override) After successful verification, you\u0026rsquo;ll be redirected to the return URL page.\nWe will setup return URL later to redirect user to our front-end homepage\n3.4 Login Close the browser page Go back to our app client page, then click View login page again You will see the folowing This is because you have previously signed-in after sign-up After a successful sign-in, Cognito will store httpOnly cookies containing access token and refresh token If those are not expired yet, you won\u0026rsquo;t have to initiate login again (input email and password) Click Sign in as [your email], you will be redirected to the return URL If you want to input email and password, simply click Sign in as a different user? Step 4: View Users in Cognito Console 4.1 View User List In Cognito console, select user pool Click Users in left navigation You should see your newly created user 4.2 View User Details Click on the username: testuser View detailed information: User attributes:\nsub: UUID (unique identifier) email: Your email email_verified: true name: Your name phone_number: +1234567890 phone_number_verified: false (we didn\u0026rsquo;t verify phone) Group memberships:\nNone (we haven\u0026rsquo;t created groups yet) Step 5: View Authentication Tokens We will view how an access token looks like via CloudShell\n5.1 Get access token via CloudShell In the bottom right corner, look for CloudShell When click on CloudShell, a drawer contain a CLI appear from bottom Create a json file on your local machine with the following content: { \u0026#34;UserPoolId\u0026#34;: \u0026#34;{your-user-pool-id}\u0026#34;, \u0026#34;ClientId\u0026#34;: \u0026#34;{your-client-id}\u0026#34;, \u0026#34;AuthFlow\u0026#34;: \u0026#34;ADMIN_NO_SRP_AUTH\u0026#34;, \u0026#34;AuthParameters\u0026#34;: { \u0026#34;USERNAME\u0026#34;: \u0026#34;admin@example.com\u0026#34;, \u0026#34;PASSWORD\u0026#34;: \u0026#34;password123\u0026#34; } } For example\nUserPoolId can be found in your user pool page ClientId can be found in your app client page username and password of your new account you just created Save the json file as auth.json Back to your CloudShell CLI, click Action on the right, click Upload file Select your auth.json file to upload Then in the CLI, run ls. You will see your auth.json file Run the following command aws cognito-idp admin-initiate-auth --region {your-aws-region} --cli-input-json file://auth.json\nIf you encounter this error An error occurred (InvalidParameterException) when calling the AdminInitiateAuth operation: Auth flow not enabled for this client\nGo to your app client page In App client information card, click Edit Under Authentication flows, check Sign in with server-side administrative credentials: ALLOW_ADMIN_USER_PASSWORD_AUTH Save changes Run the command again, you will have the following response\n{ \u0026#34;ChallengeParameters\u0026#34;: {}, \u0026#34;AuthenticationResult\u0026#34;: { \u0026#34;AccessToken\u0026#34;: \u0026#34;eyJraWQiOiJTNVwvTmlNSnorR0VQUzlVNis2dlBjRCttQ2tLZDNOdFFCcEp1NEhRbXhoUT0iLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiI4OTNhNzVmYy02MGYxLTcwOTYtNWJiYi1mNDNlMDA5ZjJiM2IiLCJpc3MiOiJodHRwczpcL1wvY29nbml0by1pZHAuYXAtc291dGhlYXN0LTEuYW1hem9uYXdzLmNvbVwvYXAtc291dGhlYXN0LTFfVHV1dGRSVExkIiwiY2xpZW50X2lkIjoiNTZtc2RjdHMwcjJ1YWhrdDZjMzBsdWxiZWgiLCJvcmlnaW5fanRpIjoiNWNmOWFiMWUtNDdjZC00MzZlLTk2OWYtZmU5ZDE3YTgxOGNhIiwiZXZlbnRfaWQiOiJkMTM5Y2Y3Zi02YmE3LTQzYzQtYjNiZS0xYWJmMmEyNjNkMzkiLCJ0b2tlbl91c2UiOiJhY2Nlc3MiLCJzY29wZSI6ImF3cy5jb2duaXRvLnNpZ25pbi51c2VyLmFkbWluIiwiYXV0aF90aW1lIjoxNzY0NDg3MDI1LCJleHAiOjE3NjQ0OTA2MjUsImlhdCI6MTc2NDQ4NzAyNSwianRpIjoiNGZhN2RkZjctZmJiYS00MjM3LWE2MzctYTkyMWE1NDZlODlmIiwidXNlcm5hbWUiOiI4OTNhNzVmYy02MGYxLTcwOTYtNWJiYi1mNDNlMDA5ZjJiM2IifQ.bUHsq_dNyxJ2GBARcP5IRY8Qi-AZbpTItD_BkH4uy7b-BYLZxNx03peI68DFmhd2mDpI_exw6DigB2I6hFGeQ2VbUlH0kKxvA0gDXqQNj4bnDnU2KCIzPbj_pIydab_tjxuU6FQi4l5tTxM6ygKciIUowqX3GLgSvyXPWprZzZtQoYgyMETT-rae4EWBhUoT1Em76YAvlXwqxHp0RJi8vwABWaB6Q34buCO1wM4SWzs5ZNfLSy28FK0xLEdGuAXwtXRnu2Myn_wmcePK_K-kiFi7aMX4wEB4Vl7zoPqUhgzI2unSLBCDNHX-svTSRy6sk9IaXZkzp_TbH4bfflFLPA\u0026#34;, \u0026#34;ExpiresIn\u0026#34;: 3600, \u0026#34;TokenType\u0026#34;: \u0026#34;Bearer\u0026#34;, \u0026#34;RefreshToken\u0026#34;: \u0026#34;eyJjdHkiOiJKV1QiLCJlbmMiOiJBMjU2R0NNIiwiYWxnIjoiUlNBLU9BRVAifQ.pHOj9o4WG_bH5rw2E2tF5Oxpx4WbVEOMZb2oVduNH6Wxsl8NE_UfAhS6IVi6rcaGZkdgC1e1DX8Z-zqSwrequpT7eIKsUeb5WDmcu4my1RVfx2vrmmKKCuyHs3fCG8eyopjAe2-9vKnVJI6T-nr_Sg9YJ54EzWie8sHO2_pdLMOkcNgxn4Hhv8f8J0PsrIyovaI62O6uomTul1lMqUQ3Q4AHTRwROZEVqCQlzEYmMnsFBUbVTzHuA0hbEy4v8hRx8jqLjNwcvOf9qqYgyusZt0wiErnMGR11p00AsOgIcJ6A_484DoH4TPu2eqAy7UIZF5g-ak66iTncSedMQermXw.uA8Bb7gC4JfnFcCx.oFSb-D6NM1q2KbP8MGDqcnDOq8OD921MSb-oyirVPIu1taU-iPizZ08IvdGSc_kV-bRSep2-vmD-jKQePXNI4aZ0S6YxTV09XwLJlRkU8DMeiH5Z1LY8_ZMf2KBNCHpfAONb3l3_xQj3hHGgIpc2W2lHX-e9YaKadDKSlU4BTs5P2BOrTq4fkVGRfjp5sC_FlcsSOULCzgM8afHQTCnONzNmMrGHn6k9famKFFDUVX7W9tTI5YwndJ9eW_vU0ZYIETIP_dF4X0DateJb108NtgLfRs5U8eerBVCUh9RnExR9tuPSzqqP0nDCD9JN1QQE_Z-zmQFngVsEvL040d19serbQ7Fjyikzmm2udX5SHgYtQlGWHaFjp3hOQKotJhlFpYR-CmC9StB2BzLl2fA2Wol5FIi0VhrnWOOmnxlgN1m7k1kLAWC7S_hieJgbCw4kSpeG7QLk_9BYllT3dNKjgzyBS7ppYYAaRk1wXtm7f52Yf8nwD0qO-9aksa6rQFRJ5xzYUfdNyZwLbJvHDcTfyxedgcUGEGtRf-wAU7bxmzmKiFtnFr4nrTNsgkfWoDoepc23PzuQqsHZ_J7AYZHzBJmrlxz5NhYG9s6c7WkeEXTogXly7-hJdDen8NFXgEZOW9Aed1W6aQNjdzd726WIVNT_r5UcgvlAQs5y43tiLnsMjKmGpwW7PuWC8VGG4w1UhWCUH_5jZjgAuI4PDeZF2a7w3-O3jDsZhs4u0yBeDxFXtx_NESDA8Uvh_j5IAip8Gm_aO_pACelyGwhKb9aWVaQtixI7GBRLIwY8RptjnRVaLKxkGMsqfvYSOZHugQIeMCADoVMkl1COHv7bHy4289JGGpIPYVOfer-TIS6NzHf3xCqxNWSn0cBuYznJTrzjpwIv6L5t302WleAH5EvF-gyEfNU7oeiZp4q4-e11kppLoP6yFg4LOA8BxPiJBsDMA0qIn3MyfFG7UQmChLxOZyMkQCEWgTQey-WMtwKqAb3En_aoCfW_-IN2b9v0pdwq86B7NTVtrxGPLbhdFmVpF42kPxYTOasFrWymN6QeDgy4DBMcqKpBf3Tz3wGeSYZfsgD8AjyhaE6N56uARAIM23i-eVaNvd-b8-SSG5IOSC0z1kg9nARQmaC89Q-F9tsFeLQfQOGgIPbc9x24VeNAaUBE9TqtKHKofLX2Ea1xsotRK8bliZn1vceLJgrH1ixueFiqVo7UPIuKOw7_v390p1Rg0Jh8CL0iA2uC9jSTfT09-M_wkVmROZPh8Ac4hNlWe-p-N5061f956fxnoGuqcQ_m0fmW0s4K45N94Gevs1nB3JrVTB-llS3fGE5vPR_hB8qTwbJx0-ev.4IY2Y2G57kBAuuYXqaNucA\u0026#34;, \u0026#34;IdToken\u0026#34;: \u0026#34;eyJraWQiOiJWdjVBZXc1bHpZQUNvdWxaNVwvN3ZPSEEwUXBycTB3Sm1peXZQaFI1bVBYbz0iLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiI4OTNhNzVmYy02MGYxLTcwOTYtNWJiYi1mNDNlMDA5ZjJiM2IiLCJlbWFpbF92ZXJpZmllZCI6dHJ1ZSwiaXNzIjoiaHR0cHM6XC9cL2NvZ25pdG8taWRwLmFwLXNvdXRoZWFzdC0xLmFtYXpvbmF3cy5jb21cL2FwLXNvdXRoZWFzdC0xX1R1dXRkUlRMZCIsInBob25lX251bWJlcl92ZXJpZmllZCI6ZmFsc2UsImNvZ25pdG86dXNlcm5hbWUiOiI4OTNhNzVmYy02MGYxLTcwOTYtNWJiYi1mNDNlMDA5ZjJiM2IiLCJvcmlnaW5fanRpIjoiNWNmOWFiMWUtNDdjZC00MzZlLTk2OWYtZmU5ZDE3YTgxOGNhIiwiYXVkIjoiNTZtc2RjdHMwcjJ1YWhrdDZjMzBsdWxiZWgiLCJldmVudF9pZCI6ImQxMzljZjdmLTZiYTctNDNjNC1iM2JlLTFhYmYyYTI2M2QzOSIsInRva2VuX3VzZSI6ImlkIiwiYXV0aF90aW1lIjoxNzY0NDg3MDI1LCJuYW1lIjoiVHJhbiBNaW5oIFRoaWVuIiwicGhvbmVfbnVtYmVyIjoiKzg0MDc4MzQ3NjM0MSIsImV4cCI6MTc2NDQ5MDYyNSwiaWF0IjoxNzY0NDg3MDI1LCJqdGkiOiIyODY4NjgwNy03YTg0LTQ5MDctOTE3OC00YTNhMDZiYjM3NzAiLCJlbWFpbCI6InRoaWVuLnRtMjcyN0BnbWFpbC5jb20ifQ.rplRmMWbQFNyVXswjDecit4hbg3niMgxti0Xr6uFgjy0NBScI1t_vcd2M2N262KWnQzzki-ovYE8YKA14RUAdbz5lo9Xllqn0OHxTqWSTQnjqf4tYT7_SjWbH3_bSgarAzxT89-sK0dAMfZZYPrGfbRZeptCWWwyJxeEtffk09Pc7p8FAxszTFKro9pTnmlHYMUjADRW6ULfzdI7wNoBsRo1MNtIjXfxq9rtVPeVNQ4MvFraRzP9Fv9oZ9H_hgH1mC3XRXYyy5YyraY4bPFxBqLMrhTHRmhMrkUB8HRuOOiaYRmsf40aHp3qpsSpR0zy7WkiUE8W6FahBCgXoTSY5g\u0026#34; } } Note your AccessToken and IdToken, we will use this to authorize 5.2 Decode JWT Token (Understanding What\u0026rsquo;s Inside) To understand what information is in the token:\nGo to https://jwt.io/ Paste your AccessToken in the Encoded value box (left side) View the decoded Payload (right side): { \u0026#34;sub\u0026#34;: \u0026#34;893a75fc-60f1-7096-5bbb-f43e009f2b3b\u0026#34;, \u0026#34;iss\u0026#34;: \u0026#34;https://cognito-idp.ap-southeast-1.amazonaws.com/ap-southeast-1_TuutdRTLd\u0026#34;, \u0026#34;client_id\u0026#34;: \u0026#34;56msdcts0r2uahkt6c30lulbeh\u0026#34;, \u0026#34;origin_jti\u0026#34;: \u0026#34;5cf9ab1e-47cd-436e-969f-fe9d17a818ca\u0026#34;, \u0026#34;event_id\u0026#34;: \u0026#34;d139cf7f-6ba7-43c4-b3be-1abf2a263d39\u0026#34;, \u0026#34;token_use\u0026#34;: \u0026#34;access\u0026#34;, \u0026#34;scope\u0026#34;: \u0026#34;aws.cognito.signin.user.admin\u0026#34;, \u0026#34;auth_time\u0026#34;: 1764487025, \u0026#34;exp\u0026#34;: 1764490625, \u0026#34;iat\u0026#34;: 1764487025, \u0026#34;jti\u0026#34;: \u0026#34;4fa7ddf7-fbba-4237-a637-a921a546e89f\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;893a75fc-60f1-7096-5bbb-f43e009f2b3b\u0026#34; } Paste your IdToken { \u0026#34;sub\u0026#34;: \u0026#34;893a75fc-60f1-7096-5bbb-f43e009f2b3b\u0026#34;, \u0026#34;email_verified\u0026#34;: true, \u0026#34;iss\u0026#34;: \u0026#34;https://cognito-idp.ap-southeast-1.amazonaws.com/ap-southeast-1_TuutdRTLd\u0026#34;, \u0026#34;phone_number_verified\u0026#34;: false, \u0026#34;cognito:username\u0026#34;: \u0026#34;893a75fc-60f1-7096-5bbb-f43e009f2b3b\u0026#34;, \u0026#34;origin_jti\u0026#34;: \u0026#34;5cf9ab1e-47cd-436e-969f-fe9d17a818ca\u0026#34;, \u0026#34;aud\u0026#34;: \u0026#34;56msdcts0r2uahkt6c30lulbeh\u0026#34;, \u0026#34;event_id\u0026#34;: \u0026#34;d139cf7f-6ba7-43c4-b3be-1abf2a263d39\u0026#34;, \u0026#34;token_use\u0026#34;: \u0026#34;id\u0026#34;, \u0026#34;auth_time\u0026#34;: 1764487025, \u0026#34;name\u0026#34;: \u0026#34;Tran Minh Thien\u0026#34;, \u0026#34;phone_number\u0026#34;: \u0026#34;+840783476341\u0026#34;, \u0026#34;exp\u0026#34;: 1764490625, \u0026#34;iat\u0026#34;: 1764487025, \u0026#34;jti\u0026#34;: \u0026#34;28686807-7a84-4907-9178-4a3a06bb3770\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;Your email here\u0026#34; } You can see information of your user Key claims explained:\nsub: User\u0026rsquo;s unique identifier (UUID) - use this as cognito_sub in your database email: User\u0026rsquo;s email address email_verified: Email verification status (true/false) name: User\u0026rsquo;s full name cognito:username: Username chosen during sign-up phone_number: User\u0026rsquo;s phone number exp: Expiration time (Unix timestamp) - token expires after this time iat: Issued at time (Unix timestamp) - when token was created aud: Audience (your app client ID) iss: Issuer (Cognito user pool URL) This information will be available to your Lambda function when requests are authenticated.\nStep 6: Integrate Cognito with API Gateway Now let\u0026rsquo;s secure your API by requiring valid Cognito JWT tokens.\nHow it works:\nAfter signed-in, access token will be stored as httpOnly cookies When initiate API calls, we will send this access token in the request header API Gateway will verify this access token to authorize users 6.1 Create Cognito Authorizer in API Gateway Go to API Gateway console Select your API: workshop-user-api Click Authorizers in left navigation Click Create an authorizer Create Authorizer:\nName: CognitoUserPoolAuthorizer\nType: Select Cognito\nCognito User Pool:\nClick in the field Select your region Select your user pool: workshop-user-pool Token Source: Authorization\nThis is the HTTP header where the token will be sent Format: Authorization: Bearer \u0026lt;id_token\u0026gt; Token Validation:\nLeave blank (optional regex to validate token format) Click Create authorizer 6.2 Test the Authorizer Before applying to methods, let\u0026rsquo;s test it works:\nIn the authorizer page In Token value field, paste your ID token from Step 5 The full JWT string starting with eyJ... Click Test authorizer Expected result:\nYou will see the same information as the jwt decoder from above\nSuccess! The authorizer validated your token and extracted the claims.\nThese claims will be passed to your Lambda function in event.requestContext.authorizer.claims.\nTest Failed?\nError: \u0026ldquo;Unauthorized\u0026rdquo;\nCauses:\nToken expired (tokens last 1 hour) Wrong user pool selected Token from different user pool Malformed token (check you copied the entire string) Solution:\nRun token fetching command again to get fresh token Verify user pool ID matches Ensure you copied the complete token (no spaces, no line breaks) Check token hasn\u0026rsquo;t expired (decode at jwt.io and check exp claim) 5.3 Apply Authorizer to API Methods Now secure your API endpoints with the authorizer.\nSecure GET /users:\nClick Resources in left navigation\nExpand /users resource\nClick GET method\nClick Method Request tab\nClick the Edit button in the Method request settings card\nSelect CognitoUserPoolAuthorizer from the dropdown You\u0026rsquo;ll see \u0026ldquo;Authorization: CognitoUserPoolAuthorizer\u0026rdquo; displayed. Section Purpose Authorization Require Cognito JWT Scopes Restrict access by OAuth scope Validator Validate input (body/params/headers) API Key Require x-api-key if enabled Operation Name Label for logs/metrics Query Params Validate URL parameters Headers Validate header fields Request Body Validate JSON body Repeat for all other methods:\nApply the same authorizer to:\nPOST /users (Create a user) GET /users/{id} (Get single user) PUT /users/{id} (Update user) DELETE /users/{id} (Delete user) Which Methods to Secure?\nFor this workshop: Secure ALL methods to demonstrate full authentication.\nFor production: Consider your requirements:\nAlways secure: POST, PUT, DELETE (write operations) Sometimes public: GET (read operations might be public data) Authorization logic: Even if authenticated, check if user has permission (covered later) Example:\nPublic: GET /users (list all users - public directory) Authenticated: POST /users (create user - must be logged in) Authorized: PUT /users/{id} (update user - must be owner or admin) Authorized: DELETE /users/{id} (delete user - must be owner or admin) 5.4 Deploy API with Authentication Click Deploy API . Deployment stage: dev Deployment description: Added Cognito authentication Click Deploy Your API is now secured!\nAll requests must include a valid JWT token in the Authorization header.\nStep 6: Test Authenticated API Calls 6.1 Test Without Token (Should Fail) Try calling the API in Postman without authentication:\nExpected response:\n{ \u0026#34;message\u0026#34;: \u0026#34;Unauthorized\u0026#34; } Status code: 401 Unauthorized\nPerfect! The API rejected the unauthenticated request.\n6.2 Test With Valid Token (Should Succeed) Call the API with your ID token:\nGo to Authorization tab Auth type: select Bearer Token Token: paste your IdToken Click Send again You will now see all users fetched with status 200 Step 7: Configure App Client Settings We will now edit App Client settings\n7.1 Update App Client Go back to your app client page in Amazon Cognito Go to *Login pages tab Click Edit in Managed login pages configuration card 7.2 Configure Allowed Callback URLs After successful authentication, Cognito redirects users to these URLs.\nIf you have a deployed frontend (from 5.1.3-Cloudfront-setup):\nhttps://d1234abcd.cloudfront.net/callback Callback URLs Explained:\nAfter successful authentication, Cognito redirects users to a callback URL with authentication tokens in the URL fragment.\nFormat:\nhttps://yourdomain.com/callback#id_token=eyJraWQi...\u0026amp;access_token=eyJraWQi... Examples:\nDevelopment: http://localhost:3000/callback Production: https://app.yourdomain.com/callback Cloudfront: https://d1234abcd.cloudfront.net/callback You can add multiple callback URLs for different environments.\n7.3 Configure Allowed Sign-out URLs URLs where users are redirected after sign-out.\nIf you have a deployed frontend:\nhttps://d1234abcd.cloudfront.net 7.4 Configure OAuth 2.0 Settings Identity providers:\nKeep Cognito user pool checked OAuth 2.0 grant types:\nAuthorization code grant Implicit grant (only enable this if you want the token exposed in the callback URL, not recommended) OpenID Connect scopes:\nOpenID Email Phone Click Save changes 7.5 Test the callback URL Still in your app client page, click View login page In the new tab, sign in with your created user If sign-in is successful, you will be redirected to your frontend that you setup in 5.1 Step 8: Access User Info in Lambda 8.1 Understanding Authorizer Context When API Gateway validates a token with the Cognito authorizer, it automatically passes user claims to your Lambda function.\nWhere to find claims:\nevent.requestContext.authorizer.claims; Available claims:\n{ sub: \u0026#34;12345678-1234-1234-1234-123456789012\u0026#34;, email: \u0026#34;test@example.com\u0026#34;, email_verified: \u0026#34;true\u0026#34;, name: \u0026#34;Test User\u0026#34;, \u0026#34;cognito:username\u0026#34;: \u0026#34;testuser\u0026#34;, phone_number: \u0026#34;+1234567890\u0026#34;, phone_number_verified: \u0026#34;false\u0026#34;, ... } 8.2 Example: Log Authenticated User Add logging to see who\u0026rsquo;s making requests:\nexport const handler = async ( event: APIGatewayEvent ): Promise\u0026lt;LambdaResponse\u0026gt; =\u0026gt; { console.log(\u0026#34;Received event:\u0026#34;, JSON.stringify(event, null, 2)); // Log authenticated user const claims = event.requestContext.authorizer?.claims; if (claims) { console.log(\u0026#34;Authenticated user:\u0026#34;, { cognitoSub: claims.sub, username: claims[\u0026#34;cognito:username\u0026#34;], email: claims.email, }); } else { console.log( \u0026#34;Unauthenticated request (authorizer not configured or bypassed)\u0026#34; ); } // ... rest of handler logic }; 8.3 Auto-populate cognitoSub from Token Instead of requiring cognitoSub in request body, extract it from the authenticated user: Modify your lambda code\nasync function createUser(body) { const client = await connectToRds(); try { const data = JSON.parse(event.body); // Get authenticated user\u0026#39;s cognito sub from token const claims = event.requestContext.authorizer?.claims; if (!claims || !claims.sub) { return createResponse(401, { success: false, error: \u0026#34;Authentication required\u0026#34;, }); } const cognitoSub = claims.sub; // From JWT token const email = claims.email; // From JWT token const username = claims[\u0026#34;cognito:username\u0026#34;]; // From JWT token const role = data.role || \u0026#34;user\u0026#34;; // Default to \u0026#39;user\u0026#39; role const phoneNumber = claims.phone_number; // From JWT token const result = await client.query( `INSERT INTO users (cognito_sub, username, email, role, phone_number) VALUES ($1, $2, $3, $4, $5) RETURNING *`, [cognitoSub, username, email, role, phoneNumber] ); return respond(201, { success: true, data: result.rows[0] }); } catch (err) { return respond(400, { success: false, error: err.message }); } finally { await client.end(); } } Update handler to pass event:\nconst handler = async (event) =\u0026gt; { log(\u0026#34;[users-handler] process start.\u0026#34;); const method = event.httpMethod; const id = event.pathParameters?.id; try { switch (method) { case \u0026#34;POST\u0026#34;: return createUser(event); //update event here case \u0026#34;GET\u0026#34;: return id ? getUser(id) : getAllUsers(); case \u0026#34;PUT\u0026#34;: if (!id) return respond(400, { success: false, error: \u0026#34;Missing user ID\u0026#34; }); return updateUser(id, event.body); case \u0026#34;DELETE\u0026#34;: if (!id) return respond(400, { success: false, error: \u0026#34;Missing user ID\u0026#34; }); return deleteUser(id); default: return respond(400, { success: false, error: `Unsupported HTTP method: ${method}`, }); } } catch (err) { return respond(500, { success: false, error: err.message }); } finally { log(\u0026#34;[users-handler] process end.\u0026#34;); } }; You can download fully working source code that includes the above changes here:\nGithub repository: https://github.com/Icyretsz/fcj-workshop-serverless-backend-ver2 Only the zip file for lambda: https://fcj-workshop-files.s3.ap-southeast-1.amazonaws.com/userHandler-final.zip Note that this repo is different from the repo in 5.2.5 After upload the zip to Lambda, you should find the response function and modify Access-Control-Allow-Origin from * to your CloudFront endpoint to only allow requests from your CloudFront Summary Congratulations! You\u0026rsquo;ve successfully:\nCreated Amazon Cognito User Pool Configured sign-in, sign-up, and verification settings Set up Cognito hosted UI Registered and verified test users Obtained JWT tokens from CLI Created Cognito authorizer in API Gateway Secured API endpoints with authentication Tested authenticated API calls Extracted user information from JWT tokens in Lambda Understood Cognito pricing model What You\u0026rsquo;ve Built Your application now has:\nUser Authentication: Sign-up, sign-in, email verification JWT Token Generation: Secure tokens for API access API Authorization: API Gateway validates tokens automatically User Identity: Lambda functions know who\u0026rsquo;s making requests Scalable Auth: Handles millions of users with Cognito Secure by Default: No passwords stored in your database "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nHere, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don‚Äôt understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? What do you think the company should improve for future interns? If recommending to a friend, would you suggest they intern here? Why or why not? Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Would you like to continue this program in the future? Any other comments (free sharing): "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives Apply Infrastructure as Code with AWS CloudFormation to deploy, update, and scale application environments. Strengthen workload reliability through resiliency testing, Auto Scaling, and automated recovery patterns. Build and secure modern architectures such as serverless SPAs with authentication and performance tracing. Explore AI, storage, and content delivery services including Amazon Polly, Rekognition, Lex, S3, and CloudFront. Monitor resources using CloudWatch dashboards across different OS platforms and prepare for the weekly assessment. Tasks Completed This Week Day Task Start Date Completion Date Reference Material 2 - Infrastructure as Code with CloudFormation + Deploy a foundational VPC and networking components via CloudFormation + Launch a multi-tier web application stack on the VPC + Review the running architecture (load balancer, Auto Scaling group, EC2 instances) + Examine stack resources and outputs to understand the deployment - Advanced Health Checks \u0026amp; Dependency Handling + Deploy a baseline web stack via CloudFormation + Simulate a dependency failure and observe application behavior + Configure deep health checks for the ALB + Implement a \u0026ldquo;fail open\u0026rdquo; mechanism to maintain limited functionality during outages - Evolving Infrastructure Using CloudFormation + Deploy a baseline stack and analyze its components + Update the running stack using parameter changes + Extend the template by adding an S3 bucket + Add another EC2 instance with custom configuration + Deploy the same stack in another AWS Region - Refactor Monolith to Microservices + Prepare the development environment and connect to the Windows instance + Analyze the monolithic application\u0026rsquo;s architecture + Build and deploy microservices: Advert, Invoice, ShoppingCart, Order, User + Serve static content via a dedicated Static microservice + Validate overall functionality of the microservices-based system 10/27/2025 10/27/2025 AWS Well-Architected Reliability Workshop https://catalog.workshops.aws/well-architected-reliability Refactoring to Microservices https://000054.awsstudygroup.com/ 3 - Resiliency Testing with AWS FIS + Create IAM roles and policies for FIS + Build an experiment template targeting specific resources + Run the fault injection experiment and monitor its effects + Review logs and experiment output to evaluate system behavior - Configure Auto Scaling for Load \u0026amp; Recovery + Create a launch template for web-tier EC2 instances + Configure a target group for the ALB + Create an Auto Scaling group + Deploy a load generator and validate scaling behavior - Automated Replacement via Health Checks + Manually terminate an EC2 instance to trigger recovery + Verify ASG replacement and ALB target health - Environment Setup + Create an EC2 Key Pair + Deploy foundational infrastructure via CloudFormation + Connect to the Windows instance to prepare the environment - Serverless Single Page Application Deployment + Create DynamoDB table + Build and deploy a Lambda microservice + Configure an API via API Gateway + Set up CI/CD with CodeStar + Deploy the SPA website and implement a client to consume the API - Authentication \u0026amp; Authorization + Integrate Cognito User Pools + Secure API/Lambda with authentication + Implement user sign-up and sign-in + Test complete auth flows - Performance Tracing with AWS X-Ray + Integrate X-Ray to trace requests and identify bottlenecks 10/28/2025 10/28/2025 AWS Well-Architected Reliability Workshop https://catalog.workshops.aws/well-architected-reliability Serverless Web Application https://000055.awsstudygroup.com/ 4 - Amazon Polly Integration + Explore the Polly console + Generate speech and speech marks using CLI + Use the Java SDK to synthesize speech - Object \u0026amp; Face Recognition with Rekognition + Prepare the environment + Detect objects in images + Implement basic facial recognition with a sample app - Build a Chatbot with Amazon Lex + Deploy base application and APIs + Create and enhance Lex bot + Implement Lambda fulfillment handler + Publish the bot - Static Web Hosting via S3 \u0026amp; CloudFront + Create S3 bucket and upload website content + Enable static website hosting + Configure public access settings and object permissions + Create CloudFront distribution - S3 Data Protection \u0026amp; Replication + Enable versioning + Practice object movement within buckets + Configure cross-region replication 10/29/2025 10/29/2025 AI Services Integration https://000056.awsstudygroup.com/ S3 \u0026amp; CloudFront https://000057.awsstudygroup.com/ 5 - CloudWatch Dashboards for Monitoring + Create dashboards for metrics visualization + Add metric and Logs Insights widgets - Monitor Windows EC2 Instance + Deploy VPC networking + Launch \u0026amp; configure Windows EC2 + Build a custom dashboard + Add CPU, network, and performance metrics + Generate synthetic load to observe changes - Monitor Linux EC2 Instance + Deploy VPC and launch Linux EC2 with a web server + Create a monitoring dashboard + Track CPU and network performance + Generate load to test responsiveness 10/30/2025 10/30/2025 AWS Well-Architected Performance Efficiency Workshop https://catalog.workshops.aws/well-architected-performance-efficiency/ 6 - EXAM DAY 10/31/2025 10/31/2025 Week 8 Achievements Developed strong proficiency in CloudFormation: provisioning VPCs, multi-tier stacks, and evolving infrastructure with new resources such as S3 buckets, EC2 instances, and multi-region deployments. Strengthened understanding of resiliency through fault injection, deep health checks, Auto Scaling configurations, and automated instance recovery. Successfully decomposed a monolithic application into multiple microservices (Advert, Invoice, ShoppingCart, Order, User, Static) and validated end-to-end operations. Built a serverless SPA with API Gateway, Lambda, DynamoDB, CI/CD (CodeStar), Cognito authentication, and X-Ray tracing. Gained hands-on experience with AI and content delivery services including Polly, Rekognition, Lex, S3, and CloudFront. Enhanced monitoring skills by creating CloudWatch dashboards and analyzing key metrics for Windows and Linux EC2 workloads. Completed the weekly assessment and consolidated knowledge across reliability, serverless, AI services, and monitoring domains. "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/5-workshop/5.2-serverless-backend/5.2.8-frontend-integration/",
	"title": "Frontend Integration",
	"tags": [],
	"description": "",
	"content": "Overview In this section, you\u0026rsquo;ll finally combine what we have built so far in the backend to connect to our CloudFront frontend we setup in the previous part.\nWhat you\u0026rsquo;ll accomplish:\nConfigure Lambda function to include CORS headers Configure API Gateway CORS settings Configure Cognito callback URL and login URL to your CloudFront endpoint Test the application CRUD operations Estimated time: 30 minutes\nStep 1: Configure Lambda function Since we are calling our APIs from a different origin (CloudFront) , we have to configure CORS headers properly\nGo to Lambda console, select workshop-lambda-sm-rds View the response function, this function will inlude proper CORS headers in the responses Make sure the Access-Control-Allow-Origin is set with your CloudFront endpoint function respond(statusCode, payload) { return { statusCode, headers: { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Access-Control-Allow-Headers\u0026#34;: \u0026#34;Content-Type\u0026#34;, \u0026#34;Access-Control-Allow-Origin\u0026#34;: \u0026#34;https://[cloudfront-id].cloudfront.net\u0026#34;, //add your CloudFront endpoint here \u0026#34;Access-Control-Allow-Methods\u0026#34;: \u0026#34;OPTIONS,POST,GET,DELETE\u0026#34;, }, body: JSON.stringify(payload), }; } This makes sure that the APIs are only allow to be called from trusted origin (which is your CloudFront front-end) Step 2: Configure API Gateway CORS settings of resources Make sure that you have set the correct CORS settings in our /users and /{id} resouces. We have already done this in 5.2.6 - API Gateway setup Step 3: Configure Cognito callback URL and login URL to your CloudFront endpoint We already done this in 5.2.7 - Cognito Step 4: Test the application Go to your CloudFront endpoint Click Sign in You will be redirected to the hosted login UI Upon succesful login, you will be redirected to the homepage. The homepage will display your information and tokens, along with the user list fetched from RDS You can test CRUD operations This concludes our workshop "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 9 Objectives: Architect conversational interfaces and event-driven systems utilizing Amazon Lex and Amazon SNS. Implement managed data and caching solutions with DynamoDB and ElastiCache, while automating EKS deployments. Enforce governance and scalability via Service Quotas, IAM-based usage controls, and EKS Blueprints. Develop and maintain serverless and containerized workloads, and assess storage performance across S3 and EFS. Secure S3 infrastructure and establish a fundamental data lake pipeline using Glue, Athena, and QuickSight. Tasks carried out this week: Day Task Start Date Completion Date Reference Material 2 - Configure an Amazon Lex Chatbot + Provision backend infrastructure and APIs + Initialize and configure a new Lex chatbot instance + Refine conversational intents and slot types + Implement Lambda hooks for fulfillment logic + Publish chatbot aliases for production use - Implement a Publish/Subscribe Messaging Model with Amazon SNS + Deploy base infrastructure via SAM template + Configure SNS Topics for event broadcasting + Build decoupled subscribers: Notification, Accounting, and Ride services + Apply message filtering policies for specific routing + Update publisher services and validate fan-out delivery 11/03/2025 11/03/2025 Amazon Lex Chatbot: https://000058.awsstudygroup.com/ Messaging with Amazon SNS: https://000059.awsstudygroup.com/ 3 - Work with Amazon DynamoDB + Execute core CRUD operations and table provisioning + Implement and query Global Secondary Indexes (GSI) + Administer tables via AWS CloudShell + Automate table management and queries using the Python SDK - Work with Amazon ElastiCache for Redis + Configure subnet groups for caching layers + Launch Redis clusters (Cluster Mode Enabled/Disabled) + Secure access and connect via client nodes + Integrate AWS SDK for Redis operations + Implement advanced patterns: Strings, Hashes, Pub/Sub, and Streams - Build a CI/CD Pipeline for an EKS Cluster + Set up Cloud9 environment and Kubernetes tools + Configure IAM roles for EKS authentication + Provision EKS cluster and validate sample app deployment + Setup CodePipeline and CodeBuild with IAM permissions + Automate manifest deployment from source repositories + Verify pipeline execution via code commit triggers 11/04/2025 11/04/2025 Amazon DynamoDB Workshop: https://000060.awsstudygroup.com/ Amazon ElastiCache Workshop: https://000061.awsstudygroup.com/ EKS CI/CD Workshop: https://000062.awsstudygroup.com/ 4 - Manage Service Quotas + Audit current utilization against limits + Request quota increases via the Service Quotas console - Implement Resource Usage and Cost Management with IAM + Structure IAM groups/users for permission boundaries + Enforce policies restricting resources by AWS Region + Limit allowed EC2 instance families + Apply restrictions on EC2 instance sizes + Control costs by limiting available EBS volume types - Deploy and Manage an EKS Cluster using EKS Blueprints + Bootstrap infrastructure: VPC and EC2 setup + Configure IAM roles for EKS Blueprints + Initialize EKS Blueprints and CDK projects + Construct deployment pipelines for cluster management + Manage multi-team access via Infrastructure as Code (IaC) + Integrate add-ons like Cluster Autoscaler + Deploy workloads using GitOps (ArgoCD) 11/05/2025 11/05/2025 Service Quotas Workshop: https://000063.awsstudygroup.com/ IAM Resource Management: https://000064.awsstudygroup.com/ EKS Blueprints Workshop: https://000065.awsstudygroup.com/ 5 - Build a Serverless Web Application using Lambda and API Gateway + Setup Cloud9 and CodeCommit for version control + Host frontend via AWS Amplify Console + Deploy serverless backend (Lambda + API Gateway) + Seed DynamoDB state data + Implement ride-booking logic integration + Develop photo processing pipelines using Lambda - Transition a Monolithic Application to Microservices using Docker and AWS Fargate + Provision environment via CloudFormation + Containerize legacy application with Docker + Deploy containers to serverless AWS Fargate + Configure Application Load Balancer and ECS Services + Manage task definition revisions and updates + Refactor and deploy microservices alongside the monolith - Evaluate Storage Performance on AWS + Deploy test infrastructure via CloudFormation + Benchmark S3 throughput and sync efficiency + Analyze performance for small-file and copy operations + Tune EFS IOPS and evaluate I/O size impact + Assess multi-threading effects on EFS performance 11/06/2025 11/06/2025 Serverless Web Application: https://000066.awsstudygroup.com/ Microservices with Fargate: https://000067.awsstudygroup.com/ Storage Performance Evaluation: https://000068.awsstudygroup.com/ 6 - Implement S3 Security Best Practices + Secure network access via CloudFormation + Configure secure access keys for EC2 + Enforce HTTPS transport and SSE-S3 encryption + Enable \u0026ldquo;Block Public Access\u0026rdquo; and disable ACLs + Restrict access using S3 VPC Endpoints + Audit configurations with AWS Config + Verify permissions with Access Analyzer - Build a Data Lake with Your Data + Stage raw datasets in Cloud9 and S3 + Profile and clean data using AWS DataBrew + Catalog and ingest data with AWS Glue + Transform datasets to Parquet format + Execute analytical queries via Amazon Athena + Visualize insights with Amazon QuickSight dashboards 11/07/2025 11/07/2025 S3 Security Best Practices: https://000069.awsstudygroup.com/ Data Lake Workshop: https://000070.awsstudygroup.com/ Week 9 Achievements: Engineered conversational workflows and messaging systems by deploying Amazon Lex chatbots with Lambda hooks and establishing a decoupled SNS Pub/Sub architecture.\nMastered managed data and caching services:\nExecuted core DynamoDB operations, including GSI management and SDK-based automation. Deployed ElastiCache for Redis clusters, implementing advanced caching patterns like Pub/Sub and Streams. Automated Kubernetes lifecycles by provisioning EKS clusters, defining IAM roles, and constructing CI/CD pipelines with CodePipeline to streamline application deployment.\nEnforced cloud governance and cost optimization:\nAudited and managed AWS Service Quotas for scalability. Applied granular IAM policies to restrict resource usage by Region, Instance Type, and Volume characteristics. Leveraged EKS Blueprints and IaC to bootstrap network infrastructure, manage multi-team access, and deploy add-ons (Autoscaler) and workloads via ArgoCD.\nArchitected modern application solutions:\nBuilt a full-stack serverless ride-share app using Amplify, API Gateway, and Lambda. Refactored a monolithic application into microservices, containerized via Docker, and deployed on AWS Fargate. Benchmarked storage performance by analyzing S3 throughput and transfer speeds, alongside tuning EFS IOPS and threading configurations.\nHardened S3 security posture:\nEnforced encryption and HTTPS, blocked public access via ACLs, and restricted traffic through VPC Endpoints. Utilized AWS Config and Access Analyzer to audit and remediate potential security risks. Established a foundational Data Lake: staged data in S3, transformed it via Glue/DataBrew, and enabled analytics with Athena querying and QuickSight visualizations.\n"
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/5-workshop/5.2-serverless-backend/5.2.9-clean-up/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "Overview This section covers how to properly clean up the resources you created during Part 2: Serverless Backend. Cleaning up is important to avoid ongoing AWS charges, especially for resources that have monthly costs like RDS.\nWhat you\u0026rsquo;ll learn:\nHow to safely delete resources in the correct order Understanding resource dependencies Cost implications of keeping vs. deleting resources How to preserve configuration for future use Partial cleanup options Estimated time: 15-20 minutes\nShould You Clean Up? Keep Resources If: You\u0026rsquo;re proceeding immediately to Part 2: Serverless Backend You want to maintain the working frontend for reference You\u0026rsquo;re using this for a real project Costs are acceptable for your use case Clean Up If: You\u0026rsquo;ve completed the workshop and don\u0026rsquo;t need the resources You want to minimize AWS costs You\u0026rsquo;re practicing and will recreate later You\u0026rsquo;re approaching Free Tier limits Complete Cleanup (Step-by-Step) Follow this order to avoid dependency errors:\nStep 1 Delete RDS instance and subnet group 1.1 Delete RDS instance Go to Aurora and RDS dashboard In the left sidebar, click Databases Click on workshop-posgresql-db On the right, click Action button, then click Delete Uncheck Create final snapshot and Retain automated backup as these two will incur charges even after you deleted your RDS instance Check I acknowledge that upon instance deletion, automated backups, including system snapshots and point-in-time recovery, will no longer be available. Type delete me when prompted click Delete 1.2 Delete subnet group Still in Aurora and RDS dashboard, in the left sidebar, click Subnet group Select workshop-db-subnet-group On the right, click Delete Click Delete in the modal Wait for the instance delete to complete, then proceed to delete subnet group in next step Step 2 Delete Lambda function Go to Lambda dashboard Select workshop-lambda-sm-rds On the right, click Action button Click Delete in the dropdown Type delete when prompted and click Delete Step 3 Delete secret in AWS Secrets Manager Go to Secrets Manager dashboard In the secrets list, click on the secret name of RDS On the right, click Action button, then click *Delete in the dropdown A modal appear, choose your Waiting period, after this period, the secret will be deleted Click Schedule deletion Step 4 Delete API Gateway resources Go to API Gateway dashboard On the left sidebar, click APIs Select workshop-user-api On the right, click Delete Type confirm when prompted Click Delete Step 5 Delete Cognito user pool Go to Cognito dashboard On the left sidebar, click User pools In the list, select User pool - id Check two checkboxes, then type in the Cognito user pool name when prompted Click Delete Step 6 Delete VPC and VPC endpoint 6.1 Delete VPC endpoint Go to VPC dashboard On the left sidebar, click Endpoints Select workshop-lambda-secretsmng-endpoint On the right, click Action button, then select Delete in the dropdown Type in delete when prompted and click Delete Wait for deletion to complete before proceed to VPC deletion 6.2 Delete VPC On the left sidebar, click Your VPCs Select workshop-backend-vpc On the right, click Action button, then click Delete VPC Step 7 Delete CloudWatch log groups Go to CloudWatch dashboard On the left sidebar, click Log groups Delete all log groups of our resources Step 8 Delete roles Go to IAM dashboard On the left sidebar, click Roles Delete these roles api-gw-push-cloudwatch-logs workshop-lamda-secretsmng-role Summary Cleanup Completion Checklist If performing complete cleanup:\nRDS: No databases or subnet groups Lambda: No workshop-lambda-sm-rds function Secrets Manager: Secret scheduled for deletion API Gateway: No workshop-user-api Cognito user pool VPC Endpoints: No workshop-secretsmng-endpoint VPC: No workshop-backend-vpc CloudWatch: No workshop log groups IAM: No workshop roles/policies "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 10 Objectives: Deploy and manage containerized workloads using Red Hat OpenShift Service on AWS (ROSA) and establish a fundamental CI/CD workflow. Architect and analyze a comprehensive data platform on AWS utilizing streaming ingestion, Glue, EMR, Athena, QuickSight, and Redshift. Design business intelligence dashboards and enhance observability via Amazon QuickSight, VPC Flow Logs, and delegated billing permissions. Develop and launch cloud-native applications utilizing AWS CDK, event-driven patterns with SNS/SQS, and serverless Lambda functions. Construct full-stack serverless web applications with API Gateway, Lambda, DynamoDB, Cognito, and CloudFront, ensuring SSL security and authentication. Automate order processing workflows and implement CI/CD pipelines for serverless architectures using SQS, SNS, SAM, and CodePipeline. Tasks carried out this week: Day Task Start Date Completion Date Reference Material 2 - Deploy Applications via Red Hat OpenShift Service on AWS (ROSA) + Enable ROSA service and install required CLI utilities + Provision and configure an OpenShift cluster on AWS infrastructure + Deploy a sample workload onto the active OpenShift cluster + Establish a basic CI/CD pipeline using CodeCommit, CodeBuild, and CodePipeline + Configure a dedicated CI/CD workflow for automated application deployment - Construct an Analytics Platform on AWS + Ingest and persist streaming data utilizing Amazon Kinesis Firehose + Automate data cataloging with AWS Glue Crawlers + Execute data transformations via Glue interactive sessions, Studio, and DataBrew + Process big data workloads using Amazon EMR + Perform interactive analysis with Amazon Athena and real-time analytics with Kinesis + Visualize insights by designing Amazon QuickSight dashboards + Serve data via AWS Lambda and architect a data warehouse with Amazon Redshift 11/10/2025 11/10/2025 ROSA Hands-on Lab: https://000071.awsstudygroup.com/ Analytics Platform: https://000072.awsstudygroup.com/ 3 - Initialize Amazon QuickSight Dashboards + Ingest data and construct initial visualizations including line charts, pie charts, and pivot tables + Refine the dashboard with advanced formatting, visual additions, and detailed data tables + Implement interactivity by configuring filters, actions, and navigation prior to publishing - Monitor Network Infrastructure via VPC Flow Logs + Configure and activate VPC Flow Logs to capture IP traffic metadata + Route flow log streams to Amazon CloudWatch Logs + Analyze traffic patterns to diagnose security group effectiveness and network health - Delegate Permissions to the AWS Billing Console + Establish an IAM user group and enable Billing Console access + Author a custom IAM policy for granular billing and cost management permissions + Attach the policy to the IAM group to finalize delegation + Validate the configuration by accessing the console as a delegated user 11/11/2025 11/11/2025 Amazon QuickSight Guide: https://000073.awsstudygroup.com/ VPC Flow Logs Lab: https://000074.awsstudygroup.com/ AWS Billing Console: https://000075.awsstudygroup.com/ 4 - Develop Infrastructure as Code with AWS CDK + Bootstrap the environment with IAM Roles and EC2 instances + Configure the VSCode development environment + Define application architecture (API Gateway, ELB, ECS) using CDK + Integrate Lambda logic and S3 storage constructs + Optimize structure using nested stacks for reusability - Architect Event-driven Systems with SNS and SQS + Provision core infrastructure and configure event generators + Implement a decoupled publish/subscribe model via SNS topics and SQS queues + Apply message filtering policies to SNS subscriptions for targeted routing + Design complex routing logic using advanced message filtering techniques - Develop Serverless Logic with AWS Lambda + Author a Lambda function for S3-triggered image processing + Configure S3 buckets and IAM execution roles for the function + Validate the image resizing operations + Provision a DynamoDB table for data persistence + Implement data writing to DynamoDB via Lambda functions 11/12/2025 11/12/2025 AWS CDK Development: https://000076.awsstudygroup.com/ Event-driven Architecture Lab: https://000077.awsstudygroup.com/ Serverless Lambda Guide: https://000078.awsstudygroup.com/ 5 - Integrate Serverless Frontend with API Gateway + Deploy client-side application assets + Provision a DynamoDB table + Deploy Lambda handlers for Create, Read, and Delete operations + Configure API Gateway methods and enable CORS + Validate API endpoints using Postman and the frontend interface - Deploy Serverless Applications via SAM + Automate frontend deployment using SAM + Define DynamoDB resources in the template + Deploy Lambda functions for listing, writing, deleting, and resizing images + Configure GET, POST, and DELETE verbs in API Gateway + Execute end-to-end testing via Postman and the web interface - Implement Identity Management with Amazon Cognito + Configure a Cognito User Pool + Secure the API and corresponding Lambda function + Verify the authentication flow via the frontend application - Configure SSL/TLS for Serverless Applications + Register a custom domain and Route 53 hosted zone + Provision an SSL certificate via AWS Certificate Manager + Configure a CloudFront distribution for HTTPS delivery 11/13/2025 11/13/2025 Serverless Frontend with API Gateway: https://000079.awsstudygroup.com/ Serverless Application with SAM: https://000080.awsstudygroup.com/ Cognito Authentication Lab: https://000081.awsstudygroup.com/ SSL Configuration: https://000082.awsstudygroup.com/ 6 - Automate Order Processing with SQS and SNS + Provision SQS queues and SNS topics + Create a DynamoDB table for order persistence + Develop Lambda functions for checkout, management, handling, and deletion + Validate the complete order processing lifecycle - Construct a CI/CD Pipeline for Serverless Workloads + Initialize a Git repository for the SAM pipeline code + Configure the SAM deployment pipeline utilizing AWS CodePipeline + Initialize a Git repository for frontend assets + Build a deployment pipeline for the frontend application + Verify the functionality and stability of the web application 11/14/2025 11/14/2025 Order Processing with SQS and SNS: https://000083.awsstudygroup.com/ CI/CD Pipeline for Serverless: https://000084.awsstudygroup.com/ Week 10 Achievements: Deployed applications using Red Hat OpenShift Service on AWS (ROSA) by enabling the service, provisioning a cluster, running sample workloads, and wiring a fundamental CI/CD workflow with CodeCommit, CodeBuild, and CodePipeline.\nArchitected and explored a data analytics platform on AWS:\nIngested streaming data via Kinesis Firehose and cataloged it using AWS Glue Crawlers. Transformed and processed data utilizing Glue (Studio and interactive sessions), DataBrew, and EMR, followed by analysis with Athena and Kinesis Data Analytics. Visualized insights via QuickSight dashboards and served data through Lambda and Redshift warehousing. Enhanced business intelligence and observability:\nDesigned interactive QuickSight dashboards featuring diverse charts, formatting, filters, and navigation actions. Activated and analyzed VPC Flow Logs in CloudWatch to interpret traffic patterns and validate security groups. Delegated AWS Billing Console access via IAM groups and custom policies, verifying permissions with user testing. Developed infrastructure and event-driven applications using AWS CDK:\nEstablished a CDK development environment on EC2 with VSCode and IAM roles, defining architectures for API Gateway, ELB, ECS, Lambda, and S3. Implemented SNS and SQS event-driven flows, incorporating both basic pub/sub models and advanced message filtering logic. Created Lambda-based workflows for image processing and data persistence utilizing S3 and DynamoDB. Built secure, end-to-end serverless application stacks:\nDeployed frontend apps integrating with API Gateway, Lambda, and DynamoDB using both manual configuration and SAM automation. Configured API Gateway with CORS, enforced Cognito-based authentication, and validated flows via Postman and UI. Secured applications with custom domains, Route 53 zones, ACM certificates, and CloudFront HTTPS distributions. Implemented automated order processing and CI/CD for serverless systems:\nConstructed an order processing pipeline leveraging SQS, SNS, DynamoDB, and Lambda functions for full lifecycle management. Established CI/CD pipelines for SAM-based backends and frontend applications using CodePipeline, linked to Git repositories for automated deployment. "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 11 Objectives: Participate in AWS community events to gain insights into advanced cloud architectures and industry standards. Engineer a serverless backend for text services, focusing on DynamoDB schema design and IAM role security. Program efficient database retrieval logic featuring pagination and complex filter expressions. Architect an in-memory caching strategy and robust API request validation mechanisms. Integrate Generative AI capabilities using Amazon Bedrock Agents for dynamic content generation. Implement comprehensive observability and debugging for serverless workloads. Construct scalable GraphQL APIs utilizing AWS AppSync and DynamoDB resolvers. Tasks carried out this week: Day Task Start Date Completion Date Reference Material 2 - Participation in \u0026ldquo;AWS Cloud Mastery Series #2\u0026rdquo; 11/17/2025 11/17/2025 3 - Provision Serverless Infrastructure for Text Service: + Deploy DynamoDB table wordsntexts with a string partition key for storage + Configure IAM execution roles permitting dynamodb:Scan, dynamodb:Query, and logging + Initialize Lambda handler skeleton and establish boto3 database connections - Develop Data Retrieval Algorithms: + Implement fetch_words_from_db utilizing scan operations with LastEvaluatedKey pagination + Create fetch_paragraph_from_db applying attributes filtering for content type and length 11/18/2025 11/18/2025 4 - Implement Caching and Data Processing Logic: + Architect a global in-memory cache with a 300-second TTL to optimize read costs + Code get_random_words to sample data from either cache or database sources + Code get_paragraph to handle quantity clamping (1-3) and text splitting logic - Engineer API Validation and Response Formatting: + Develop a Decimal_encoder class to serialize DynamoDB numeric types for JSON + Implement validation for query parameters type and count to catch TypeError/ValueError + Standardize JSON responses with correct HTTP headers and status codes 11/19/2025 11/19/2025 5 - Integrate Amazon Bedrock Agent for GenAI: + Update IAM policies to allow bedrock:InvokeAgent on Agent ID HUEBUXSALX + Instantiate bedrock-agent-runtime client and manage invoke_agent sessions + Program logic to decode streaming byte chunks into a cohesive string response - Execute Prompt Engineering and Model Testing: + Refine trigger prompts to enforce a specific output format (three paragraphs with separators) + Evaluate underlying models for formatting consistency + Implement parsing logic to segment the AI-generated text 11/20/2025 11/20/2025 6 - Monitor and Debug via CloudWatch and X-Ray + Audit Lambda logs in CloudWatch to diagnose execution failures + Define custom metrics to track specific application performance indicators + Set up CloudWatch Alarms to alert on critical metric deviations + Activate AWS X-Ray tracing to map service dependencies and latency - Build GraphQL APIs with AWS AppSync + Provision AppSync instance and link DynamoDB as a datasource + Develop resolvers for atomic Write and Read operations + Implement Update and Delete resolvers for data lifecycle management + Configure Scan/Query resolvers for bulk retrieval + Design complex resolvers for nested data fields 11/21/2025 11/21/2025 CloudWatch and X-Ray Monitoring: https://000085.awsstudygroup.com/ AppSync GraphQL APIs: https://000086.awsstudygroup.com/ Week 11 Achievements: Participated in the \u0026ldquo;AWS Cloud Mastery Series #2\u0026rdquo; to acquire knowledge on advanced AWS services and architectural patterns.\nEngineered the serverless infrastructure for a typing practice application:\nProvisioned DynamoDB table wordsntexts to house ~64,726 items using a string partition key. Secured the backend with granular IAM roles for DynamoDB access and CloudWatch logging. Configured optimized Lambda handlers (128 MB, 15s timeout) with persistent boto3 connections. Programmed advanced data retrieval algorithms:\nImplemented fetch_words_from_db utilizing efficient pagination via LastEvaluatedKey. Built fetch_paragraph_from_db utilizing filter expressions for length categories (Short: 10-25, Medium: 25-60, Long: 60+). Optimized performance and request handling:\nDeployed an in-memory caching mechanism (300s TTL) to minimize DB reads under a load of ~500 requests/hour. Implemented data sampling functions get_random_words and get_paragraph with input clamping. Created a custom Decimal_encoder for JSON serialization and robust input validation to prevent runtime errors. Standardized API responses with correct HTTP status codes and structure. Integrated Generative AI via Amazon Bedrock Agents:\nConfigured IAM access for bedrock:InvokeAgent targeting Agent ID HUEBUXSALX. Developed a runtime client to handle session-based invocation and stream decoding. Engineered prompts to generate structured content (three separated paragraphs) for daily challenges. Implemented parsing logic to seamlessly process AI-generated outputs. Established comprehensive monitoring and debugging:\nUtilized CloudWatch Logs for error analysis and defined custom metrics for performance tracking. Configured CloudWatch Alarms for critical thresholds and enabled X-Ray for distributed tracing. Developed a robust GraphQL API layer using AWS AppSync:\nIntegrated DynamoDB data sources and implemented full CRUD resolvers. Enabled bulk data access via Scan/Query resolvers and handled nested structures with complex object resolvers. "
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://nicolaihong.github.io/InternReport_FCJ/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]